; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -aarch64-sve-vector-bits-min=128  < %s | FileCheck %s -D#VBYTES=16  -check-prefixes=CHECK,VBITS_EQ_128
; RUN: llc -aarch64-sve-vector-bits-min=256  < %s | FileCheck %s -D#VBYTES=32  -check-prefixes=CHECK,VBITS_GE_256
; RUN: llc -aarch64-sve-vector-bits-min=384  < %s | FileCheck %s -D#VBYTES=32  -check-prefixes=CHECK,VBITS_GE_256,VBITS_GE_512
; RUN: llc -aarch64-sve-vector-bits-min=512  < %s | FileCheck %s -D#VBYTES=64  -check-prefixes=CHECK,VBITS_GE_256,VBITS_GE_512
; RUN: llc -aarch64-sve-vector-bits-min=640  < %s | FileCheck %s -D#VBYTES=64  -check-prefixes=CHECK,VBITS_GE_256,VBITS_GE_512
; RUN: llc -aarch64-sve-vector-bits-min=768  < %s | FileCheck %s -D#VBYTES=64  -check-prefixes=CHECK,VBITS_GE_256,VBITS_GE_512
; RUN: llc -aarch64-sve-vector-bits-min=896  < %s | FileCheck %s -D#VBYTES=64  -check-prefixes=CHECK,VBITS_GE_256,VBITS_GE_512
; RUN: llc -aarch64-sve-vector-bits-min=1024 < %s | FileCheck %s -D#VBYTES=128 -check-prefixes=CHECK,VBITS_GE_256,VBITS_GE_512,VBITS_GE_1024
; RUN: llc -aarch64-sve-vector-bits-min=1152 < %s | FileCheck %s -D#VBYTES=128 -check-prefixes=CHECK,VBITS_GE_256,VBITS_GE_512,VBITS_GE_1024
; RUN: llc -aarch64-sve-vector-bits-min=1280 < %s | FileCheck %s -D#VBYTES=128 -check-prefixes=CHECK,VBITS_GE_256,VBITS_GE_512,VBITS_GE_1024
; RUN: llc -aarch64-sve-vector-bits-min=1408 < %s | FileCheck %s -D#VBYTES=128 -check-prefixes=CHECK,VBITS_GE_256,VBITS_GE_512,VBITS_GE_1024
; RUN: llc -aarch64-sve-vector-bits-min=1536 < %s | FileCheck %s -D#VBYTES=128 -check-prefixes=CHECK,VBITS_GE_256,VBITS_GE_512,VBITS_GE_1024
; RUN: llc -aarch64-sve-vector-bits-min=1664 < %s | FileCheck %s -D#VBYTES=128 -check-prefixes=CHECK,VBITS_GE_256,VBITS_GE_512,VBITS_GE_1024
; RUN: llc -aarch64-sve-vector-bits-min=1792 < %s | FileCheck %s -D#VBYTES=128 -check-prefixes=CHECK,VBITS_GE_256,VBITS_GE_512,VBITS_GE_1024
; RUN: llc -aarch64-sve-vector-bits-min=1920 < %s | FileCheck %s -D#VBYTES=128 -check-prefixes=CHECK,VBITS_GE_256,VBITS_GE_512,VBITS_GE_1024
; RUN: llc -aarch64-sve-vector-bits-min=2048 < %s | FileCheck %s -D#VBYTES=256 -check-prefixes=CHECK,VBITS_GE_256,VBITS_GE_512,VBITS_GE_1024,VBITS_GE_2048

; VBYTES represents the useful byte size of a vector register from the code
; generator's point of view. It is clamped to power-of-2 values because
; only power-of-2 vector lengths are considered legal, regardless of the
; user specified vector length.

; This test only tests the legal types for a given vector width, as mulh nodes
; do not get generated for non-legal types.

target triple = "aarch64-unknown-linux-gnu"

;
; SMULH
;

; Don't use SVE for 64-bit vectors.
; FIXME: The codegen for the >=256 bits case can be improved.
define <8 x i8> @smulh_v8i8(<8 x i8> %op1, <8 x i8> %op2) #0 {
; VBITS_EQ_128-LABEL: smulh_v8i8:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    smull v0.8h, v0.8b, v1.8b
; VBITS_EQ_128-NEXT:    shrn v0.8b, v0.8h, #8
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_256-LABEL: smulh_v8i8:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    smull v0.8h, v0.8b, v1.8b
; VBITS_GE_256-NEXT:    ushr v1.8h, v0.8h, #8
; VBITS_GE_256-NEXT:    umov w8, v1.h[0]
; VBITS_GE_256-NEXT:    umov w9, v1.h[1]
; VBITS_GE_256-NEXT:    fmov s0, w8
; VBITS_GE_256-NEXT:    umov w8, v1.h[2]
; VBITS_GE_256-NEXT:    mov v0.b[1], w9
; VBITS_GE_256-NEXT:    mov v0.b[2], w8
; VBITS_GE_256-NEXT:    umov w8, v1.h[3]
; VBITS_GE_256-NEXT:    mov v0.b[3], w8
; VBITS_GE_256-NEXT:    umov w8, v1.h[4]
; VBITS_GE_256-NEXT:    mov v0.b[4], w8
; VBITS_GE_256-NEXT:    umov w8, v1.h[5]
; VBITS_GE_256-NEXT:    mov v0.b[5], w8
; VBITS_GE_256-NEXT:    umov w8, v1.h[6]
; VBITS_GE_256-NEXT:    mov v0.b[6], w8
; VBITS_GE_256-NEXT:    umov w8, v1.h[7]
; VBITS_GE_256-NEXT:    mov v0.b[7], w8
; VBITS_GE_256-NEXT:    // kill: def $d0 killed $d0 killed $q0
; VBITS_GE_256-NEXT:    ret
  %insert = insertelement <8 x i16> undef, i16 8, i64 0
  %splat = shufflevector <8 x i16> %insert, <8 x i16> undef, <8 x i32> zeroinitializer
  %1 = sext <8 x i8> %op1 to <8 x i16>
  %2 = sext <8 x i8> %op2 to <8 x i16>
  %mul = mul <8 x i16> %1, %2
  %shr = lshr <8 x i16> %mul, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %res = trunc <8 x i16> %shr to <8 x i8>
  ret <8 x i8> %res
}

; Don't use SVE for 128-bit vectors.
define <16 x i8> @smulh_v16i8(<16 x i8> %op1, <16 x i8> %op2) #0 {
; CHECK-LABEL: smulh_v16i8:
; CHECK:       // %bb.0:
; CHECK-NEXT:    smull2 v2.8h, v0.16b, v1.16b
; CHECK-NEXT:    smull v0.8h, v0.8b, v1.8b
; CHECK-NEXT:    uzp2 v0.16b, v0.16b, v2.16b
; CHECK-NEXT:    ret
  %1 = sext <16 x i8> %op1 to <16 x i16>
  %2 = sext <16 x i8> %op2 to <16 x i16>
  %mul = mul <16 x i16> %1, %2
  %shr = lshr <16 x i16> %mul, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %res = trunc <16 x i16> %shr to <16 x i8>
  ret <16 x i8> %res
}

define void @smulh_v32i8(<32 x i8>* %a, <32 x i8>* %b) #0 {
; VBITS_EQ_128-LABEL: smulh_v32i8:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0]
; VBITS_EQ_128-NEXT:    ldp q2, q3, [x1]
; VBITS_EQ_128-NEXT:    smull v4.8h, v0.8b, v2.8b
; VBITS_EQ_128-NEXT:    smull2 v0.8h, v0.16b, v2.16b
; VBITS_EQ_128-NEXT:    smull v5.8h, v1.8b, v3.8b
; VBITS_EQ_128-NEXT:    smull2 v1.8h, v1.16b, v3.16b
; VBITS_EQ_128-NEXT:    shrn v2.8b, v4.8h, #8
; VBITS_EQ_128-NEXT:    shrn v3.8b, v5.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v2.16b, v0.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v3.16b, v1.8h, #8
; VBITS_EQ_128-NEXT:    stp q2, q3, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_256-LABEL: smulh_v32i8:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ptrue p0.b, vl32
; VBITS_GE_256-NEXT:    ld1b { z0.b }, p0/z, [x0]
; VBITS_GE_256-NEXT:    ld1b { z1.b }, p0/z, [x1]
; VBITS_GE_256-NEXT:    smulh z0.b, p0/m, z0.b, z1.b
; VBITS_GE_256-NEXT:    st1b { z0.b }, p0, [x0]
; VBITS_GE_256-NEXT:    ret
  %op1 = load <32 x i8>, <32 x i8>* %a
  %op2 = load <32 x i8>, <32 x i8>* %b
  %1 = sext <32 x i8> %op1 to <32 x i16>
  %2 = sext <32 x i8> %op2 to <32 x i16>
  %mul = mul <32 x i16> %1, %2
  %shr = lshr <32 x i16> %mul, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %res = trunc <32 x i16> %shr to <32 x i8>
  store <32 x i8> %res, <32 x i8>* %a
  ret void
}

define void @smulh_v64i8(<64 x i8>* %a, <64 x i8>* %b) #0 {
; VBITS_EQ_128-LABEL: smulh_v64i8:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0, #32]
; VBITS_EQ_128-NEXT:    ldp q2, q5, [x1, #32]
; VBITS_EQ_128-NEXT:    smull2 v6.8h, v0.16b, v2.16b
; VBITS_EQ_128-NEXT:    smull v0.8h, v0.8b, v2.8b
; VBITS_EQ_128-NEXT:    ldp q3, q4, [x0]
; VBITS_EQ_128-NEXT:    smull2 v7.8h, v1.16b, v5.16b
; VBITS_EQ_128-NEXT:    smull v1.8h, v1.8b, v5.8b
; VBITS_EQ_128-NEXT:    shrn v0.8b, v0.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v0.16b, v6.8h, #8
; VBITS_EQ_128-NEXT:    ldp q2, q5, [x1]
; VBITS_EQ_128-NEXT:    shrn v1.8b, v1.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v1.16b, v7.8h, #8
; VBITS_EQ_128-NEXT:    smull2 v16.8h, v3.16b, v2.16b
; VBITS_EQ_128-NEXT:    smull v2.8h, v3.8b, v2.8b
; VBITS_EQ_128-NEXT:    stp q0, q1, [x0, #32]
; VBITS_EQ_128-NEXT:    smull2 v3.8h, v4.16b, v5.16b
; VBITS_EQ_128-NEXT:    smull v4.8h, v4.8b, v5.8b
; VBITS_EQ_128-NEXT:    shrn v2.8b, v2.8h, #8
; VBITS_EQ_128-NEXT:    shrn v4.8b, v4.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v2.16b, v16.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v4.16b, v3.8h, #8
; VBITS_EQ_128-NEXT:    stp q2, q4, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_1024-LABEL: smulh_v64i8:
; VBITS_GE_1024:       // %bb.0:
; VBITS_GE_1024-NEXT:    ptrue p0.b, vl64
; VBITS_GE_1024-NEXT:    ld1b { z0.b }, p0/z, [x0]
; VBITS_GE_1024-NEXT:    ld1b { z1.b }, p0/z, [x1]
; VBITS_GE_1024-NEXT:    smulh z0.b, p0/m, z0.b, z1.b
; VBITS_GE_1024-NEXT:    st1b { z0.b }, p0, [x0]
; VBITS_GE_1024-NEXT:    ret
  %op1 = load <64 x i8>, <64 x i8>* %a
  %op2 = load <64 x i8>, <64 x i8>* %b
  %insert = insertelement <64 x i16> undef, i16 8, i64 0
  %splat = shufflevector <64 x i16> %insert, <64 x i16> undef, <64 x i32> zeroinitializer
  %1 = sext <64 x i8> %op1 to <64 x i16>
  %2 = sext <64 x i8> %op2 to <64 x i16>
  %mul = mul <64 x i16> %1, %2
  %shr = lshr <64 x i16> %mul, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %res = trunc <64 x i16> %shr to <64 x i8>
  store <64 x i8> %res, <64 x i8>* %a
  ret void
}

define void @smulh_v128i8(<128 x i8>* %a, <128 x i8>* %b) #0 {
; VBITS_EQ_128-LABEL: smulh_v128i8:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0, #96]
; VBITS_EQ_128-NEXT:    ldp q2, q5, [x1, #96]
; VBITS_EQ_128-NEXT:    smull2 v6.8h, v0.16b, v2.16b
; VBITS_EQ_128-NEXT:    smull v0.8h, v0.8b, v2.8b
; VBITS_EQ_128-NEXT:    ldp q3, q4, [x0, #64]
; VBITS_EQ_128-NEXT:    smull2 v7.8h, v1.16b, v5.16b
; VBITS_EQ_128-NEXT:    smull v1.8h, v1.8b, v5.8b
; VBITS_EQ_128-NEXT:    shrn v0.8b, v0.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v0.16b, v6.8h, #8
; VBITS_EQ_128-NEXT:    ldp q2, q16, [x1, #64]
; VBITS_EQ_128-NEXT:    shrn v1.8b, v1.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v1.16b, v7.8h, #8
; VBITS_EQ_128-NEXT:    smull2 v17.8h, v3.16b, v2.16b
; VBITS_EQ_128-NEXT:    smull v2.8h, v3.8b, v2.8b
; VBITS_EQ_128-NEXT:    ldp q5, q18, [x0, #32]
; VBITS_EQ_128-NEXT:    smull2 v19.8h, v4.16b, v16.16b
; VBITS_EQ_128-NEXT:    smull v4.8h, v4.8b, v16.8b
; VBITS_EQ_128-NEXT:    shrn v2.8b, v2.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v2.16b, v17.8h, #8
; VBITS_EQ_128-NEXT:    ldp q3, q20, [x1, #32]
; VBITS_EQ_128-NEXT:    shrn v4.8b, v4.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v4.16b, v19.8h, #8
; VBITS_EQ_128-NEXT:    smull2 v21.8h, v5.16b, v3.16b
; VBITS_EQ_128-NEXT:    smull v3.8h, v5.8b, v3.8b
; VBITS_EQ_128-NEXT:    ldp q16, q22, [x0]
; VBITS_EQ_128-NEXT:    smull2 v23.8h, v18.16b, v20.16b
; VBITS_EQ_128-NEXT:    smull v18.8h, v18.8b, v20.8b
; VBITS_EQ_128-NEXT:    shrn v3.8b, v3.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v3.16b, v21.8h, #8
; VBITS_EQ_128-NEXT:    ldp q5, q24, [x1]
; VBITS_EQ_128-NEXT:    shrn v18.8b, v18.8h, #8
; VBITS_EQ_128-NEXT:    stp q2, q4, [x0, #64]
; VBITS_EQ_128-NEXT:    stp q0, q1, [x0, #96]
; VBITS_EQ_128-NEXT:    shrn2 v18.16b, v23.8h, #8
; VBITS_EQ_128-NEXT:    smull v20.8h, v16.8b, v5.8b
; VBITS_EQ_128-NEXT:    smull2 v5.8h, v16.16b, v5.16b
; VBITS_EQ_128-NEXT:    stp q3, q18, [x0, #32]
; VBITS_EQ_128-NEXT:    smull v25.8h, v22.8b, v24.8b
; VBITS_EQ_128-NEXT:    smull2 v16.8h, v22.16b, v24.16b
; VBITS_EQ_128-NEXT:    shrn v20.8b, v20.8h, #8
; VBITS_EQ_128-NEXT:    shrn v22.8b, v25.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v20.16b, v5.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v22.16b, v16.8h, #8
; VBITS_EQ_128-NEXT:    stp q20, q22, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_1024-LABEL: smulh_v128i8:
; VBITS_GE_1024:       // %bb.0:
; VBITS_GE_1024-NEXT:    ptrue p0.b, vl128
; VBITS_GE_1024-NEXT:    ld1b { z0.b }, p0/z, [x0]
; VBITS_GE_1024-NEXT:    ld1b { z1.b }, p0/z, [x1]
; VBITS_GE_1024-NEXT:    smulh z0.b, p0/m, z0.b, z1.b
; VBITS_GE_1024-NEXT:    st1b { z0.b }, p0, [x0]
; VBITS_GE_1024-NEXT:    ret

  %op1 = load <128 x i8>, <128 x i8>* %a
  %op2 = load <128 x i8>, <128 x i8>* %b
  %1 = sext <128 x i8> %op1 to <128 x i16>
  %2 = sext <128 x i8> %op2 to <128 x i16>
  %mul = mul <128 x i16> %1, %2
  %shr = lshr <128 x i16> %mul, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %res = trunc <128 x i16> %shr to <128 x i8>
  store <128 x i8> %res, <128 x i8>* %a
  ret void
}

define void @smulh_v256i8(<256 x i8>* %a, <256 x i8>* %b) #0 {
; VBITS_EQ_128-LABEL: smulh_v256i8:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    sub sp, sp, #96
; VBITS_EQ_128-NEXT:    .cfi_def_cfa_offset 96
; VBITS_EQ_128-NEXT:    stp d15, d14, [sp, #32] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d13, d12, [sp, #48] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d11, d10, [sp, #64] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d9, d8, [sp, #80] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_offset b8, -8
; VBITS_EQ_128-NEXT:    .cfi_offset b9, -16
; VBITS_EQ_128-NEXT:    .cfi_offset b10, -24
; VBITS_EQ_128-NEXT:    .cfi_offset b11, -32
; VBITS_EQ_128-NEXT:    .cfi_offset b12, -40
; VBITS_EQ_128-NEXT:    .cfi_offset b13, -48
; VBITS_EQ_128-NEXT:    .cfi_offset b14, -56
; VBITS_EQ_128-NEXT:    .cfi_offset b15, -64
; VBITS_EQ_128-NEXT:    ldp q2, q1, [x0, #224]
; VBITS_EQ_128-NEXT:    ldp q6, q3, [x1, #224]
; VBITS_EQ_128-NEXT:    ldp q7, q5, [x0, #192]
; VBITS_EQ_128-NEXT:    smull2 v0.8h, v1.16b, v3.16b
; VBITS_EQ_128-NEXT:    smull v4.8h, v1.8b, v3.8b
; VBITS_EQ_128-NEXT:    str q0, [sp, #16] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    ldp q16, q3, [x1, #192]
; VBITS_EQ_128-NEXT:    smull2 v0.8h, v2.16b, v6.16b
; VBITS_EQ_128-NEXT:    shrn v4.8b, v4.8h, #8
; VBITS_EQ_128-NEXT:    smull v6.8h, v2.8b, v6.8b
; VBITS_EQ_128-NEXT:    str q0, [sp] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    smull2 v2.8h, v5.16b, v3.16b
; VBITS_EQ_128-NEXT:    shrn v6.8b, v6.8h, #8
; VBITS_EQ_128-NEXT:    smull v5.8h, v5.8b, v3.8b
; VBITS_EQ_128-NEXT:    ldp q19, q18, [x0, #160]
; VBITS_EQ_128-NEXT:    smull2 v3.8h, v7.16b, v16.16b
; VBITS_EQ_128-NEXT:    smull v7.8h, v7.8b, v16.8b
; VBITS_EQ_128-NEXT:    shrn v5.8b, v5.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v5.16b, v2.8h, #8
; VBITS_EQ_128-NEXT:    ldp q16, q17, [x1, #160]
; VBITS_EQ_128-NEXT:    shrn v7.8b, v7.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v7.16b, v3.8h, #8
; VBITS_EQ_128-NEXT:    smull2 v31.8h, v19.16b, v16.16b
; VBITS_EQ_128-NEXT:    smull v9.8h, v19.8b, v16.8b
; VBITS_EQ_128-NEXT:    smull2 v21.8h, v18.16b, v17.16b
; VBITS_EQ_128-NEXT:    smull v30.8h, v18.8b, v17.8b
; VBITS_EQ_128-NEXT:    ldp q22, q17, [x0, #128]
; VBITS_EQ_128-NEXT:    shrn v9.8b, v9.8h, #8
; VBITS_EQ_128-NEXT:    shrn v30.8b, v30.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v9.16b, v31.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v30.16b, v21.8h, #8
; VBITS_EQ_128-NEXT:    ldp q19, q20, [x1, #128]
; VBITS_EQ_128-NEXT:    smull2 v16.8h, v17.16b, v20.16b
; VBITS_EQ_128-NEXT:    ldr q21, [sp, #16] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    smull v18.8h, v17.8b, v20.8b
; VBITS_EQ_128-NEXT:    ldp q24, q20, [x0, #96]
; VBITS_EQ_128-NEXT:    smull2 v17.8h, v22.16b, v19.16b
; VBITS_EQ_128-NEXT:    shrn2 v4.16b, v21.8h, #8
; VBITS_EQ_128-NEXT:    smull v19.8h, v22.8b, v19.8b
; VBITS_EQ_128-NEXT:    shrn v2.8b, v18.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v2.16b, v16.8h, #8
; VBITS_EQ_128-NEXT:    ldp q22, q23, [x1, #96]
; VBITS_EQ_128-NEXT:    shrn v3.8b, v19.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v3.16b, v17.8h, #8
; VBITS_EQ_128-NEXT:    smull2 v12.8h, v24.16b, v22.16b
; VBITS_EQ_128-NEXT:    smull v13.8h, v24.8b, v22.8b
; VBITS_EQ_128-NEXT:    smull2 v10.8h, v20.16b, v23.16b
; VBITS_EQ_128-NEXT:    ldr q21, [sp] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    smull v11.8h, v20.8b, v23.8b
; VBITS_EQ_128-NEXT:    ldp q26, q23, [x0, #64]
; VBITS_EQ_128-NEXT:    shrn2 v6.16b, v21.8h, #8
; VBITS_EQ_128-NEXT:    ldp q24, q25, [x1, #64]
; VBITS_EQ_128-NEXT:    smull2 v22.8h, v26.16b, v24.16b
; VBITS_EQ_128-NEXT:    smull v24.8h, v26.8b, v24.8b
; VBITS_EQ_128-NEXT:    smull2 v20.8h, v23.16b, v25.16b
; VBITS_EQ_128-NEXT:    smull v23.8h, v23.8b, v25.8b
; VBITS_EQ_128-NEXT:    ldp q28, q25, [x0, #32]
; VBITS_EQ_128-NEXT:    ldp q26, q27, [x1, #32]
; VBITS_EQ_128-NEXT:    smull2 v15.8h, v28.16b, v26.16b
; VBITS_EQ_128-NEXT:    smull v1.8h, v28.8b, v26.8b
; VBITS_EQ_128-NEXT:    smull2 v14.8h, v25.16b, v27.16b
; VBITS_EQ_128-NEXT:    smull v8.8h, v25.8b, v27.8b
; VBITS_EQ_128-NEXT:    ldp q0, q27, [x0]
; VBITS_EQ_128-NEXT:    shrn v8.8b, v8.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v8.16b, v14.8h, #8
; VBITS_EQ_128-NEXT:    ldp q28, q29, [x1]
; VBITS_EQ_128-NEXT:    stp q3, q2, [x0, #128]
; VBITS_EQ_128-NEXT:    shrn v2.8b, v23.8h, #8
; VBITS_EQ_128-NEXT:    stp q9, q30, [x0, #160]
; VBITS_EQ_128-NEXT:    shrn v3.8b, v24.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v2.16b, v20.8h, #8
; VBITS_EQ_128-NEXT:    stp q7, q5, [x0, #192]
; VBITS_EQ_128-NEXT:    smull2 v26.8h, v0.16b, v28.16b
; VBITS_EQ_128-NEXT:    shrn2 v3.16b, v22.8h, #8
; VBITS_EQ_128-NEXT:    smull v28.8h, v0.8b, v28.8b
; VBITS_EQ_128-NEXT:    stp q6, q4, [x0, #224]
; VBITS_EQ_128-NEXT:    smull2 v25.8h, v27.16b, v29.16b
; VBITS_EQ_128-NEXT:    stp q3, q2, [x0, #64]
; VBITS_EQ_128-NEXT:    smull v27.8h, v27.8b, v29.8b
; VBITS_EQ_128-NEXT:    shrn v29.8b, v1.8h, #8
; VBITS_EQ_128-NEXT:    shrn v0.8b, v13.8h, #8
; VBITS_EQ_128-NEXT:    shrn v1.8b, v11.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v29.16b, v15.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v0.16b, v12.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v1.16b, v10.8h, #8
; VBITS_EQ_128-NEXT:    stp q29, q8, [x0, #32]
; VBITS_EQ_128-NEXT:    stp q0, q1, [x0, #96]
; VBITS_EQ_128-NEXT:    shrn v0.8b, v27.8h, #8
; VBITS_EQ_128-NEXT:    shrn v1.8b, v28.8h, #8
; VBITS_EQ_128-NEXT:    ldp d9, d8, [sp, #80] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    shrn2 v0.16b, v25.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v1.16b, v26.8h, #8
; VBITS_EQ_128-NEXT:    ldp d11, d10, [sp, #64] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    stp q1, q0, [x0]
; VBITS_EQ_128-NEXT:    ldp d13, d12, [sp, #48] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d15, d14, [sp, #32] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add sp, sp, #96
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_2048-LABEL: smulh_v256i8:
; VBITS_GE_2048:       // %bb.0:
; VBITS_GE_2048-NEXT:    ptrue p0.b, vl256
; VBITS_GE_2048-NEXT:    ld1b { z0.b }, p0/z, [x0]
; VBITS_GE_2048-NEXT:    ld1b { z1.b }, p0/z, [x1]
; VBITS_GE_2048-NEXT:    smulh z0.b, p0/m, z0.b, z1.b
; VBITS_GE_2048-NEXT:    st1b { z0.b }, p0, [x0]
; VBITS_GE_2048-NEXT:    ret
  %op1 = load <256 x i8>, <256 x i8>* %a
  %op2 = load <256 x i8>, <256 x i8>* %b
  %1 = sext <256 x i8> %op1 to <256 x i16>
  %2 = sext <256 x i8> %op2 to <256 x i16>
  %mul = mul <256 x i16> %1, %2
  %shr = lshr <256 x i16> %mul, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %res = trunc <256 x i16> %shr to <256 x i8>
  store <256 x i8> %res, <256 x i8>* %a
  ret void
}

; Don't use SVE for 64-bit vectors.
; FIXME: The codegen for the >=256 bits case can be improved.
define <4 x i16> @smulh_v4i16(<4 x i16> %op1, <4 x i16> %op2) #0 {
; VBITS_EQ_128-LABEL: smulh_v4i16:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    smull v0.4s, v0.4h, v1.4h
; VBITS_EQ_128-NEXT:    shrn v0.4h, v0.4s, #16
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_256-LABEL: smulh_v4i16:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    smull v0.4s, v0.4h, v1.4h
; VBITS_GE_256-NEXT:    ushr v1.4s, v0.4s, #16
; VBITS_GE_256-NEXT:    mov w8, v1.s[1]
; VBITS_GE_256-NEXT:    mov w9, v1.s[2]
; VBITS_GE_256-NEXT:    mov v0.16b, v1.16b
; VBITS_GE_256-NEXT:    mov v0.h[1], w8
; VBITS_GE_256-NEXT:    mov w8, v1.s[3]
; VBITS_GE_256-NEXT:    mov v0.h[2], w9
; VBITS_GE_256-NEXT:    mov v0.h[3], w8
; VBITS_GE_256-NEXT:    // kill: def $d0 killed $d0 killed $q0
; VBITS_GE_256-NEXT:    ret
  %1 = sext <4 x i16> %op1 to <4 x i32>
  %2 = sext <4 x i16> %op2 to <4 x i32>
  %mul = mul <4 x i32> %1, %2
  %shr = lshr <4 x i32> %mul, <i32 16, i32 16, i32 16, i32 16>
  %res = trunc <4 x i32> %shr to <4 x i16>
  ret <4 x i16> %res
}

; Don't use SVE for 128-bit vectors.
define <8 x i16> @smulh_v8i16(<8 x i16> %op1, <8 x i16> %op2) #0 {
; CHECK-LABEL: smulh_v8i16:
; CHECK:       // %bb.0:
; CHECK-NEXT:    smull2 v2.4s, v0.8h, v1.8h
; CHECK-NEXT:    smull v0.4s, v0.4h, v1.4h
; CHECK-NEXT:    uzp2 v0.8h, v0.8h, v2.8h
; CHECK-NEXT:    ret
  %1 = sext <8 x i16> %op1 to <8 x i32>
  %2 = sext <8 x i16> %op2 to <8 x i32>
  %mul = mul <8 x i32> %1, %2
  %shr = lshr <8 x i32> %mul, <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %res = trunc <8 x i32> %shr to <8 x i16>
  ret <8 x i16> %res
}

define void @smulh_v16i16(<16 x i16>* %a, <16 x i16>* %b) #0 {
; VBITS_EQ_128-LABEL: smulh_v16i16:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0]
; VBITS_EQ_128-NEXT:    ldp q2, q3, [x1]
; VBITS_EQ_128-NEXT:    smull v4.4s, v0.4h, v2.4h
; VBITS_EQ_128-NEXT:    smull2 v0.4s, v0.8h, v2.8h
; VBITS_EQ_128-NEXT:    smull v5.4s, v1.4h, v3.4h
; VBITS_EQ_128-NEXT:    smull2 v1.4s, v1.8h, v3.8h
; VBITS_EQ_128-NEXT:    shrn v2.4h, v4.4s, #16
; VBITS_EQ_128-NEXT:    shrn v3.4h, v5.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v2.8h, v0.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v3.8h, v1.4s, #16
; VBITS_EQ_128-NEXT:    stp q2, q3, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_256-LABEL: smulh_v16i16:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ptrue p0.h, vl16
; VBITS_GE_256-NEXT:    ld1h { z0.h }, p0/z, [x0]
; VBITS_GE_256-NEXT:    ld1h { z1.h }, p0/z, [x1]
; VBITS_GE_256-NEXT:    smulh z0.h, p0/m, z0.h, z1.h
; VBITS_GE_256-NEXT:    st1h { z0.h }, p0, [x0]
; VBITS_GE_256-NEXT:    ret
  %op1 = load <16 x i16>, <16 x i16>* %a
  %op2 = load <16 x i16>, <16 x i16>* %b
  %1 = sext <16 x i16> %op1 to <16 x i32>
  %2 = sext <16 x i16> %op2 to <16 x i32>
  %mul = mul <16 x i32> %1, %2
  %shr = lshr <16 x i32> %mul, <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %res = trunc <16 x i32> %shr to <16 x i16>
  store <16 x i16> %res, <16 x i16>* %a
  ret void
}

define void @smulh_v32i16(<32 x i16>* %a, <32 x i16>* %b) #0 {
; VBITS_EQ_128-LABEL: smulh_v32i16:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0, #32]
; VBITS_EQ_128-NEXT:    ldp q2, q5, [x1, #32]
; VBITS_EQ_128-NEXT:    smull2 v6.4s, v0.8h, v2.8h
; VBITS_EQ_128-NEXT:    smull v0.4s, v0.4h, v2.4h
; VBITS_EQ_128-NEXT:    ldp q3, q4, [x0]
; VBITS_EQ_128-NEXT:    smull2 v7.4s, v1.8h, v5.8h
; VBITS_EQ_128-NEXT:    smull v1.4s, v1.4h, v5.4h
; VBITS_EQ_128-NEXT:    shrn v0.4h, v0.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v0.8h, v6.4s, #16
; VBITS_EQ_128-NEXT:    ldp q2, q5, [x1]
; VBITS_EQ_128-NEXT:    shrn v1.4h, v1.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v1.8h, v7.4s, #16
; VBITS_EQ_128-NEXT:    smull2 v16.4s, v3.8h, v2.8h
; VBITS_EQ_128-NEXT:    smull v2.4s, v3.4h, v2.4h
; VBITS_EQ_128-NEXT:    stp q0, q1, [x0, #32]
; VBITS_EQ_128-NEXT:    smull2 v3.4s, v4.8h, v5.8h
; VBITS_EQ_128-NEXT:    smull v4.4s, v4.4h, v5.4h
; VBITS_EQ_128-NEXT:    shrn v2.4h, v2.4s, #16
; VBITS_EQ_128-NEXT:    shrn v4.4h, v4.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v2.8h, v16.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v4.8h, v3.4s, #16
; VBITS_EQ_128-NEXT:    stp q2, q4, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_1024-LABEL: smulh_v32i16:
; VBITS_GE_1024:       // %bb.0:
; VBITS_GE_1024-NEXT:    ptrue p0.h, vl32
; VBITS_GE_1024-NEXT:    ld1h { z0.h }, p0/z, [x0]
; VBITS_GE_1024-NEXT:    ld1h { z1.h }, p0/z, [x1]
; VBITS_GE_1024-NEXT:    smulh z0.h, p0/m, z0.h, z1.h
; VBITS_GE_1024-NEXT:    st1h { z0.h }, p0, [x0]
; VBITS_GE_1024-NEXT:    ret
  %op1 = load <32 x i16>, <32 x i16>* %a
  %op2 = load <32 x i16>, <32 x i16>* %b
  %1 = sext <32 x i16> %op1 to <32 x i32>
  %2 = sext <32 x i16> %op2 to <32 x i32>
  %mul = mul <32 x i32> %1, %2
  %shr = lshr <32 x i32> %mul, <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %res = trunc <32 x i32> %shr to <32 x i16>
  store <32 x i16> %res, <32 x i16>* %a
  ret void
}

define void @smulh_v64i16(<64 x i16>* %a, <64 x i16>* %b) #0 {
; VBITS_EQ_128-LABEL: smulh_v64i16:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0, #96]
; VBITS_EQ_128-NEXT:    ldp q2, q5, [x1, #96]
; VBITS_EQ_128-NEXT:    smull2 v6.4s, v0.8h, v2.8h
; VBITS_EQ_128-NEXT:    smull v0.4s, v0.4h, v2.4h
; VBITS_EQ_128-NEXT:    ldp q3, q4, [x0, #64]
; VBITS_EQ_128-NEXT:    smull2 v7.4s, v1.8h, v5.8h
; VBITS_EQ_128-NEXT:    smull v1.4s, v1.4h, v5.4h
; VBITS_EQ_128-NEXT:    shrn v0.4h, v0.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v0.8h, v6.4s, #16
; VBITS_EQ_128-NEXT:    ldp q2, q16, [x1, #64]
; VBITS_EQ_128-NEXT:    shrn v1.4h, v1.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v1.8h, v7.4s, #16
; VBITS_EQ_128-NEXT:    smull2 v17.4s, v3.8h, v2.8h
; VBITS_EQ_128-NEXT:    smull v2.4s, v3.4h, v2.4h
; VBITS_EQ_128-NEXT:    ldp q5, q18, [x0, #32]
; VBITS_EQ_128-NEXT:    smull2 v19.4s, v4.8h, v16.8h
; VBITS_EQ_128-NEXT:    smull v4.4s, v4.4h, v16.4h
; VBITS_EQ_128-NEXT:    shrn v2.4h, v2.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v2.8h, v17.4s, #16
; VBITS_EQ_128-NEXT:    ldp q3, q20, [x1, #32]
; VBITS_EQ_128-NEXT:    shrn v4.4h, v4.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v4.8h, v19.4s, #16
; VBITS_EQ_128-NEXT:    smull2 v21.4s, v5.8h, v3.8h
; VBITS_EQ_128-NEXT:    smull v3.4s, v5.4h, v3.4h
; VBITS_EQ_128-NEXT:    ldp q16, q22, [x0]
; VBITS_EQ_128-NEXT:    smull2 v23.4s, v18.8h, v20.8h
; VBITS_EQ_128-NEXT:    smull v18.4s, v18.4h, v20.4h
; VBITS_EQ_128-NEXT:    shrn v3.4h, v3.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v3.8h, v21.4s, #16
; VBITS_EQ_128-NEXT:    ldp q5, q24, [x1]
; VBITS_EQ_128-NEXT:    shrn v18.4h, v18.4s, #16
; VBITS_EQ_128-NEXT:    stp q2, q4, [x0, #64]
; VBITS_EQ_128-NEXT:    stp q0, q1, [x0, #96]
; VBITS_EQ_128-NEXT:    shrn2 v18.8h, v23.4s, #16
; VBITS_EQ_128-NEXT:    smull v20.4s, v16.4h, v5.4h
; VBITS_EQ_128-NEXT:    smull2 v5.4s, v16.8h, v5.8h
; VBITS_EQ_128-NEXT:    stp q3, q18, [x0, #32]
; VBITS_EQ_128-NEXT:    smull v25.4s, v22.4h, v24.4h
; VBITS_EQ_128-NEXT:    smull2 v16.4s, v22.8h, v24.8h
; VBITS_EQ_128-NEXT:    shrn v20.4h, v20.4s, #16
; VBITS_EQ_128-NEXT:    shrn v22.4h, v25.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v20.8h, v5.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v22.8h, v16.4s, #16
; VBITS_EQ_128-NEXT:    stp q20, q22, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_1024-LABEL: smulh_v64i16:
; VBITS_GE_1024:       // %bb.0:
; VBITS_GE_1024-NEXT:    ptrue p0.h, vl64
; VBITS_GE_1024-NEXT:    ld1h { z0.h }, p0/z, [x0]
; VBITS_GE_1024-NEXT:    ld1h { z1.h }, p0/z, [x1]
; VBITS_GE_1024-NEXT:    smulh z0.h, p0/m, z0.h, z1.h
; VBITS_GE_1024-NEXT:    st1h { z0.h }, p0, [x0]
; VBITS_GE_1024-NEXT:    ret
  %op1 = load <64 x i16>, <64 x i16>* %a
  %op2 = load <64 x i16>, <64 x i16>* %b
  %1 = sext <64 x i16> %op1 to <64 x i32>
  %2 = sext <64 x i16> %op2 to <64 x i32>
  %mul = mul <64 x i32> %1, %2
  %shr = lshr <64 x i32> %mul, <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %res = trunc <64 x i32> %shr to <64 x i16>
  store <64 x i16> %res, <64 x i16>* %a
  ret void
}

define void @smulh_v128i16(<128 x i16>* %a, <128 x i16>* %b) #0 {
; VBITS_EQ_128-LABEL: smulh_v128i16:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    sub sp, sp, #96
; VBITS_EQ_128-NEXT:    .cfi_def_cfa_offset 96
; VBITS_EQ_128-NEXT:    stp d15, d14, [sp, #32] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d13, d12, [sp, #48] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d11, d10, [sp, #64] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d9, d8, [sp, #80] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_offset b8, -8
; VBITS_EQ_128-NEXT:    .cfi_offset b9, -16
; VBITS_EQ_128-NEXT:    .cfi_offset b10, -24
; VBITS_EQ_128-NEXT:    .cfi_offset b11, -32
; VBITS_EQ_128-NEXT:    .cfi_offset b12, -40
; VBITS_EQ_128-NEXT:    .cfi_offset b13, -48
; VBITS_EQ_128-NEXT:    .cfi_offset b14, -56
; VBITS_EQ_128-NEXT:    .cfi_offset b15, -64
; VBITS_EQ_128-NEXT:    ldp q2, q1, [x0, #224]
; VBITS_EQ_128-NEXT:    ldp q6, q3, [x1, #224]
; VBITS_EQ_128-NEXT:    ldp q7, q5, [x0, #192]
; VBITS_EQ_128-NEXT:    smull2 v0.4s, v1.8h, v3.8h
; VBITS_EQ_128-NEXT:    smull v4.4s, v1.4h, v3.4h
; VBITS_EQ_128-NEXT:    str q0, [sp, #16] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    ldp q16, q3, [x1, #192]
; VBITS_EQ_128-NEXT:    smull2 v0.4s, v2.8h, v6.8h
; VBITS_EQ_128-NEXT:    shrn v4.4h, v4.4s, #16
; VBITS_EQ_128-NEXT:    smull v6.4s, v2.4h, v6.4h
; VBITS_EQ_128-NEXT:    str q0, [sp] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    smull2 v2.4s, v5.8h, v3.8h
; VBITS_EQ_128-NEXT:    shrn v6.4h, v6.4s, #16
; VBITS_EQ_128-NEXT:    smull v5.4s, v5.4h, v3.4h
; VBITS_EQ_128-NEXT:    ldp q19, q18, [x0, #160]
; VBITS_EQ_128-NEXT:    smull2 v3.4s, v7.8h, v16.8h
; VBITS_EQ_128-NEXT:    smull v7.4s, v7.4h, v16.4h
; VBITS_EQ_128-NEXT:    shrn v5.4h, v5.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v5.8h, v2.4s, #16
; VBITS_EQ_128-NEXT:    ldp q16, q17, [x1, #160]
; VBITS_EQ_128-NEXT:    shrn v7.4h, v7.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v7.8h, v3.4s, #16
; VBITS_EQ_128-NEXT:    smull2 v31.4s, v19.8h, v16.8h
; VBITS_EQ_128-NEXT:    smull v9.4s, v19.4h, v16.4h
; VBITS_EQ_128-NEXT:    smull2 v21.4s, v18.8h, v17.8h
; VBITS_EQ_128-NEXT:    smull v30.4s, v18.4h, v17.4h
; VBITS_EQ_128-NEXT:    ldp q22, q17, [x0, #128]
; VBITS_EQ_128-NEXT:    shrn v9.4h, v9.4s, #16
; VBITS_EQ_128-NEXT:    shrn v30.4h, v30.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v9.8h, v31.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v30.8h, v21.4s, #16
; VBITS_EQ_128-NEXT:    ldp q19, q20, [x1, #128]
; VBITS_EQ_128-NEXT:    smull2 v16.4s, v17.8h, v20.8h
; VBITS_EQ_128-NEXT:    ldr q21, [sp, #16] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    smull v18.4s, v17.4h, v20.4h
; VBITS_EQ_128-NEXT:    ldp q24, q20, [x0, #96]
; VBITS_EQ_128-NEXT:    smull2 v17.4s, v22.8h, v19.8h
; VBITS_EQ_128-NEXT:    shrn2 v4.8h, v21.4s, #16
; VBITS_EQ_128-NEXT:    smull v19.4s, v22.4h, v19.4h
; VBITS_EQ_128-NEXT:    shrn v2.4h, v18.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v2.8h, v16.4s, #16
; VBITS_EQ_128-NEXT:    ldp q22, q23, [x1, #96]
; VBITS_EQ_128-NEXT:    shrn v3.4h, v19.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v3.8h, v17.4s, #16
; VBITS_EQ_128-NEXT:    smull2 v12.4s, v24.8h, v22.8h
; VBITS_EQ_128-NEXT:    smull v13.4s, v24.4h, v22.4h
; VBITS_EQ_128-NEXT:    smull2 v10.4s, v20.8h, v23.8h
; VBITS_EQ_128-NEXT:    ldr q21, [sp] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    smull v11.4s, v20.4h, v23.4h
; VBITS_EQ_128-NEXT:    ldp q26, q23, [x0, #64]
; VBITS_EQ_128-NEXT:    shrn2 v6.8h, v21.4s, #16
; VBITS_EQ_128-NEXT:    ldp q24, q25, [x1, #64]
; VBITS_EQ_128-NEXT:    smull2 v22.4s, v26.8h, v24.8h
; VBITS_EQ_128-NEXT:    smull v24.4s, v26.4h, v24.4h
; VBITS_EQ_128-NEXT:    smull2 v20.4s, v23.8h, v25.8h
; VBITS_EQ_128-NEXT:    smull v23.4s, v23.4h, v25.4h
; VBITS_EQ_128-NEXT:    ldp q28, q25, [x0, #32]
; VBITS_EQ_128-NEXT:    ldp q26, q27, [x1, #32]
; VBITS_EQ_128-NEXT:    smull2 v15.4s, v28.8h, v26.8h
; VBITS_EQ_128-NEXT:    smull v1.4s, v28.4h, v26.4h
; VBITS_EQ_128-NEXT:    smull2 v14.4s, v25.8h, v27.8h
; VBITS_EQ_128-NEXT:    smull v8.4s, v25.4h, v27.4h
; VBITS_EQ_128-NEXT:    ldp q0, q27, [x0]
; VBITS_EQ_128-NEXT:    shrn v8.4h, v8.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v8.8h, v14.4s, #16
; VBITS_EQ_128-NEXT:    ldp q28, q29, [x1]
; VBITS_EQ_128-NEXT:    stp q3, q2, [x0, #128]
; VBITS_EQ_128-NEXT:    shrn v2.4h, v23.4s, #16
; VBITS_EQ_128-NEXT:    stp q9, q30, [x0, #160]
; VBITS_EQ_128-NEXT:    shrn v3.4h, v24.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v2.8h, v20.4s, #16
; VBITS_EQ_128-NEXT:    stp q7, q5, [x0, #192]
; VBITS_EQ_128-NEXT:    smull2 v26.4s, v0.8h, v28.8h
; VBITS_EQ_128-NEXT:    shrn2 v3.8h, v22.4s, #16
; VBITS_EQ_128-NEXT:    smull v28.4s, v0.4h, v28.4h
; VBITS_EQ_128-NEXT:    stp q6, q4, [x0, #224]
; VBITS_EQ_128-NEXT:    smull2 v25.4s, v27.8h, v29.8h
; VBITS_EQ_128-NEXT:    stp q3, q2, [x0, #64]
; VBITS_EQ_128-NEXT:    smull v27.4s, v27.4h, v29.4h
; VBITS_EQ_128-NEXT:    shrn v29.4h, v1.4s, #16
; VBITS_EQ_128-NEXT:    shrn v0.4h, v13.4s, #16
; VBITS_EQ_128-NEXT:    shrn v1.4h, v11.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v29.8h, v15.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v0.8h, v12.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v1.8h, v10.4s, #16
; VBITS_EQ_128-NEXT:    stp q29, q8, [x0, #32]
; VBITS_EQ_128-NEXT:    stp q0, q1, [x0, #96]
; VBITS_EQ_128-NEXT:    shrn v0.4h, v27.4s, #16
; VBITS_EQ_128-NEXT:    shrn v1.4h, v28.4s, #16
; VBITS_EQ_128-NEXT:    ldp d9, d8, [sp, #80] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    shrn2 v0.8h, v25.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v1.8h, v26.4s, #16
; VBITS_EQ_128-NEXT:    ldp d11, d10, [sp, #64] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    stp q1, q0, [x0]
; VBITS_EQ_128-NEXT:    ldp d13, d12, [sp, #48] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d15, d14, [sp, #32] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add sp, sp, #96
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_2048-LABEL: smulh_v128i16:
; VBITS_GE_2048:       // %bb.0:
; VBITS_GE_2048-NEXT:    ptrue p0.h, vl128
; VBITS_GE_2048-NEXT:    ld1h { z0.h }, p0/z, [x0]
; VBITS_GE_2048-NEXT:    ld1h { z1.h }, p0/z, [x1]
; VBITS_GE_2048-NEXT:    smulh z0.h, p0/m, z0.h, z1.h
; VBITS_GE_2048-NEXT:    st1h { z0.h }, p0, [x0]
; VBITS_GE_2048-NEXT:    ret
  %op1 = load <128 x i16>, <128 x i16>* %a
  %op2 = load <128 x i16>, <128 x i16>* %b
  %1 = sext <128 x i16> %op1 to <128 x i32>
  %2 = sext <128 x i16> %op2 to <128 x i32>
  %mul = mul <128 x i32> %1, %2
  %shr = lshr <128 x i32> %mul, <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %res = trunc <128 x i32> %shr to <128 x i16>
  store <128 x i16> %res, <128 x i16>* %a
  ret void
}

; Vector i64 multiplications are not legal for NEON so use SVE when available.
define <2 x i32> @smulh_v2i32(<2 x i32> %op1, <2 x i32> %op2) #0 {
; CHECK-LABEL: smulh_v2i32:
; CHECK:       // %bb.0:
; CHECK-NEXT:    sshll v0.2d, v0.2s, #0
; CHECK-NEXT:    ptrue p0.d, vl2
; CHECK-NEXT:    sshll v1.2d, v1.2s, #0
; CHECK-NEXT:    mul z0.d, p0/m, z0.d, z1.d
; CHECK-NEXT:    shrn v0.2s, v0.2d, #32
; CHECK-NEXT:    ret


  %1 = sext <2 x i32> %op1 to <2 x i64>
  %2 = sext <2 x i32> %op2 to <2 x i64>
  %mul = mul <2 x i64> %1, %2
  %shr = lshr <2 x i64> %mul, <i64 32, i64 32>
  %res = trunc <2 x i64> %shr to <2 x i32>
  ret <2 x i32> %res
}

; Don't use SVE for 128-bit vectors.
define <4 x i32> @smulh_v4i32(<4 x i32> %op1, <4 x i32> %op2) #0 {
; CHECK-LABEL: smulh_v4i32:
; CHECK:       // %bb.0:
; CHECK-NEXT:    smull2 v2.2d, v0.4s, v1.4s
; CHECK-NEXT:    smull v0.2d, v0.2s, v1.2s
; CHECK-NEXT:    uzp2 v0.4s, v0.4s, v2.4s
; CHECK-NEXT:    ret
  %1 = sext <4 x i32> %op1 to <4 x i64>
  %2 = sext <4 x i32> %op2 to <4 x i64>
  %mul = mul <4 x i64> %1, %2
  %shr = lshr <4 x i64> %mul, <i64 32, i64 32, i64 32, i64 32>
  %res = trunc <4 x i64> %shr to <4 x i32>
  ret <4 x i32> %res
}

define void @smulh_v8i32(<8 x i32>* %a, <8 x i32>* %b) #0 {
; VBITS_EQ_128-LABEL: smulh_v8i32:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0]
; VBITS_EQ_128-NEXT:    ptrue p0.d, vl2
; VBITS_EQ_128-NEXT:    sshll v5.2d, v0.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v0.4s, #0
; VBITS_EQ_128-NEXT:    ldp q2, q3, [x1]
; VBITS_EQ_128-NEXT:    sshll v4.2d, v1.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v1.2d, v1.4s, #0
; VBITS_EQ_128-NEXT:    sshll v7.2d, v2.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v2.2d, v2.4s, #0
; VBITS_EQ_128-NEXT:    sshll v6.2d, v3.2s, #0
; VBITS_EQ_128-NEXT:    mul z5.d, p0/m, z5.d, z7.d
; VBITS_EQ_128-NEXT:    sshll2 v3.2d, v3.4s, #0
; VBITS_EQ_128-NEXT:    mul z0.d, p0/m, z0.d, z2.d
; VBITS_EQ_128-NEXT:    mul z4.d, p0/m, z4.d, z6.d
; VBITS_EQ_128-NEXT:    shrn v5.2s, v5.2d, #32
; VBITS_EQ_128-NEXT:    shrn v2.2s, v4.2d, #32
; VBITS_EQ_128-NEXT:    mul z1.d, p0/m, z1.d, z3.d
; VBITS_EQ_128-NEXT:    shrn2 v5.4s, v0.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v2.4s, v1.2d, #32
; VBITS_EQ_128-NEXT:    stp q5, q2, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_256-LABEL: smulh_v8i32:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ptrue p0.s, vl8
; VBITS_GE_256-NEXT:    ld1w { z0.s }, p0/z, [x0]
; VBITS_GE_256-NEXT:    ld1w { z1.s }, p0/z, [x1]
; VBITS_GE_256-NEXT:    smulh z0.s, p0/m, z0.s, z1.s
; VBITS_GE_256-NEXT:    st1w { z0.s }, p0, [x0]
; VBITS_GE_256-NEXT:    ret
  %op1 = load <8 x i32>, <8 x i32>* %a
  %op2 = load <8 x i32>, <8 x i32>* %b
  %1 = sext <8 x i32> %op1 to <8 x i64>
  %2 = sext <8 x i32> %op2 to <8 x i64>
  %mul = mul <8 x i64> %1, %2
  %shr = lshr <8 x i64> %mul,  <i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32>
  %res = trunc <8 x i64> %shr to <8 x i32>
  store <8 x i32> %res, <8 x i32>* %a
  ret void
}

define void @smulh_v16i32(<16 x i32>* %a, <16 x i32>* %b) #0 {
; VBITS_EQ_128-LABEL: smulh_v16i32:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q1, q2, [x0, #32]
; VBITS_EQ_128-NEXT:    ptrue p0.d, vl2
; VBITS_EQ_128-NEXT:    sshll v19.2d, v1.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v1.2d, v1.4s, #0
; VBITS_EQ_128-NEXT:    ldp q3, q4, [x0]
; VBITS_EQ_128-NEXT:    sshll v18.2d, v2.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v2.2d, v2.4s, #0
; VBITS_EQ_128-NEXT:    sshll v7.2d, v3.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v3.2d, v3.4s, #0
; VBITS_EQ_128-NEXT:    ldp q5, q6, [x1, #32]
; VBITS_EQ_128-NEXT:    sshll v0.2d, v4.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v4.2d, v4.4s, #0
; VBITS_EQ_128-NEXT:    sshll2 v21.2d, v5.4s, #0
; VBITS_EQ_128-NEXT:    sshll v5.2d, v5.2s, #0
; VBITS_EQ_128-NEXT:    ldp q16, q17, [x1]
; VBITS_EQ_128-NEXT:    sshll2 v22.2d, v6.4s, #0
; VBITS_EQ_128-NEXT:    mul z5.d, p0/m, z5.d, z19.d
; VBITS_EQ_128-NEXT:    sshll v6.2d, v6.2s, #0
; VBITS_EQ_128-NEXT:    mul z1.d, p0/m, z1.d, z21.d
; VBITS_EQ_128-NEXT:    shrn v5.2s, v5.2d, #32
; VBITS_EQ_128-NEXT:    mul z2.d, p0/m, z2.d, z22.d
; VBITS_EQ_128-NEXT:    sshll v19.2d, v16.2s, #0
; VBITS_EQ_128-NEXT:    mul z6.d, p0/m, z6.d, z18.d
; VBITS_EQ_128-NEXT:    sshll2 v16.2d, v16.4s, #0
; VBITS_EQ_128-NEXT:    sshll v20.2d, v17.2s, #0
; VBITS_EQ_128-NEXT:    mul z7.d, p0/m, z7.d, z19.d
; VBITS_EQ_128-NEXT:    sshll2 v17.2d, v17.4s, #0
; VBITS_EQ_128-NEXT:    mul z3.d, p0/m, z3.d, z16.d
; VBITS_EQ_128-NEXT:    mul z0.d, p0/m, z0.d, z20.d
; VBITS_EQ_128-NEXT:    shrn v6.2s, v6.2d, #32
; VBITS_EQ_128-NEXT:    shrn v7.2s, v7.2d, #32
; VBITS_EQ_128-NEXT:    shrn v0.2s, v0.2d, #32
; VBITS_EQ_128-NEXT:    mul z4.d, p0/m, z4.d, z17.d
; VBITS_EQ_128-NEXT:    shrn2 v5.4s, v1.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v6.4s, v2.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v7.4s, v3.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v0.4s, v4.2d, #32
; VBITS_EQ_128-NEXT:    stp q5, q6, [x0, #32]
; VBITS_EQ_128-NEXT:    stp q7, q0, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_1024-LABEL: smulh_v16i32:
; VBITS_GE_1024:       // %bb.0:
; VBITS_GE_1024-NEXT:    ptrue p0.s, vl16
; VBITS_GE_1024-NEXT:    ld1w { z0.s }, p0/z, [x0]
; VBITS_GE_1024-NEXT:    ld1w { z1.s }, p0/z, [x1]
; VBITS_GE_1024-NEXT:    smulh z0.s, p0/m, z0.s, z1.s
; VBITS_GE_1024-NEXT:    st1w { z0.s }, p0, [x0]
; VBITS_GE_1024-NEXT:    ret
  %op1 = load <16 x i32>, <16 x i32>* %a
  %op2 = load <16 x i32>, <16 x i32>* %b
  %1 = sext <16 x i32> %op1 to <16 x i64>
  %2 = sext <16 x i32> %op2 to <16 x i64>
  %mul = mul <16 x i64> %1, %2
  %shr = lshr <16 x i64> %mul, <i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32>
  %res = trunc <16 x i64> %shr to <16 x i32>
  store <16 x i32> %res, <16 x i32>* %a
  ret void
}

define void @smulh_v32i32(<32 x i32>* %a, <32 x i32>* %b) #0 {
; VBITS_EQ_128-LABEL: smulh_v32i32:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    str d10, [sp, #-32]! // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_def_cfa_offset 32
; VBITS_EQ_128-NEXT:    stp d9, d8, [sp, #16] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_offset b8, -8
; VBITS_EQ_128-NEXT:    .cfi_offset b9, -16
; VBITS_EQ_128-NEXT:    .cfi_offset b10, -32
; VBITS_EQ_128-NEXT:    ldp q17, q16, [x0, #64]
; VBITS_EQ_128-NEXT:    ptrue p0.d, vl2
; VBITS_EQ_128-NEXT:    sshll v27.2d, v17.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v29.2d, v17.4s, #0
; VBITS_EQ_128-NEXT:    ldp q23, q28, [x0, #96]
; VBITS_EQ_128-NEXT:    sshll v19.2d, v16.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v22.2d, v16.4s, #0
; VBITS_EQ_128-NEXT:    sshll v31.2d, v23.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v8.2d, v23.4s, #0
; VBITS_EQ_128-NEXT:    ldp q26, q25, [x1, #96]
; VBITS_EQ_128-NEXT:    sshll v30.2d, v28.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v28.2d, v28.4s, #0
; VBITS_EQ_128-NEXT:    sshll2 v9.2d, v26.4s, #0
; VBITS_EQ_128-NEXT:    sshll v26.2d, v26.2s, #0
; VBITS_EQ_128-NEXT:    ldp q24, q21, [x1, #64]
; VBITS_EQ_128-NEXT:    mul z26.d, p0/m, z26.d, z31.d
; VBITS_EQ_128-NEXT:    mul z8.d, p0/m, z8.d, z9.d
; VBITS_EQ_128-NEXT:    sshll2 v10.2d, v25.4s, #0
; VBITS_EQ_128-NEXT:    sshll v25.2d, v25.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v31.2d, v24.4s, #0
; VBITS_EQ_128-NEXT:    mul z28.d, p0/m, z28.d, z10.d
; VBITS_EQ_128-NEXT:    sshll v24.2d, v24.2s, #0
; VBITS_EQ_128-NEXT:    mul z25.d, p0/m, z25.d, z30.d
; VBITS_EQ_128-NEXT:    ldp q7, q5, [x0, #32]
; VBITS_EQ_128-NEXT:    mul z24.d, p0/m, z24.d, z27.d
; VBITS_EQ_128-NEXT:    mul z29.d, p0/m, z29.d, z31.d
; VBITS_EQ_128-NEXT:    sshll2 v30.2d, v21.4s, #0
; VBITS_EQ_128-NEXT:    sshll v21.2d, v21.2s, #0
; VBITS_EQ_128-NEXT:    sshll v6.2d, v7.2s, #0
; VBITS_EQ_128-NEXT:    mul z22.d, p0/m, z22.d, z30.d
; VBITS_EQ_128-NEXT:    mul z19.d, p0/m, z19.d, z21.d
; VBITS_EQ_128-NEXT:    ldp q20, q18, [x1, #32]
; VBITS_EQ_128-NEXT:    sshll v4.2d, v5.2s, #0
; VBITS_EQ_128-NEXT:    shrn v19.2s, v19.2d, #32
; VBITS_EQ_128-NEXT:    sshll2 v5.2d, v5.4s, #0
; VBITS_EQ_128-NEXT:    sshll2 v7.2d, v7.4s, #0
; VBITS_EQ_128-NEXT:    sshll2 v27.2d, v20.4s, #0
; VBITS_EQ_128-NEXT:    sshll v20.2d, v20.2s, #0
; VBITS_EQ_128-NEXT:    ldp q3, q1, [x0]
; VBITS_EQ_128-NEXT:    mul z6.d, p0/m, z6.d, z20.d
; VBITS_EQ_128-NEXT:    mul z7.d, p0/m, z7.d, z27.d
; VBITS_EQ_128-NEXT:    sshll2 v21.2d, v18.4s, #0
; VBITS_EQ_128-NEXT:    shrn v6.2s, v6.2d, #32
; VBITS_EQ_128-NEXT:    sshll v18.2d, v18.2s, #0
; VBITS_EQ_128-NEXT:    sshll v2.2d, v3.2s, #0
; VBITS_EQ_128-NEXT:    mul z5.d, p0/m, z5.d, z21.d
; VBITS_EQ_128-NEXT:    sshll2 v3.2d, v3.4s, #0
; VBITS_EQ_128-NEXT:    mul z4.d, p0/m, z4.d, z18.d
; VBITS_EQ_128-NEXT:    ldp q16, q17, [x1]
; VBITS_EQ_128-NEXT:    sshll v0.2d, v1.2s, #0
; VBITS_EQ_128-NEXT:    shrn v4.2s, v4.2d, #32
; VBITS_EQ_128-NEXT:    sshll2 v1.2d, v1.4s, #0
; VBITS_EQ_128-NEXT:    shrn v18.2s, v24.2d, #32
; VBITS_EQ_128-NEXT:    sshll v20.2d, v16.2s, #0
; VBITS_EQ_128-NEXT:    shrn2 v19.4s, v22.2d, #32
; VBITS_EQ_128-NEXT:    sshll2 v16.2d, v16.4s, #0
; VBITS_EQ_128-NEXT:    sshll v23.2d, v17.2s, #0
; VBITS_EQ_128-NEXT:    mul z2.d, p0/m, z2.d, z20.d
; VBITS_EQ_128-NEXT:    sshll2 v17.2d, v17.4s, #0
; VBITS_EQ_128-NEXT:    mul z3.d, p0/m, z3.d, z16.d
; VBITS_EQ_128-NEXT:    shrn v16.2s, v26.2d, #32
; VBITS_EQ_128-NEXT:    mul z0.d, p0/m, z0.d, z23.d
; VBITS_EQ_128-NEXT:    mul z1.d, p0/m, z1.d, z17.d
; VBITS_EQ_128-NEXT:    shrn v0.2s, v0.2d, #32
; VBITS_EQ_128-NEXT:    shrn v2.2s, v2.2d, #32
; VBITS_EQ_128-NEXT:    shrn v17.2s, v25.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v16.4s, v8.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v18.4s, v29.2d, #32
; VBITS_EQ_128-NEXT:    ldp d9, d8, [sp, #16] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    shrn2 v17.4s, v28.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v6.4s, v7.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v4.4s, v5.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v2.4s, v3.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v0.4s, v1.2d, #32
; VBITS_EQ_128-NEXT:    stp q18, q19, [x0, #64]
; VBITS_EQ_128-NEXT:    stp q6, q4, [x0, #32]
; VBITS_EQ_128-NEXT:    stp q2, q0, [x0]
; VBITS_EQ_128-NEXT:    stp q16, q17, [x0, #96]
; VBITS_EQ_128-NEXT:    ldr d10, [sp], #32 // 8-byte Folded Reload
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_1024-LABEL: smulh_v32i32:
; VBITS_GE_1024:       // %bb.0:
; VBITS_GE_1024-NEXT:    ptrue p0.s, vl32
; VBITS_GE_1024-NEXT:    ld1w { z0.s }, p0/z, [x0]
; VBITS_GE_1024-NEXT:    ld1w { z1.s }, p0/z, [x1]
; VBITS_GE_1024-NEXT:    smulh z0.s, p0/m, z0.s, z1.s
; VBITS_GE_1024-NEXT:    st1w { z0.s }, p0, [x0]
; VBITS_GE_1024-NEXT:    ret
  %op1 = load <32 x i32>, <32 x i32>* %a
  %op2 = load <32 x i32>, <32 x i32>* %b
  %1 = sext <32 x i32> %op1 to <32 x i64>
  %2 = sext <32 x i32> %op2 to <32 x i64>
  %mul = mul <32 x i64> %1, %2
  %shr = lshr <32 x i64> %mul, <i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32>
  %res = trunc <32 x i64> %shr to <32 x i32>
  store <32 x i32> %res, <32 x i32>* %a
  ret void
}

define void @smulh_v64i32(<64 x i32>* %a, <64 x i32>* %b) #0 {
; VBITS_EQ_128-LABEL: smulh_v64i32:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    stp d15, d14, [sp, #-80]! // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_def_cfa_offset 80
; VBITS_EQ_128-NEXT:    stp d13, d12, [sp, #16] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d11, d10, [sp, #32] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d9, d8, [sp, #48] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    str x29, [sp, #64] // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_offset w29, -16
; VBITS_EQ_128-NEXT:    .cfi_offset b8, -24
; VBITS_EQ_128-NEXT:    .cfi_offset b9, -32
; VBITS_EQ_128-NEXT:    .cfi_offset b10, -40
; VBITS_EQ_128-NEXT:    .cfi_offset b11, -48
; VBITS_EQ_128-NEXT:    .cfi_offset b12, -56
; VBITS_EQ_128-NEXT:    .cfi_offset b13, -64
; VBITS_EQ_128-NEXT:    .cfi_offset b14, -72
; VBITS_EQ_128-NEXT:    .cfi_offset b15, -80
; VBITS_EQ_128-NEXT:    addvl sp, sp, #-12
; VBITS_EQ_128-NEXT:    .cfi_escape 0x0f, 0x0e, 0x8f, 0x00, 0x11, 0xd0, 0x00, 0x22, 0x11, 0xe0, 0x00, 0x92, 0x2e, 0x00, 0x1e, 0x22 // sp + 80 + 96 * VG
; VBITS_EQ_128-NEXT:    .cfi_escape 0x0f, 0x0e, 0x8f, 0x00, 0x11, 0xa0, 0x01, 0x22, 0x11, 0xe0, 0x00, 0x92, 0x2e, 0x00, 0x1e, 0x22 // sp + 160 + 96 * VG
; VBITS_EQ_128-NEXT:    ldp q4, q5, [x0, #96]
; VBITS_EQ_128-NEXT:    ptrue p0.d, vl2
; VBITS_EQ_128-NEXT:    stp q5, q4, [sp, #-80]! // 32-byte Folded Spill
; VBITS_EQ_128-NEXT:    ldp q0, q2, [x0, #48]
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ldr q1, [x0, #32]
; VBITS_EQ_128-NEXT:    ldr q3, [x0, #80]
; VBITS_EQ_128-NEXT:    str q1, [sp, #64] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    sshll v1.2d, v0.2s, #0
; VBITS_EQ_128-NEXT:    stp q3, q2, [sp, #32] // 32-byte Folded Spill
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v0.4s, #0
; VBITS_EQ_128-NEXT:    str z1, [x8, #11, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    str z0, [x8, #10, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v2.4s, #0
; VBITS_EQ_128-NEXT:    str z0, [x8, #9, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v3.4s, #0
; VBITS_EQ_128-NEXT:    ldp q23, q26, [x0, #128]
; VBITS_EQ_128-NEXT:    str z0, [x8, #8, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v4.4s, #0
; VBITS_EQ_128-NEXT:    str z0, [x8, #7, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v5.4s, #0
; VBITS_EQ_128-NEXT:    ldp q25, q24, [x0, #160]
; VBITS_EQ_128-NEXT:    str z0, [x8, #6, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v23.4s, #0
; VBITS_EQ_128-NEXT:    sshll2 v1.2d, v26.4s, #0
; VBITS_EQ_128-NEXT:    str z0, [x8, #5, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    sshll2 v27.2d, v25.4s, #0
; VBITS_EQ_128-NEXT:    ldp q30, q0, [x0, #192]
; VBITS_EQ_128-NEXT:    str z1, [x8, #4, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    sshll2 v9.2d, v24.4s, #0
; VBITS_EQ_128-NEXT:    sshll2 v12.2d, v30.4s, #0
; VBITS_EQ_128-NEXT:    ldp q31, q1, [x0, #224]
; VBITS_EQ_128-NEXT:    sshll v11.2d, v0.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v8.2d, v0.4s, #0
; VBITS_EQ_128-NEXT:    sshll v10.2d, v31.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v15.2d, v31.4s, #0
; VBITS_EQ_128-NEXT:    ldp q29, q28, [x1, #224]
; VBITS_EQ_128-NEXT:    sshll2 v18.2d, v1.4s, #0
; VBITS_EQ_128-NEXT:    sshll v31.2d, v1.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v2.2d, v29.4s, #0
; VBITS_EQ_128-NEXT:    ldp q14, q0, [x1, #192]
; VBITS_EQ_128-NEXT:    sshll v1.2d, v28.2s, #0
; VBITS_EQ_128-NEXT:    sshll v20.2d, v0.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v19.2d, v0.4s, #0
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v28.4s, #0
; VBITS_EQ_128-NEXT:    mul z11.d, p0/m, z11.d, z20.d
; VBITS_EQ_128-NEXT:    ldp q21, q22, [x0]
; VBITS_EQ_128-NEXT:    mul z0.d, p0/m, z0.d, z18.d
; VBITS_EQ_128-NEXT:    sshll v18.2d, v29.2s, #0
; VBITS_EQ_128-NEXT:    sshll v20.2d, v14.2s, #0
; VBITS_EQ_128-NEXT:    ldp q4, q13, [x1, #160]
; VBITS_EQ_128-NEXT:    ldp q5, q6, [x1, #128]
; VBITS_EQ_128-NEXT:    ldp q7, q3, [x1, #96]
; VBITS_EQ_128-NEXT:    str z0, [x8, #3, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ldp q17, q16, [x1, #64]
; VBITS_EQ_128-NEXT:    movprfx z0, z31
; VBITS_EQ_128-NEXT:    mul z0.d, p0/m, z0.d, z1.d
; VBITS_EQ_128-NEXT:    str z0, [x8, #1, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    movprfx z0, z15
; VBITS_EQ_128-NEXT:    mul z0.d, p0/m, z0.d, z2.d
; VBITS_EQ_128-NEXT:    sshll v1.2d, v30.2s, #0
; VBITS_EQ_128-NEXT:    str z0, [x8, #2, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ldp q2, q29, [x1, #32]
; VBITS_EQ_128-NEXT:    movprfx z15, z10
; VBITS_EQ_128-NEXT:    mul z15.d, p0/m, z15.d, z18.d
; VBITS_EQ_128-NEXT:    movprfx z0, z8
; VBITS_EQ_128-NEXT:    mul z0.d, p0/m, z0.d, z19.d
; VBITS_EQ_128-NEXT:    str z0, [x8] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v14.4s, #0
; VBITS_EQ_128-NEXT:    ldp q19, q18, [x1]
; VBITS_EQ_128-NEXT:    movprfx z10, z12
; VBITS_EQ_128-NEXT:    mul z10.d, p0/m, z10.d, z0.d
; VBITS_EQ_128-NEXT:    movprfx z8, z1
; VBITS_EQ_128-NEXT:    mul z8.d, p0/m, z8.d, z20.d
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v13.4s, #0
; VBITS_EQ_128-NEXT:    sshll v12.2d, v24.2s, #0
; VBITS_EQ_128-NEXT:    sshll v1.2d, v13.2s, #0
; VBITS_EQ_128-NEXT:    mul z9.d, p0/m, z9.d, z0.d
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v4.4s, #0
; VBITS_EQ_128-NEXT:    mul z12.d, p0/m, z12.d, z1.d
; VBITS_EQ_128-NEXT:    sshll v1.2d, v4.2s, #0
; VBITS_EQ_128-NEXT:    mul z27.d, p0/m, z27.d, z0.d
; VBITS_EQ_128-NEXT:    sshll v20.2d, v25.2s, #0
; VBITS_EQ_128-NEXT:    movprfx z13, z20
; VBITS_EQ_128-NEXT:    mul z13.d, p0/m, z13.d, z1.d
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v6.4s, #0
; VBITS_EQ_128-NEXT:    sshll v1.2d, v6.2s, #0
; VBITS_EQ_128-NEXT:    ldr z6, [x8, #4, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    movprfx z14, z6
; VBITS_EQ_128-NEXT:    mul z14.d, p0/m, z14.d, z0.d
; VBITS_EQ_128-NEXT:    sshll v4.2d, v26.2s, #0
; VBITS_EQ_128-NEXT:    movprfx z30, z4
; VBITS_EQ_128-NEXT:    mul z30.d, p0/m, z30.d, z1.d
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v5.4s, #0
; VBITS_EQ_128-NEXT:    ldr z4, [x8, #5, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    sshll v1.2d, v5.2s, #0
; VBITS_EQ_128-NEXT:    movprfx z31, z4
; VBITS_EQ_128-NEXT:    mul z31.d, p0/m, z31.d, z0.d
; VBITS_EQ_128-NEXT:    sshll v6.2d, v23.2s, #0
; VBITS_EQ_128-NEXT:    ldr q4, [sp] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v3.4s, #0
; VBITS_EQ_128-NEXT:    movprfx z28, z6
; VBITS_EQ_128-NEXT:    mul z28.d, p0/m, z28.d, z1.d
; VBITS_EQ_128-NEXT:    sshll v1.2d, v3.2s, #0
; VBITS_EQ_128-NEXT:    ldr z3, [x8, #6, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    movprfx z23, z3
; VBITS_EQ_128-NEXT:    mul z23.d, p0/m, z23.d, z0.d
; VBITS_EQ_128-NEXT:    sshll v5.2d, v4.2s, #0
; VBITS_EQ_128-NEXT:    ldr q3, [sp, #16] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    movprfx z20, z5
; VBITS_EQ_128-NEXT:    mul z20.d, p0/m, z20.d, z1.d
; VBITS_EQ_128-NEXT:    ldr z1, [x8, #7, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v7.4s, #0
; VBITS_EQ_128-NEXT:    sshll v4.2d, v7.2s, #0
; VBITS_EQ_128-NEXT:    movprfx z7, z1
; VBITS_EQ_128-NEXT:    mul z7.d, p0/m, z7.d, z0.d
; VBITS_EQ_128-NEXT:    ldr q1, [sp, #32] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    sshll v3.2d, v3.2s, #0
; VBITS_EQ_128-NEXT:    movprfx z6, z3
; VBITS_EQ_128-NEXT:    mul z6.d, p0/m, z6.d, z4.d
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v16.4s, #0
; VBITS_EQ_128-NEXT:    sshll v5.2d, v1.2s, #0
; VBITS_EQ_128-NEXT:    ldr z1, [x8, #8, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    movprfx z26, z1
; VBITS_EQ_128-NEXT:    mul z26.d, p0/m, z26.d, z0.d
; VBITS_EQ_128-NEXT:    ldr q1, [sp, #48] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    sshll v3.2d, v16.2s, #0
; VBITS_EQ_128-NEXT:    movprfx z24, z5
; VBITS_EQ_128-NEXT:    mul z24.d, p0/m, z24.d, z3.d
; VBITS_EQ_128-NEXT:    sshll v16.2d, v1.2s, #0
; VBITS_EQ_128-NEXT:    ldr z1, [x8, #9, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v17.4s, #0
; VBITS_EQ_128-NEXT:    movprfx z25, z1
; VBITS_EQ_128-NEXT:    mul z25.d, p0/m, z25.d, z0.d
; VBITS_EQ_128-NEXT:    sshll v5.2d, v17.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v29.4s, #0
; VBITS_EQ_128-NEXT:    sshll v17.2d, v29.2s, #0
; VBITS_EQ_128-NEXT:    movprfx z29, z16
; VBITS_EQ_128-NEXT:    mul z29.d, p0/m, z29.d, z5.d
; VBITS_EQ_128-NEXT:    ldr z1, [x8, #10, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    movprfx z4, z1
; VBITS_EQ_128-NEXT:    mul z4.d, p0/m, z4.d, z0.d
; VBITS_EQ_128-NEXT:    sshll v5.2d, v22.2s, #0
; VBITS_EQ_128-NEXT:    ldr z0, [x8, #11, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    sshll2 v16.2d, v22.4s, #0
; VBITS_EQ_128-NEXT:    movprfx z22, z0
; VBITS_EQ_128-NEXT:    mul z22.d, p0/m, z22.d, z17.d
; VBITS_EQ_128-NEXT:    ldr q0, [sp, #64] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    sshll v1.2d, v2.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v2.2d, v2.4s, #0
; VBITS_EQ_128-NEXT:    sshll v17.2d, v0.2s, #0
; VBITS_EQ_128-NEXT:    sshll2 v0.2d, v0.4s, #0
; VBITS_EQ_128-NEXT:    sshll v3.2d, v18.2s, #0
; VBITS_EQ_128-NEXT:    mul z1.d, p0/m, z1.d, z17.d
; VBITS_EQ_128-NEXT:    sshll2 v18.2d, v18.4s, #0
; VBITS_EQ_128-NEXT:    mul z0.d, p0/m, z0.d, z2.d
; VBITS_EQ_128-NEXT:    movprfx z2, z5
; VBITS_EQ_128-NEXT:    mul z2.d, p0/m, z2.d, z3.d
; VBITS_EQ_128-NEXT:    mul z18.d, p0/m, z18.d, z16.d
; VBITS_EQ_128-NEXT:    sshll2 v5.2d, v21.4s, #0
; VBITS_EQ_128-NEXT:    sshll2 v16.2d, v19.4s, #0
; VBITS_EQ_128-NEXT:    sshll v17.2d, v19.2s, #0
; VBITS_EQ_128-NEXT:    mul z5.d, p0/m, z5.d, z16.d
; VBITS_EQ_128-NEXT:    shrn v16.2s, v1.2d, #32
; VBITS_EQ_128-NEXT:    sshll v3.2d, v21.2s, #0
; VBITS_EQ_128-NEXT:    shrn v21.2s, v22.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v16.4s, v0.2d, #32
; VBITS_EQ_128-NEXT:    shrn v0.2s, v6.2d, #32
; VBITS_EQ_128-NEXT:    ldr z6, [x8, #1, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    shrn v1.2s, v20.2d, #32
; VBITS_EQ_128-NEXT:    mul z17.d, p0/m, z17.d, z3.d
; VBITS_EQ_128-NEXT:    shrn2 v21.4s, v4.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v0.4s, v7.2d, #32
; VBITS_EQ_128-NEXT:    shrn v3.2s, v13.2d, #32
; VBITS_EQ_128-NEXT:    ldr z19, [x8, #3, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    shrn v4.2s, v12.2d, #32
; VBITS_EQ_128-NEXT:    shrn v6.2s, v6.2d, #32
; VBITS_EQ_128-NEXT:    shrn v7.2s, v15.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v1.4s, v23.2d, #32
; VBITS_EQ_128-NEXT:    ldr z20, [x8, #2, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    shrn2 v3.4s, v27.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v4.4s, v9.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v6.4s, v19.2d, #32
; VBITS_EQ_128-NEXT:    shrn v19.2s, v11.2d, #32
; VBITS_EQ_128-NEXT:    ldr z22, [x8] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    stp q16, q21, [x0, #32]
; VBITS_EQ_128-NEXT:    shrn2 v7.4s, v20.2d, #32
; VBITS_EQ_128-NEXT:    shrn v20.2s, v8.2d, #32
; VBITS_EQ_128-NEXT:    stp q0, q1, [x0, #96]
; VBITS_EQ_128-NEXT:    shrn v0.2s, v2.2d, #32
; VBITS_EQ_128-NEXT:    stp q3, q4, [x0, #160]
; VBITS_EQ_128-NEXT:    shrn v3.2s, v24.2d, #32
; VBITS_EQ_128-NEXT:    stp q7, q6, [x0, #224]
; VBITS_EQ_128-NEXT:    shrn v6.2s, v30.2d, #32
; VBITS_EQ_128-NEXT:    shrn v7.2s, v28.2d, #32
; VBITS_EQ_128-NEXT:    shrn v4.2s, v29.2d, #32
; VBITS_EQ_128-NEXT:    shrn v1.2s, v17.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v19.4s, v22.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v20.4s, v10.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v6.4s, v14.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v7.4s, v31.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v3.4s, v26.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v4.4s, v25.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v0.4s, v18.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v1.4s, v5.2d, #32
; VBITS_EQ_128-NEXT:    stp q7, q6, [x0, #128]
; VBITS_EQ_128-NEXT:    stp q4, q3, [x0, #64]
; VBITS_EQ_128-NEXT:    stp q1, q0, [x0]
; VBITS_EQ_128-NEXT:    stp q20, q19, [x0, #192]
; VBITS_EQ_128-NEXT:    addvl sp, sp, #12
; VBITS_EQ_128-NEXT:    add sp, sp, #80
; VBITS_EQ_128-NEXT:    ldp d9, d8, [sp, #48] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d11, d10, [sp, #32] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d13, d12, [sp, #16] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldr x29, [sp, #64] // 8-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d15, d14, [sp], #80 // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_2048-LABEL: smulh_v64i32:
; VBITS_GE_2048:       // %bb.0:
; VBITS_GE_2048-NEXT:    ptrue p0.s, vl64
; VBITS_GE_2048-NEXT:    ld1w { z0.s }, p0/z, [x0]
; VBITS_GE_2048-NEXT:    ld1w { z1.s }, p0/z, [x1]
; VBITS_GE_2048-NEXT:    smulh z0.s, p0/m, z0.s, z1.s
; VBITS_GE_2048-NEXT:    st1w { z0.s }, p0, [x0]
; VBITS_GE_2048-NEXT:    ret
  %op1 = load <64 x i32>, <64 x i32>* %a
  %op2 = load <64 x i32>, <64 x i32>* %b
  %1 = sext <64 x i32> %op1 to <64 x i64>
  %2 = sext <64 x i32> %op2 to <64 x i64>
  %mul = mul <64 x i64> %1, %2
  %shr = lshr <64 x i64> %mul, <i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32>
  %res = trunc <64 x i64> %shr to <64 x i32>
  store <64 x i32> %res, <64 x i32>* %a
  ret void
}

; Vector i64 multiplications are not legal for NEON so use SVE when available.
define <1 x i64> @smulh_v1i64(<1 x i64> %op1, <1 x i64> %op2) #0 {
; VBITS_EQ_128-LABEL: smulh_v1i64:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    // kill: def $d1 killed $d1 def $q1
; VBITS_EQ_128-NEXT:    // kill: def $d0 killed $d0 def $q0
; VBITS_EQ_128-NEXT:    fmov x8, d0
; VBITS_EQ_128-NEXT:    fmov x9, d1
; VBITS_EQ_128-NEXT:    smulh x8, x8, x9
; VBITS_EQ_128-NEXT:    fmov d0, x8
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_256-LABEL: smulh_v1i64:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    // kill: def $d0 killed $d0 def $z0
; VBITS_GE_256-NEXT:    ptrue p0.d, vl1
; VBITS_GE_256-NEXT:    // kill: def $d1 killed $d1 def $z1
; VBITS_GE_256-NEXT:    smulh z0.d, p0/m, z0.d, z1.d
; VBITS_GE_256-NEXT:    // kill: def $d0 killed $d0 killed $z0
; VBITS_GE_256-NEXT:    ret
  %insert = insertelement <1 x i128> undef, i128 64, i128 0
  %splat = shufflevector <1 x i128> %insert, <1 x i128> undef, <1 x i32> zeroinitializer
  %1 = sext <1 x i64> %op1 to <1 x i128>
  %2 = sext <1 x i64> %op2 to <1 x i128>
  %mul = mul <1 x i128> %1, %2
  %shr = lshr <1 x i128> %mul, %splat
  %res = trunc <1 x i128> %shr to <1 x i64>
  ret <1 x i64> %res
}

; Vector i64 multiplications are not legal for NEON so use SVE when available.
define <2 x i64> @smulh_v2i64(<2 x i64> %op1, <2 x i64> %op2) #0 {
; VBITS_EQ_128-LABEL: smulh_v2i64:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    mov x8, v0.d[1]
; VBITS_EQ_128-NEXT:    fmov x10, d0
; VBITS_EQ_128-NEXT:    mov x9, v1.d[1]
; VBITS_EQ_128-NEXT:    fmov x11, d1
; VBITS_EQ_128-NEXT:    smulh x10, x10, x11
; VBITS_EQ_128-NEXT:    smulh x8, x8, x9
; VBITS_EQ_128-NEXT:    fmov d0, x10
; VBITS_EQ_128-NEXT:    fmov d1, x8
; VBITS_EQ_128-NEXT:    mov v0.d[1], v1.d[0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_256-LABEL: smulh_v2i64:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    // kill: def $q0 killed $q0 def $z0
; VBITS_GE_256-NEXT:    ptrue p0.d, vl2
; VBITS_GE_256-NEXT:    // kill: def $q1 killed $q1 def $z1
; VBITS_GE_256-NEXT:    smulh z0.d, p0/m, z0.d, z1.d
; VBITS_GE_256-NEXT:    // kill: def $q0 killed $q0 killed $z0
; VBITS_GE_256-NEXT:    ret
  %1 = sext <2 x i64> %op1 to <2 x i128>
  %2 = sext <2 x i64> %op2 to <2 x i128>
  %mul = mul <2 x i128> %1, %2
  %shr = lshr <2 x i128> %mul, <i128 64, i128 64>
  %res = trunc <2 x i128> %shr to <2 x i64>
  ret <2 x i64> %res
}

define void @smulh_v4i64(<4 x i64>* %a, <4 x i64>* %b) #0 {
; VBITS_EQ_128-LABEL: smulh_v4i64:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0]
; VBITS_EQ_128-NEXT:    mov x10, v0.d[1]
; VBITS_EQ_128-NEXT:    fmov x11, d0
; VBITS_EQ_128-NEXT:    ldp q2, q3, [x1]
; VBITS_EQ_128-NEXT:    mov x8, v1.d[1]
; VBITS_EQ_128-NEXT:    fmov x9, d1
; VBITS_EQ_128-NEXT:    mov x12, v2.d[1]
; VBITS_EQ_128-NEXT:    fmov x13, d2
; VBITS_EQ_128-NEXT:    mov x14, v3.d[1]
; VBITS_EQ_128-NEXT:    fmov x15, d3
; VBITS_EQ_128-NEXT:    smulh x11, x11, x13
; VBITS_EQ_128-NEXT:    smulh x10, x10, x12
; VBITS_EQ_128-NEXT:    smulh x9, x9, x15
; VBITS_EQ_128-NEXT:    smulh x8, x8, x14
; VBITS_EQ_128-NEXT:    fmov d0, x11
; VBITS_EQ_128-NEXT:    fmov d1, x10
; VBITS_EQ_128-NEXT:    fmov d2, x9
; VBITS_EQ_128-NEXT:    fmov d3, x8
; VBITS_EQ_128-NEXT:    mov v0.d[1], v1.d[0]
; VBITS_EQ_128-NEXT:    mov v2.d[1], v3.d[0]
; VBITS_EQ_128-NEXT:    stp q0, q2, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_256-LABEL: smulh_v4i64:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ptrue p0.d, vl4
; VBITS_GE_256-NEXT:    ld1d { z0.d }, p0/z, [x0]
; VBITS_GE_256-NEXT:    ld1d { z1.d }, p0/z, [x1]
; VBITS_GE_256-NEXT:    smulh z0.d, p0/m, z0.d, z1.d
; VBITS_GE_256-NEXT:    st1d { z0.d }, p0, [x0]
; VBITS_GE_256-NEXT:    ret
  %op1 = load <4 x i64>, <4 x i64>* %a
  %op2 = load <4 x i64>, <4 x i64>* %b
  %1 = sext <4 x i64> %op1 to <4 x i128>
  %2 = sext <4 x i64> %op2 to <4 x i128>
  %mul = mul <4 x i128> %1, %2
  %shr = lshr <4 x i128> %mul, <i128 64, i128 64, i128 64, i128 64>
  %res = trunc <4 x i128> %shr to <4 x i64>
  store <4 x i64> %res, <4 x i64>* %a
  ret void
}

define void @smulh_v8i64(<8 x i64>* %a, <8 x i64>* %b) #0 {
; VBITS_EQ_128-LABEL: smulh_v8i64:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0, #32]
; VBITS_EQ_128-NEXT:    fmov x14, d0
; VBITS_EQ_128-NEXT:    mov x13, v0.d[1]
; VBITS_EQ_128-NEXT:    ldp q2, q3, [x0]
; VBITS_EQ_128-NEXT:    mov x11, v1.d[1]
; VBITS_EQ_128-NEXT:    fmov x12, d1
; VBITS_EQ_128-NEXT:    mov x10, v2.d[1]
; VBITS_EQ_128-NEXT:    ldp q4, q5, [x1, #32]
; VBITS_EQ_128-NEXT:    mov x8, v3.d[1]
; VBITS_EQ_128-NEXT:    fmov x9, d3
; VBITS_EQ_128-NEXT:    fmov x17, d4
; VBITS_EQ_128-NEXT:    mov x15, v4.d[1]
; VBITS_EQ_128-NEXT:    ldp q3, q1, [x1]
; VBITS_EQ_128-NEXT:    fmov x1, d5
; VBITS_EQ_128-NEXT:    smulh x14, x14, x17
; VBITS_EQ_128-NEXT:    mov x18, v5.d[1]
; VBITS_EQ_128-NEXT:    smulh x13, x13, x15
; VBITS_EQ_128-NEXT:    fmov x15, d2
; VBITS_EQ_128-NEXT:    smulh x12, x12, x1
; VBITS_EQ_128-NEXT:    mov x1, v3.d[1]
; VBITS_EQ_128-NEXT:    fmov x17, d1
; VBITS_EQ_128-NEXT:    smulh x11, x11, x18
; VBITS_EQ_128-NEXT:    mov x16, v1.d[1]
; VBITS_EQ_128-NEXT:    fmov d2, x13
; VBITS_EQ_128-NEXT:    fmov d5, x12
; VBITS_EQ_128-NEXT:    smulh x9, x9, x17
; VBITS_EQ_128-NEXT:    fmov x17, d3
; VBITS_EQ_128-NEXT:    smulh x10, x10, x1
; VBITS_EQ_128-NEXT:    fmov d3, x14
; VBITS_EQ_128-NEXT:    smulh x8, x8, x16
; VBITS_EQ_128-NEXT:    fmov d4, x11
; VBITS_EQ_128-NEXT:    smulh x15, x15, x17
; VBITS_EQ_128-NEXT:    fmov d1, x9
; VBITS_EQ_128-NEXT:    fmov d6, x10
; VBITS_EQ_128-NEXT:    fmov d0, x8
; VBITS_EQ_128-NEXT:    fmov d7, x15
; VBITS_EQ_128-NEXT:    mov v3.d[1], v2.d[0]
; VBITS_EQ_128-NEXT:    mov v5.d[1], v4.d[0]
; VBITS_EQ_128-NEXT:    mov v7.d[1], v6.d[0]
; VBITS_EQ_128-NEXT:    mov v1.d[1], v0.d[0]
; VBITS_EQ_128-NEXT:    stp q3, q5, [x0, #32]
; VBITS_EQ_128-NEXT:    stp q7, q1, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_1024-LABEL: smulh_v8i64:
; VBITS_GE_1024:       // %bb.0:
; VBITS_GE_1024-NEXT:    ptrue p0.d, vl8
; VBITS_GE_1024-NEXT:    ld1d { z0.d }, p0/z, [x0]
; VBITS_GE_1024-NEXT:    ld1d { z1.d }, p0/z, [x1]
; VBITS_GE_1024-NEXT:    smulh z0.d, p0/m, z0.d, z1.d
; VBITS_GE_1024-NEXT:    st1d { z0.d }, p0, [x0]
; VBITS_GE_1024-NEXT:    ret
  %op1 = load <8 x i64>, <8 x i64>* %a
  %op2 = load <8 x i64>, <8 x i64>* %b
  %1 = sext <8 x i64> %op1 to <8 x i128>
  %2 = sext <8 x i64> %op2 to <8 x i128>
  %mul = mul <8 x i128> %1, %2
  %shr = lshr <8 x i128> %mul, <i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64>
  %res = trunc <8 x i128> %shr to <8 x i64>
  store <8 x i64> %res, <8 x i64>* %a
  ret void
}

define void @smulh_v16i64(<16 x i64>* %a, <16 x i64>* %b) #0 {
; VBITS_EQ_128-LABEL: smulh_v16i64:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    str x21, [sp, #-32]! // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_def_cfa_offset 32
; VBITS_EQ_128-NEXT:    stp x20, x19, [sp, #16] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_offset w19, -8
; VBITS_EQ_128-NEXT:    .cfi_offset w20, -16
; VBITS_EQ_128-NEXT:    .cfi_offset w21, -32
; VBITS_EQ_128-NEXT:    ldp q2, q3, [x0]
; VBITS_EQ_128-NEXT:    mov x10, v2.d[1]
; VBITS_EQ_128-NEXT:    fmov x11, d2
; VBITS_EQ_128-NEXT:    ldp q4, q5, [x0, #32]
; VBITS_EQ_128-NEXT:    mov x8, v3.d[1]
; VBITS_EQ_128-NEXT:    fmov x9, d3
; VBITS_EQ_128-NEXT:    mov x14, v4.d[1]
; VBITS_EQ_128-NEXT:    fmov x15, d4
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0, #96]
; VBITS_EQ_128-NEXT:    mov x12, v5.d[1]
; VBITS_EQ_128-NEXT:    fmov x13, d5
; VBITS_EQ_128-NEXT:    fmov x5, d0
; VBITS_EQ_128-NEXT:    mov x4, v0.d[1]
; VBITS_EQ_128-NEXT:    ldp q2, q3, [x0, #64]
; VBITS_EQ_128-NEXT:    mov x3, v1.d[1]
; VBITS_EQ_128-NEXT:    mov x18, v2.d[1]
; VBITS_EQ_128-NEXT:    fmov x2, d2
; VBITS_EQ_128-NEXT:    ldp q5, q6, [x1, #96]
; VBITS_EQ_128-NEXT:    mov x16, v3.d[1]
; VBITS_EQ_128-NEXT:    fmov x17, d3
; VBITS_EQ_128-NEXT:    fmov x19, d5
; VBITS_EQ_128-NEXT:    mov x6, v5.d[1]
; VBITS_EQ_128-NEXT:    ldp q4, q7, [x1, #64]
; VBITS_EQ_128-NEXT:    mov x20, v6.d[1]
; VBITS_EQ_128-NEXT:    fmov x21, d6
; VBITS_EQ_128-NEXT:    smulh x5, x5, x19
; VBITS_EQ_128-NEXT:    smulh x4, x4, x6
; VBITS_EQ_128-NEXT:    mov x19, v4.d[1]
; VBITS_EQ_128-NEXT:    fmov x6, d4
; VBITS_EQ_128-NEXT:    smulh x3, x3, x20
; VBITS_EQ_128-NEXT:    ldp q3, q16, [x1, #32]
; VBITS_EQ_128-NEXT:    fmov x20, d7
; VBITS_EQ_128-NEXT:    smulh x2, x2, x6
; VBITS_EQ_128-NEXT:    smulh x18, x18, x19
; VBITS_EQ_128-NEXT:    fmov d18, x4
; VBITS_EQ_128-NEXT:    fmov d19, x5
; VBITS_EQ_128-NEXT:    fmov d20, x3
; VBITS_EQ_128-NEXT:    smulh x17, x17, x20
; VBITS_EQ_128-NEXT:    fmov x19, d3
; VBITS_EQ_128-NEXT:    fmov d23, x2
; VBITS_EQ_128-NEXT:    ldp q2, q17, [x1]
; VBITS_EQ_128-NEXT:    fmov x1, d1
; VBITS_EQ_128-NEXT:    fmov x20, d16
; VBITS_EQ_128-NEXT:    smulh x15, x15, x19
; VBITS_EQ_128-NEXT:    fmov d22, x18
; VBITS_EQ_128-NEXT:    mov v19.d[1], v18.d[0]
; VBITS_EQ_128-NEXT:    smulh x1, x1, x21
; VBITS_EQ_128-NEXT:    mov x21, v7.d[1]
; VBITS_EQ_128-NEXT:    smulh x13, x13, x20
; VBITS_EQ_128-NEXT:    mov x7, v17.d[1]
; VBITS_EQ_128-NEXT:    mov x6, v2.d[1]
; VBITS_EQ_128-NEXT:    mov x20, v16.d[1]
; VBITS_EQ_128-NEXT:    smulh x16, x16, x21
; VBITS_EQ_128-NEXT:    fmov x21, d2
; VBITS_EQ_128-NEXT:    fmov x19, d17
; VBITS_EQ_128-NEXT:    smulh x8, x8, x7
; VBITS_EQ_128-NEXT:    smulh x10, x10, x6
; VBITS_EQ_128-NEXT:    fmov d5, x13
; VBITS_EQ_128-NEXT:    smulh x11, x11, x21
; VBITS_EQ_128-NEXT:    fmov d7, x15
; VBITS_EQ_128-NEXT:    mov x21, v3.d[1]
; VBITS_EQ_128-NEXT:    smulh x9, x9, x19
; VBITS_EQ_128-NEXT:    smulh x12, x12, x20
; VBITS_EQ_128-NEXT:    fmov d0, x8
; VBITS_EQ_128-NEXT:    fmov d2, x10
; VBITS_EQ_128-NEXT:    fmov d16, x16
; VBITS_EQ_128-NEXT:    fmov d3, x11
; VBITS_EQ_128-NEXT:    fmov d17, x17
; VBITS_EQ_128-NEXT:    smulh x14, x14, x21
; VBITS_EQ_128-NEXT:    fmov d1, x9
; VBITS_EQ_128-NEXT:    fmov d4, x12
; VBITS_EQ_128-NEXT:    fmov d21, x1
; VBITS_EQ_128-NEXT:    mov v23.d[1], v22.d[0]
; VBITS_EQ_128-NEXT:    mov v17.d[1], v16.d[0]
; VBITS_EQ_128-NEXT:    fmov d6, x14
; VBITS_EQ_128-NEXT:    mov v21.d[1], v20.d[0]
; VBITS_EQ_128-NEXT:    mov v5.d[1], v4.d[0]
; VBITS_EQ_128-NEXT:    mov v7.d[1], v6.d[0]
; VBITS_EQ_128-NEXT:    stp q23, q17, [x0, #64]
; VBITS_EQ_128-NEXT:    mov v3.d[1], v2.d[0]
; VBITS_EQ_128-NEXT:    mov v1.d[1], v0.d[0]
; VBITS_EQ_128-NEXT:    stp q19, q21, [x0, #96]
; VBITS_EQ_128-NEXT:    ldp x20, x19, [sp, #16] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    stp q7, q5, [x0, #32]
; VBITS_EQ_128-NEXT:    stp q3, q1, [x0]
; VBITS_EQ_128-NEXT:    ldr x21, [sp], #32 // 8-byte Folded Reload
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_1024-LABEL: smulh_v16i64:
; VBITS_GE_1024:       // %bb.0:
; VBITS_GE_1024-NEXT:    ptrue p0.d, vl16
; VBITS_GE_1024-NEXT:    ld1d { z0.d }, p0/z, [x0]
; VBITS_GE_1024-NEXT:    ld1d { z1.d }, p0/z, [x1]
; VBITS_GE_1024-NEXT:    smulh z0.d, p0/m, z0.d, z1.d
; VBITS_GE_1024-NEXT:    st1d { z0.d }, p0, [x0]
; VBITS_GE_1024-NEXT:    ret
  %op1 = load <16 x i64>, <16 x i64>* %a
  %op2 = load <16 x i64>, <16 x i64>* %b
  %1 = sext <16 x i64> %op1 to <16 x i128>
  %2 = sext <16 x i64> %op2 to <16 x i128>
  %mul = mul <16 x i128> %1, %2
  %shr = lshr <16 x i128> %mul, <i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64>
  %res = trunc <16 x i128> %shr to <16 x i64>
  store <16 x i64> %res, <16 x i64>* %a
  ret void
}

define void @smulh_v32i64(<32 x i64>* %a, <32 x i64>* %b) #0 {
; VBITS_EQ_128-LABEL: smulh_v32i64:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    sub sp, sp, #224
; VBITS_EQ_128-NEXT:    .cfi_def_cfa_offset 224
; VBITS_EQ_128-NEXT:    stp d15, d14, [sp, #64] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d13, d12, [sp, #80] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d11, d10, [sp, #96] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d9, d8, [sp, #112] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp x29, x30, [sp, #128] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp x28, x27, [sp, #144] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp x26, x25, [sp, #160] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp x24, x23, [sp, #176] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp x22, x21, [sp, #192] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp x20, x19, [sp, #208] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_offset w19, -8
; VBITS_EQ_128-NEXT:    .cfi_offset w20, -16
; VBITS_EQ_128-NEXT:    .cfi_offset w21, -24
; VBITS_EQ_128-NEXT:    .cfi_offset w22, -32
; VBITS_EQ_128-NEXT:    .cfi_offset w23, -40
; VBITS_EQ_128-NEXT:    .cfi_offset w24, -48
; VBITS_EQ_128-NEXT:    .cfi_offset w25, -56
; VBITS_EQ_128-NEXT:    .cfi_offset w26, -64
; VBITS_EQ_128-NEXT:    .cfi_offset w27, -72
; VBITS_EQ_128-NEXT:    .cfi_offset w28, -80
; VBITS_EQ_128-NEXT:    .cfi_offset w30, -88
; VBITS_EQ_128-NEXT:    .cfi_offset w29, -96
; VBITS_EQ_128-NEXT:    .cfi_offset b8, -104
; VBITS_EQ_128-NEXT:    .cfi_offset b9, -112
; VBITS_EQ_128-NEXT:    .cfi_offset b10, -120
; VBITS_EQ_128-NEXT:    .cfi_offset b11, -128
; VBITS_EQ_128-NEXT:    .cfi_offset b12, -136
; VBITS_EQ_128-NEXT:    .cfi_offset b13, -144
; VBITS_EQ_128-NEXT:    .cfi_offset b14, -152
; VBITS_EQ_128-NEXT:    .cfi_offset b15, -160
; VBITS_EQ_128-NEXT:    ldp q3, q2, [x0]
; VBITS_EQ_128-NEXT:    mov x8, v3.d[1]
; VBITS_EQ_128-NEXT:    ldp q5, q4, [x0, #64]
; VBITS_EQ_128-NEXT:    fmov x2, d2
; VBITS_EQ_128-NEXT:    str x8, [sp, #16] // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    fmov x8, d3
; VBITS_EQ_128-NEXT:    mov x6, v5.d[1]
; VBITS_EQ_128-NEXT:    fmov x7, d5
; VBITS_EQ_128-NEXT:    str x8, [sp] // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    ldp q6, q3, [x0, #96]
; VBITS_EQ_128-NEXT:    mov x20, v4.d[1]
; VBITS_EQ_128-NEXT:    fmov x21, d4
; VBITS_EQ_128-NEXT:    mov x23, v6.d[1]
; VBITS_EQ_128-NEXT:    fmov x24, d6
; VBITS_EQ_128-NEXT:    ldp q16, q4, [x0, #128]
; VBITS_EQ_128-NEXT:    mov x26, v3.d[1]
; VBITS_EQ_128-NEXT:    fmov x27, d3
; VBITS_EQ_128-NEXT:    mov x28, v16.d[1]
; VBITS_EQ_128-NEXT:    fmov x25, d16
; VBITS_EQ_128-NEXT:    ldp q7, q5, [x0, #224]
; VBITS_EQ_128-NEXT:    mov x22, v4.d[1]
; VBITS_EQ_128-NEXT:    fmov x19, d4
; VBITS_EQ_128-NEXT:    mov x13, v7.d[1]
; VBITS_EQ_128-NEXT:    fmov x11, d7
; VBITS_EQ_128-NEXT:    ldp q17, q6, [x0, #192]
; VBITS_EQ_128-NEXT:    mov x12, v5.d[1]
; VBITS_EQ_128-NEXT:    fmov x10, d5
; VBITS_EQ_128-NEXT:    mov x17, v17.d[1]
; VBITS_EQ_128-NEXT:    fmov x16, d17
; VBITS_EQ_128-NEXT:    ldp q18, q3, [x0, #160]
; VBITS_EQ_128-NEXT:    mov x15, v6.d[1]
; VBITS_EQ_128-NEXT:    fmov x14, d6
; VBITS_EQ_128-NEXT:    mov x5, v18.d[1]
; VBITS_EQ_128-NEXT:    fmov x4, d18
; VBITS_EQ_128-NEXT:    ldp q19, q16, [x1, #224]
; VBITS_EQ_128-NEXT:    mov x29, v3.d[1]
; VBITS_EQ_128-NEXT:    fmov x18, d3
; VBITS_EQ_128-NEXT:    fmov x8, d19
; VBITS_EQ_128-NEXT:    mov x9, v19.d[1]
; VBITS_EQ_128-NEXT:    ldp q21, q20, [x1, #192]
; VBITS_EQ_128-NEXT:    mov x30, v16.d[1]
; VBITS_EQ_128-NEXT:    smulh x8, x11, x8
; VBITS_EQ_128-NEXT:    smulh x11, x13, x9
; VBITS_EQ_128-NEXT:    fmov x9, d21
; VBITS_EQ_128-NEXT:    str x8, [sp, #48] // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    ldp q22, q18, [x1, #160]
; VBITS_EQ_128-NEXT:    ldp q24, q23, [x1, #128]
; VBITS_EQ_128-NEXT:    ldp q25, q17, [x1, #96]
; VBITS_EQ_128-NEXT:    ldp q26, q6, [x1, #64]
; VBITS_EQ_128-NEXT:    ldp q4, q3, [x1, #32]
; VBITS_EQ_128-NEXT:    ldp q7, q5, [x1]
; VBITS_EQ_128-NEXT:    fmov x1, d16
; VBITS_EQ_128-NEXT:    smulh x10, x10, x1
; VBITS_EQ_128-NEXT:    mov x1, v20.d[1]
; VBITS_EQ_128-NEXT:    ldp q1, q0, [x0, #32]
; VBITS_EQ_128-NEXT:    str x10, [sp, #56] // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    smulh x10, x12, x30
; VBITS_EQ_128-NEXT:    mov x30, v21.d[1]
; VBITS_EQ_128-NEXT:    fmov x3, d1
; VBITS_EQ_128-NEXT:    str x10, [sp, #24] // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    fmov x10, d20
; VBITS_EQ_128-NEXT:    ldr x13, [sp, #16] // 8-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d13, d11, [sp, #48] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    smulh x8, x14, x10
; VBITS_EQ_128-NEXT:    smulh x10, x15, x1
; VBITS_EQ_128-NEXT:    fmov x15, d18
; VBITS_EQ_128-NEXT:    smulh x14, x16, x9
; VBITS_EQ_128-NEXT:    mov x9, v22.d[1]
; VBITS_EQ_128-NEXT:    smulh x16, x17, x30
; VBITS_EQ_128-NEXT:    stp x11, x8, [sp, #32] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    fmov x17, d22
; VBITS_EQ_128-NEXT:    mov x8, v18.d[1]
; VBITS_EQ_128-NEXT:    smulh x18, x18, x15
; VBITS_EQ_128-NEXT:    mov x15, v23.d[1]
; VBITS_EQ_128-NEXT:    str x10, [sp, #8] // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    smulh x4, x4, x17
; VBITS_EQ_128-NEXT:    fmov d8, x16
; VBITS_EQ_128-NEXT:    mov x17, v24.d[1]
; VBITS_EQ_128-NEXT:    smulh x5, x5, x9
; VBITS_EQ_128-NEXT:    smulh x1, x29, x8
; VBITS_EQ_128-NEXT:    fmov x8, d23
; VBITS_EQ_128-NEXT:    fmov x9, d24
; VBITS_EQ_128-NEXT:    smulh x22, x22, x15
; VBITS_EQ_128-NEXT:    fmov x15, d17
; VBITS_EQ_128-NEXT:    fmov d9, x14
; VBITS_EQ_128-NEXT:    smulh x19, x19, x8
; VBITS_EQ_128-NEXT:    ldr d14, [sp, #8] // 8-byte Folded Reload
; VBITS_EQ_128-NEXT:    mov x8, v17.d[1]
; VBITS_EQ_128-NEXT:    smulh x25, x25, x9
; VBITS_EQ_128-NEXT:    mov x9, v25.d[1]
; VBITS_EQ_128-NEXT:    smulh x28, x28, x17
; VBITS_EQ_128-NEXT:    fmov x17, d25
; VBITS_EQ_128-NEXT:    smulh x15, x27, x15
; VBITS_EQ_128-NEXT:    mov x27, v6.d[1]
; VBITS_EQ_128-NEXT:    ldr d15, [sp, #40] // 8-byte Folded Reload
; VBITS_EQ_128-NEXT:    smulh x12, x26, x8
; VBITS_EQ_128-NEXT:    fmov x26, d6
; VBITS_EQ_128-NEXT:    smulh x17, x24, x17
; VBITS_EQ_128-NEXT:    ldr x8, [sp] // 8-byte Folded Reload
; VBITS_EQ_128-NEXT:    mov x24, v26.d[1]
; VBITS_EQ_128-NEXT:    smulh x11, x23, x9
; VBITS_EQ_128-NEXT:    fmov x23, d26
; VBITS_EQ_128-NEXT:    smulh x21, x21, x26
; VBITS_EQ_128-NEXT:    fmov x26, d0
; VBITS_EQ_128-NEXT:    smulh x20, x20, x27
; VBITS_EQ_128-NEXT:    fmov x27, d3
; VBITS_EQ_128-NEXT:    fmov d20, x17
; VBITS_EQ_128-NEXT:    smulh x7, x7, x23
; VBITS_EQ_128-NEXT:    fmov x23, d4
; VBITS_EQ_128-NEXT:    smulh x6, x6, x24
; VBITS_EQ_128-NEXT:    fmov x24, d5
; VBITS_EQ_128-NEXT:    smulh x26, x26, x27
; VBITS_EQ_128-NEXT:    fmov x27, d7
; VBITS_EQ_128-NEXT:    smulh x3, x3, x23
; VBITS_EQ_128-NEXT:    fmov d19, x20
; VBITS_EQ_128-NEXT:    mov x23, v2.d[1]
; VBITS_EQ_128-NEXT:    smulh x2, x2, x24
; VBITS_EQ_128-NEXT:    mov x24, v1.d[1]
; VBITS_EQ_128-NEXT:    smulh x27, x8, x27
; VBITS_EQ_128-NEXT:    mov x29, v0.d[1]
; VBITS_EQ_128-NEXT:    mov x30, v7.d[1]
; VBITS_EQ_128-NEXT:    mov x8, v5.d[1]
; VBITS_EQ_128-NEXT:    mov x9, v4.d[1]
; VBITS_EQ_128-NEXT:    mov x10, v3.d[1]
; VBITS_EQ_128-NEXT:    ldp d10, d12, [sp, #24] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    smulh x30, x13, x30
; VBITS_EQ_128-NEXT:    fmov d0, x27
; VBITS_EQ_128-NEXT:    smulh x8, x23, x8
; VBITS_EQ_128-NEXT:    fmov d2, x2
; VBITS_EQ_128-NEXT:    smulh x9, x24, x9
; VBITS_EQ_128-NEXT:    fmov d4, x3
; VBITS_EQ_128-NEXT:    smulh x10, x29, x10
; VBITS_EQ_128-NEXT:    fmov d6, x26
; VBITS_EQ_128-NEXT:    mov v11.d[1], v10.d[0]
; VBITS_EQ_128-NEXT:    fmov d1, x30
; VBITS_EQ_128-NEXT:    mov v13.d[1], v12.d[0]
; VBITS_EQ_128-NEXT:    mov v15.d[1], v14.d[0]
; VBITS_EQ_128-NEXT:    mov v9.d[1], v8.d[0]
; VBITS_EQ_128-NEXT:    fmov d3, x8
; VBITS_EQ_128-NEXT:    fmov d5, x9
; VBITS_EQ_128-NEXT:    fmov d7, x10
; VBITS_EQ_128-NEXT:    fmov d17, x6
; VBITS_EQ_128-NEXT:    fmov d16, x7
; VBITS_EQ_128-NEXT:    fmov d18, x21
; VBITS_EQ_128-NEXT:    fmov d21, x11
; VBITS_EQ_128-NEXT:    fmov d22, x12
; VBITS_EQ_128-NEXT:    fmov d23, x15
; VBITS_EQ_128-NEXT:    fmov d24, x28
; VBITS_EQ_128-NEXT:    fmov d25, x25
; VBITS_EQ_128-NEXT:    fmov d26, x22
; VBITS_EQ_128-NEXT:    fmov d27, x19
; VBITS_EQ_128-NEXT:    fmov d28, x5
; VBITS_EQ_128-NEXT:    fmov d29, x4
; VBITS_EQ_128-NEXT:    fmov d30, x1
; VBITS_EQ_128-NEXT:    fmov d31, x18
; VBITS_EQ_128-NEXT:    mov v27.d[1], v26.d[0]
; VBITS_EQ_128-NEXT:    stp q9, q15, [x0, #192]
; VBITS_EQ_128-NEXT:    stp q13, q11, [x0, #224]
; VBITS_EQ_128-NEXT:    mov v31.d[1], v30.d[0]
; VBITS_EQ_128-NEXT:    mov v29.d[1], v28.d[0]
; VBITS_EQ_128-NEXT:    mov v25.d[1], v24.d[0]
; VBITS_EQ_128-NEXT:    mov v23.d[1], v22.d[0]
; VBITS_EQ_128-NEXT:    mov v20.d[1], v21.d[0]
; VBITS_EQ_128-NEXT:    mov v18.d[1], v19.d[0]
; VBITS_EQ_128-NEXT:    stp q29, q31, [x0, #160]
; VBITS_EQ_128-NEXT:    mov v16.d[1], v17.d[0]
; VBITS_EQ_128-NEXT:    stp q25, q27, [x0, #128]
; VBITS_EQ_128-NEXT:    mov v6.d[1], v7.d[0]
; VBITS_EQ_128-NEXT:    mov v4.d[1], v5.d[0]
; VBITS_EQ_128-NEXT:    stp q20, q23, [x0, #96]
; VBITS_EQ_128-NEXT:    mov v2.d[1], v3.d[0]
; VBITS_EQ_128-NEXT:    mov v0.d[1], v1.d[0]
; VBITS_EQ_128-NEXT:    stp q16, q18, [x0, #64]
; VBITS_EQ_128-NEXT:    ldp x20, x19, [sp, #208] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    stp q4, q6, [x0, #32]
; VBITS_EQ_128-NEXT:    ldp x22, x21, [sp, #192] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    stp q0, q2, [x0]
; VBITS_EQ_128-NEXT:    ldp x24, x23, [sp, #176] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp x26, x25, [sp, #160] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp x28, x27, [sp, #144] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp x29, x30, [sp, #128] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d9, d8, [sp, #112] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d11, d10, [sp, #96] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d13, d12, [sp, #80] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d15, d14, [sp, #64] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add sp, sp, #224
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_2048-LABEL: smulh_v32i64:
; VBITS_GE_2048:       // %bb.0:
; VBITS_GE_2048-NEXT:    ptrue p0.d, vl32
; VBITS_GE_2048-NEXT:    ld1d { z0.d }, p0/z, [x0]
; VBITS_GE_2048-NEXT:    ld1d { z1.d }, p0/z, [x1]
; VBITS_GE_2048-NEXT:    smulh z0.d, p0/m, z0.d, z1.d
; VBITS_GE_2048-NEXT:    st1d { z0.d }, p0, [x0]
; VBITS_GE_2048-NEXT:    ret
  %op1 = load <32 x i64>, <32 x i64>* %a
  %op2 = load <32 x i64>, <32 x i64>* %b
  %1 = sext <32 x i64> %op1 to <32 x i128>
  %2 = sext <32 x i64> %op2 to <32 x i128>
  %mul = mul <32 x i128> %1, %2
  %shr = lshr <32 x i128> %mul, <i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64>
  %res = trunc <32 x i128> %shr to <32 x i64>
  store <32 x i64> %res, <32 x i64>* %a
  ret void
}

;
; UMULH
;

; Don't use SVE for 64-bit vectors.
; FIXME: The codegen for the >=256 bits case can be improved.
define <8 x i8> @umulh_v8i8(<8 x i8> %op1, <8 x i8> %op2) #0 {
; VBITS_EQ_128-LABEL: umulh_v8i8:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    umull v0.8h, v0.8b, v1.8b
; VBITS_EQ_128-NEXT:    shrn v0.8b, v0.8h, #8
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_256-LABEL: umulh_v8i8:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    umull v0.8h, v0.8b, v1.8b
; VBITS_GE_256-NEXT:    ushr v1.8h, v0.8h, #8
; VBITS_GE_256-NEXT:    umov w8, v1.h[0]
; VBITS_GE_256-NEXT:    umov w9, v1.h[1]
; VBITS_GE_256-NEXT:    fmov s0, w8
; VBITS_GE_256-NEXT:    umov w8, v1.h[2]
; VBITS_GE_256-NEXT:    mov v0.b[1], w9
; VBITS_GE_256-NEXT:    mov v0.b[2], w8
; VBITS_GE_256-NEXT:    umov w8, v1.h[3]
; VBITS_GE_256-NEXT:    mov v0.b[3], w8
; VBITS_GE_256-NEXT:    umov w8, v1.h[4]
; VBITS_GE_256-NEXT:    mov v0.b[4], w8
; VBITS_GE_256-NEXT:    umov w8, v1.h[5]
; VBITS_GE_256-NEXT:    mov v0.b[5], w8
; VBITS_GE_256-NEXT:    umov w8, v1.h[6]
; VBITS_GE_256-NEXT:    mov v0.b[6], w8
; VBITS_GE_256-NEXT:    umov w8, v1.h[7]
; VBITS_GE_256-NEXT:    mov v0.b[7], w8
; VBITS_GE_256-NEXT:    // kill: def $d0 killed $d0 killed $q0
; VBITS_GE_256-NEXT:    ret
  %1 = zext <8 x i8> %op1 to <8 x i16>
  %2 = zext <8 x i8> %op2 to <8 x i16>
  %mul = mul <8 x i16> %1, %2
  %shr = lshr <8 x i16> %mul, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %res = trunc <8 x i16> %shr to <8 x i8>
  ret <8 x i8> %res
}

; Don't use SVE for 128-bit vectors.
define <16 x i8> @umulh_v16i8(<16 x i8> %op1, <16 x i8> %op2) #0 {
; CHECK-LABEL: umulh_v16i8:
; CHECK:       // %bb.0:
; CHECK-NEXT:    umull2 v2.8h, v0.16b, v1.16b
; CHECK-NEXT:    umull v0.8h, v0.8b, v1.8b
; CHECK-NEXT:    uzp2 v0.16b, v0.16b, v2.16b
; CHECK-NEXT:    ret
  %1 = zext <16 x i8> %op1 to <16 x i16>
  %2 = zext <16 x i8> %op2 to <16 x i16>
  %mul = mul <16 x i16> %1, %2
  %shr = lshr <16 x i16> %mul, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %res = trunc <16 x i16> %shr to <16 x i8>
  ret <16 x i8> %res
}

define void @umulh_v32i8(<32 x i8>* %a, <32 x i8>* %b) #0 {
; VBITS_EQ_128-LABEL: umulh_v32i8:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0]
; VBITS_EQ_128-NEXT:    ldp q2, q3, [x1]
; VBITS_EQ_128-NEXT:    umull v4.8h, v0.8b, v2.8b
; VBITS_EQ_128-NEXT:    umull2 v0.8h, v0.16b, v2.16b
; VBITS_EQ_128-NEXT:    umull v5.8h, v1.8b, v3.8b
; VBITS_EQ_128-NEXT:    umull2 v1.8h, v1.16b, v3.16b
; VBITS_EQ_128-NEXT:    shrn v2.8b, v4.8h, #8
; VBITS_EQ_128-NEXT:    shrn v3.8b, v5.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v2.16b, v0.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v3.16b, v1.8h, #8
; VBITS_EQ_128-NEXT:    stp q2, q3, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_256-LABEL: umulh_v32i8:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ptrue p0.b, vl32
; VBITS_GE_256-NEXT:    ld1b { z0.b }, p0/z, [x0]
; VBITS_GE_256-NEXT:    ld1b { z1.b }, p0/z, [x1]
; VBITS_GE_256-NEXT:    umulh z0.b, p0/m, z0.b, z1.b
; VBITS_GE_256-NEXT:    st1b { z0.b }, p0, [x0]
; VBITS_GE_256-NEXT:    ret
  %op1 = load <32 x i8>, <32 x i8>* %a
  %op2 = load <32 x i8>, <32 x i8>* %b
  %1 = zext <32 x i8> %op1 to <32 x i16>
  %2 = zext <32 x i8> %op2 to <32 x i16>
  %mul = mul <32 x i16> %1, %2
  %shr = lshr <32 x i16> %mul, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %res = trunc <32 x i16> %shr to <32 x i8>
  store <32 x i8> %res, <32 x i8>* %a
  ret void
}

define void @umulh_v64i8(<64 x i8>* %a, <64 x i8>* %b) #0 {
; VBITS_EQ_128-LABEL: umulh_v64i8:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0, #32]
; VBITS_EQ_128-NEXT:    ldp q2, q5, [x1, #32]
; VBITS_EQ_128-NEXT:    umull2 v6.8h, v0.16b, v2.16b
; VBITS_EQ_128-NEXT:    umull v0.8h, v0.8b, v2.8b
; VBITS_EQ_128-NEXT:    ldp q3, q4, [x0]
; VBITS_EQ_128-NEXT:    umull2 v7.8h, v1.16b, v5.16b
; VBITS_EQ_128-NEXT:    umull v1.8h, v1.8b, v5.8b
; VBITS_EQ_128-NEXT:    shrn v0.8b, v0.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v0.16b, v6.8h, #8
; VBITS_EQ_128-NEXT:    ldp q2, q5, [x1]
; VBITS_EQ_128-NEXT:    shrn v1.8b, v1.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v1.16b, v7.8h, #8
; VBITS_EQ_128-NEXT:    umull2 v16.8h, v3.16b, v2.16b
; VBITS_EQ_128-NEXT:    umull v2.8h, v3.8b, v2.8b
; VBITS_EQ_128-NEXT:    stp q0, q1, [x0, #32]
; VBITS_EQ_128-NEXT:    umull2 v3.8h, v4.16b, v5.16b
; VBITS_EQ_128-NEXT:    umull v4.8h, v4.8b, v5.8b
; VBITS_EQ_128-NEXT:    shrn v2.8b, v2.8h, #8
; VBITS_EQ_128-NEXT:    shrn v4.8b, v4.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v2.16b, v16.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v4.16b, v3.8h, #8
; VBITS_EQ_128-NEXT:    stp q2, q4, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_1024-LABEL: umulh_v64i8:
; VBITS_GE_1024:       // %bb.0:
; VBITS_GE_1024-NEXT:    ptrue p0.b, vl64
; VBITS_GE_1024-NEXT:    ld1b { z0.b }, p0/z, [x0]
; VBITS_GE_1024-NEXT:    ld1b { z1.b }, p0/z, [x1]
; VBITS_GE_1024-NEXT:    umulh z0.b, p0/m, z0.b, z1.b
; VBITS_GE_1024-NEXT:    st1b { z0.b }, p0, [x0]
; VBITS_GE_1024-NEXT:    ret
  %op1 = load <64 x i8>, <64 x i8>* %a
  %op2 = load <64 x i8>, <64 x i8>* %b
  %1 = zext <64 x i8> %op1 to <64 x i16>
  %2 = zext <64 x i8> %op2 to <64 x i16>
  %mul = mul <64 x i16> %1, %2
  %shr = lshr <64 x i16> %mul, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %res = trunc <64 x i16> %shr to <64 x i8>
  store <64 x i8> %res, <64 x i8>* %a
  ret void
}

define void @umulh_v128i8(<128 x i8>* %a, <128 x i8>* %b) #0 {
; VBITS_EQ_128-LABEL: umulh_v128i8:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0, #96]
; VBITS_EQ_128-NEXT:    ldp q2, q5, [x1, #96]
; VBITS_EQ_128-NEXT:    umull2 v6.8h, v0.16b, v2.16b
; VBITS_EQ_128-NEXT:    umull v0.8h, v0.8b, v2.8b
; VBITS_EQ_128-NEXT:    ldp q3, q4, [x0, #64]
; VBITS_EQ_128-NEXT:    umull2 v7.8h, v1.16b, v5.16b
; VBITS_EQ_128-NEXT:    umull v1.8h, v1.8b, v5.8b
; VBITS_EQ_128-NEXT:    shrn v0.8b, v0.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v0.16b, v6.8h, #8
; VBITS_EQ_128-NEXT:    ldp q2, q16, [x1, #64]
; VBITS_EQ_128-NEXT:    shrn v1.8b, v1.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v1.16b, v7.8h, #8
; VBITS_EQ_128-NEXT:    umull2 v17.8h, v3.16b, v2.16b
; VBITS_EQ_128-NEXT:    umull v2.8h, v3.8b, v2.8b
; VBITS_EQ_128-NEXT:    ldp q5, q18, [x0, #32]
; VBITS_EQ_128-NEXT:    umull2 v19.8h, v4.16b, v16.16b
; VBITS_EQ_128-NEXT:    umull v4.8h, v4.8b, v16.8b
; VBITS_EQ_128-NEXT:    shrn v2.8b, v2.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v2.16b, v17.8h, #8
; VBITS_EQ_128-NEXT:    ldp q3, q20, [x1, #32]
; VBITS_EQ_128-NEXT:    shrn v4.8b, v4.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v4.16b, v19.8h, #8
; VBITS_EQ_128-NEXT:    umull2 v21.8h, v5.16b, v3.16b
; VBITS_EQ_128-NEXT:    umull v3.8h, v5.8b, v3.8b
; VBITS_EQ_128-NEXT:    ldp q16, q22, [x0]
; VBITS_EQ_128-NEXT:    umull2 v23.8h, v18.16b, v20.16b
; VBITS_EQ_128-NEXT:    umull v18.8h, v18.8b, v20.8b
; VBITS_EQ_128-NEXT:    shrn v3.8b, v3.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v3.16b, v21.8h, #8
; VBITS_EQ_128-NEXT:    ldp q5, q24, [x1]
; VBITS_EQ_128-NEXT:    shrn v18.8b, v18.8h, #8
; VBITS_EQ_128-NEXT:    stp q2, q4, [x0, #64]
; VBITS_EQ_128-NEXT:    stp q0, q1, [x0, #96]
; VBITS_EQ_128-NEXT:    shrn2 v18.16b, v23.8h, #8
; VBITS_EQ_128-NEXT:    umull v20.8h, v16.8b, v5.8b
; VBITS_EQ_128-NEXT:    umull2 v5.8h, v16.16b, v5.16b
; VBITS_EQ_128-NEXT:    stp q3, q18, [x0, #32]
; VBITS_EQ_128-NEXT:    umull v25.8h, v22.8b, v24.8b
; VBITS_EQ_128-NEXT:    umull2 v16.8h, v22.16b, v24.16b
; VBITS_EQ_128-NEXT:    shrn v20.8b, v20.8h, #8
; VBITS_EQ_128-NEXT:    shrn v22.8b, v25.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v20.16b, v5.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v22.16b, v16.8h, #8
; VBITS_EQ_128-NEXT:    stp q20, q22, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_1024-LABEL: umulh_v128i8:
; VBITS_GE_1024:       // %bb.0:
; VBITS_GE_1024-NEXT:    ptrue p0.b, vl128
; VBITS_GE_1024-NEXT:    ld1b { z0.b }, p0/z, [x0]
; VBITS_GE_1024-NEXT:    ld1b { z1.b }, p0/z, [x1]
; VBITS_GE_1024-NEXT:    umulh z0.b, p0/m, z0.b, z1.b
; VBITS_GE_1024-NEXT:    st1b { z0.b }, p0, [x0]
; VBITS_GE_1024-NEXT:    ret

  %op1 = load <128 x i8>, <128 x i8>* %a
  %op2 = load <128 x i8>, <128 x i8>* %b
  %insert = insertelement <128 x i16> undef, i16 8, i64 0
  %splat = shufflevector <128 x i16> %insert, <128 x i16> undef, <128 x i32> zeroinitializer
  %1 = zext <128 x i8> %op1 to <128 x i16>
  %2 = zext <128 x i8> %op2 to <128 x i16>
  %mul = mul <128 x i16> %1, %2
  %shr = lshr <128 x i16> %mul, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %res = trunc <128 x i16> %shr to <128 x i8>
  store <128 x i8> %res, <128 x i8>* %a
  ret void
}

define void @umulh_v256i8(<256 x i8>* %a, <256 x i8>* %b) #0 {
; VBITS_EQ_128-LABEL: umulh_v256i8:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    sub sp, sp, #96
; VBITS_EQ_128-NEXT:    .cfi_def_cfa_offset 96
; VBITS_EQ_128-NEXT:    stp d15, d14, [sp, #32] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d13, d12, [sp, #48] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d11, d10, [sp, #64] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d9, d8, [sp, #80] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_offset b8, -8
; VBITS_EQ_128-NEXT:    .cfi_offset b9, -16
; VBITS_EQ_128-NEXT:    .cfi_offset b10, -24
; VBITS_EQ_128-NEXT:    .cfi_offset b11, -32
; VBITS_EQ_128-NEXT:    .cfi_offset b12, -40
; VBITS_EQ_128-NEXT:    .cfi_offset b13, -48
; VBITS_EQ_128-NEXT:    .cfi_offset b14, -56
; VBITS_EQ_128-NEXT:    .cfi_offset b15, -64
; VBITS_EQ_128-NEXT:    ldp q2, q1, [x0, #224]
; VBITS_EQ_128-NEXT:    ldp q6, q3, [x1, #224]
; VBITS_EQ_128-NEXT:    ldp q7, q5, [x0, #192]
; VBITS_EQ_128-NEXT:    umull2 v0.8h, v1.16b, v3.16b
; VBITS_EQ_128-NEXT:    umull v4.8h, v1.8b, v3.8b
; VBITS_EQ_128-NEXT:    str q0, [sp, #16] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    ldp q16, q3, [x1, #192]
; VBITS_EQ_128-NEXT:    umull2 v0.8h, v2.16b, v6.16b
; VBITS_EQ_128-NEXT:    shrn v4.8b, v4.8h, #8
; VBITS_EQ_128-NEXT:    umull v6.8h, v2.8b, v6.8b
; VBITS_EQ_128-NEXT:    str q0, [sp] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    umull2 v2.8h, v5.16b, v3.16b
; VBITS_EQ_128-NEXT:    shrn v6.8b, v6.8h, #8
; VBITS_EQ_128-NEXT:    umull v5.8h, v5.8b, v3.8b
; VBITS_EQ_128-NEXT:    ldp q19, q18, [x0, #160]
; VBITS_EQ_128-NEXT:    umull2 v3.8h, v7.16b, v16.16b
; VBITS_EQ_128-NEXT:    umull v7.8h, v7.8b, v16.8b
; VBITS_EQ_128-NEXT:    shrn v5.8b, v5.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v5.16b, v2.8h, #8
; VBITS_EQ_128-NEXT:    ldp q16, q17, [x1, #160]
; VBITS_EQ_128-NEXT:    shrn v7.8b, v7.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v7.16b, v3.8h, #8
; VBITS_EQ_128-NEXT:    umull2 v31.8h, v19.16b, v16.16b
; VBITS_EQ_128-NEXT:    umull v9.8h, v19.8b, v16.8b
; VBITS_EQ_128-NEXT:    umull2 v21.8h, v18.16b, v17.16b
; VBITS_EQ_128-NEXT:    umull v30.8h, v18.8b, v17.8b
; VBITS_EQ_128-NEXT:    ldp q22, q17, [x0, #128]
; VBITS_EQ_128-NEXT:    shrn v9.8b, v9.8h, #8
; VBITS_EQ_128-NEXT:    shrn v30.8b, v30.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v9.16b, v31.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v30.16b, v21.8h, #8
; VBITS_EQ_128-NEXT:    ldp q19, q20, [x1, #128]
; VBITS_EQ_128-NEXT:    umull2 v16.8h, v17.16b, v20.16b
; VBITS_EQ_128-NEXT:    ldr q21, [sp, #16] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    umull v18.8h, v17.8b, v20.8b
; VBITS_EQ_128-NEXT:    ldp q24, q20, [x0, #96]
; VBITS_EQ_128-NEXT:    umull2 v17.8h, v22.16b, v19.16b
; VBITS_EQ_128-NEXT:    shrn2 v4.16b, v21.8h, #8
; VBITS_EQ_128-NEXT:    umull v19.8h, v22.8b, v19.8b
; VBITS_EQ_128-NEXT:    shrn v2.8b, v18.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v2.16b, v16.8h, #8
; VBITS_EQ_128-NEXT:    ldp q22, q23, [x1, #96]
; VBITS_EQ_128-NEXT:    shrn v3.8b, v19.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v3.16b, v17.8h, #8
; VBITS_EQ_128-NEXT:    umull2 v12.8h, v24.16b, v22.16b
; VBITS_EQ_128-NEXT:    umull v13.8h, v24.8b, v22.8b
; VBITS_EQ_128-NEXT:    umull2 v10.8h, v20.16b, v23.16b
; VBITS_EQ_128-NEXT:    ldr q21, [sp] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    umull v11.8h, v20.8b, v23.8b
; VBITS_EQ_128-NEXT:    ldp q26, q23, [x0, #64]
; VBITS_EQ_128-NEXT:    shrn2 v6.16b, v21.8h, #8
; VBITS_EQ_128-NEXT:    ldp q24, q25, [x1, #64]
; VBITS_EQ_128-NEXT:    umull2 v22.8h, v26.16b, v24.16b
; VBITS_EQ_128-NEXT:    umull v24.8h, v26.8b, v24.8b
; VBITS_EQ_128-NEXT:    umull2 v20.8h, v23.16b, v25.16b
; VBITS_EQ_128-NEXT:    umull v23.8h, v23.8b, v25.8b
; VBITS_EQ_128-NEXT:    ldp q28, q25, [x0, #32]
; VBITS_EQ_128-NEXT:    ldp q26, q27, [x1, #32]
; VBITS_EQ_128-NEXT:    umull2 v15.8h, v28.16b, v26.16b
; VBITS_EQ_128-NEXT:    umull v1.8h, v28.8b, v26.8b
; VBITS_EQ_128-NEXT:    umull2 v14.8h, v25.16b, v27.16b
; VBITS_EQ_128-NEXT:    umull v8.8h, v25.8b, v27.8b
; VBITS_EQ_128-NEXT:    ldp q0, q27, [x0]
; VBITS_EQ_128-NEXT:    shrn v8.8b, v8.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v8.16b, v14.8h, #8
; VBITS_EQ_128-NEXT:    ldp q28, q29, [x1]
; VBITS_EQ_128-NEXT:    stp q3, q2, [x0, #128]
; VBITS_EQ_128-NEXT:    shrn v2.8b, v23.8h, #8
; VBITS_EQ_128-NEXT:    stp q9, q30, [x0, #160]
; VBITS_EQ_128-NEXT:    shrn v3.8b, v24.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v2.16b, v20.8h, #8
; VBITS_EQ_128-NEXT:    stp q7, q5, [x0, #192]
; VBITS_EQ_128-NEXT:    umull2 v26.8h, v0.16b, v28.16b
; VBITS_EQ_128-NEXT:    shrn2 v3.16b, v22.8h, #8
; VBITS_EQ_128-NEXT:    umull v28.8h, v0.8b, v28.8b
; VBITS_EQ_128-NEXT:    stp q6, q4, [x0, #224]
; VBITS_EQ_128-NEXT:    umull2 v25.8h, v27.16b, v29.16b
; VBITS_EQ_128-NEXT:    stp q3, q2, [x0, #64]
; VBITS_EQ_128-NEXT:    umull v27.8h, v27.8b, v29.8b
; VBITS_EQ_128-NEXT:    shrn v29.8b, v1.8h, #8
; VBITS_EQ_128-NEXT:    shrn v0.8b, v13.8h, #8
; VBITS_EQ_128-NEXT:    shrn v1.8b, v11.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v29.16b, v15.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v0.16b, v12.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v1.16b, v10.8h, #8
; VBITS_EQ_128-NEXT:    stp q29, q8, [x0, #32]
; VBITS_EQ_128-NEXT:    stp q0, q1, [x0, #96]
; VBITS_EQ_128-NEXT:    shrn v0.8b, v27.8h, #8
; VBITS_EQ_128-NEXT:    shrn v1.8b, v28.8h, #8
; VBITS_EQ_128-NEXT:    ldp d9, d8, [sp, #80] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    shrn2 v0.16b, v25.8h, #8
; VBITS_EQ_128-NEXT:    shrn2 v1.16b, v26.8h, #8
; VBITS_EQ_128-NEXT:    ldp d11, d10, [sp, #64] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    stp q1, q0, [x0]
; VBITS_EQ_128-NEXT:    ldp d13, d12, [sp, #48] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d15, d14, [sp, #32] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add sp, sp, #96
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_2048-LABEL: umulh_v256i8:
; VBITS_GE_2048:       // %bb.0:
; VBITS_GE_2048-NEXT:    ptrue p0.b, vl256
; VBITS_GE_2048-NEXT:    ld1b { z0.b }, p0/z, [x0]
; VBITS_GE_2048-NEXT:    ld1b { z1.b }, p0/z, [x1]
; VBITS_GE_2048-NEXT:    umulh z0.b, p0/m, z0.b, z1.b
; VBITS_GE_2048-NEXT:    st1b { z0.b }, p0, [x0]
; VBITS_GE_2048-NEXT:    ret
  %op1 = load <256 x i8>, <256 x i8>* %a
  %op2 = load <256 x i8>, <256 x i8>* %b
  %1 = zext <256 x i8> %op1 to <256 x i16>
  %2 = zext <256 x i8> %op2 to <256 x i16>
  %mul = mul <256 x i16> %1, %2
  %shr = lshr <256 x i16> %mul, <i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8, i16 8>
  %res = trunc <256 x i16> %shr to <256 x i8>
  store <256 x i8> %res, <256 x i8>* %a
  ret void
}

; Don't use SVE for 64-bit vectors.
; FIXME: The codegen for the >=256 bits case can be improved.
define <4 x i16> @umulh_v4i16(<4 x i16> %op1, <4 x i16> %op2) #0 {
; VBITS_EQ_128-LABEL: umulh_v4i16:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    umull v0.4s, v0.4h, v1.4h
; VBITS_EQ_128-NEXT:    shrn v0.4h, v0.4s, #16
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_256-LABEL: umulh_v4i16:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    umull v0.4s, v0.4h, v1.4h
; VBITS_GE_256-NEXT:    ushr v1.4s, v0.4s, #16
; VBITS_GE_256-NEXT:    mov w8, v1.s[1]
; VBITS_GE_256-NEXT:    mov w9, v1.s[2]
; VBITS_GE_256-NEXT:    mov v0.16b, v1.16b
; VBITS_GE_256-NEXT:    mov v0.h[1], w8
; VBITS_GE_256-NEXT:    mov w8, v1.s[3]
; VBITS_GE_256-NEXT:    mov v0.h[2], w9
; VBITS_GE_256-NEXT:    mov v0.h[3], w8
; VBITS_GE_256-NEXT:    // kill: def $d0 killed $d0 killed $q0
; VBITS_GE_256-NEXT:    ret
  %1 = zext <4 x i16> %op1 to <4 x i32>
  %2 = zext <4 x i16> %op2 to <4 x i32>
  %mul = mul <4 x i32> %1, %2
  %shr = lshr <4 x i32> %mul, <i32 16, i32 16, i32 16, i32 16>
  %res = trunc <4 x i32> %shr to <4 x i16>
  ret <4 x i16> %res
}

; Don't use SVE for 128-bit vectors.
define <8 x i16> @umulh_v8i16(<8 x i16> %op1, <8 x i16> %op2) #0 {
; CHECK-LABEL: umulh_v8i16:
; CHECK:       // %bb.0:
; CHECK-NEXT:    umull2 v2.4s, v0.8h, v1.8h
; CHECK-NEXT:    umull v0.4s, v0.4h, v1.4h
; CHECK-NEXT:    uzp2 v0.8h, v0.8h, v2.8h
; CHECK-NEXT:    ret
  %1 = zext <8 x i16> %op1 to <8 x i32>
  %2 = zext <8 x i16> %op2 to <8 x i32>
  %mul = mul <8 x i32> %1, %2
  %shr = lshr <8 x i32> %mul, <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %res = trunc <8 x i32> %shr to <8 x i16>
  ret <8 x i16> %res
}

define void @umulh_v16i16(<16 x i16>* %a, <16 x i16>* %b) #0 {
; VBITS_EQ_128-LABEL: umulh_v16i16:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0]
; VBITS_EQ_128-NEXT:    ldp q2, q3, [x1]
; VBITS_EQ_128-NEXT:    umull v4.4s, v0.4h, v2.4h
; VBITS_EQ_128-NEXT:    umull2 v0.4s, v0.8h, v2.8h
; VBITS_EQ_128-NEXT:    umull v5.4s, v1.4h, v3.4h
; VBITS_EQ_128-NEXT:    umull2 v1.4s, v1.8h, v3.8h
; VBITS_EQ_128-NEXT:    shrn v2.4h, v4.4s, #16
; VBITS_EQ_128-NEXT:    shrn v3.4h, v5.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v2.8h, v0.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v3.8h, v1.4s, #16
; VBITS_EQ_128-NEXT:    stp q2, q3, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_256-LABEL: umulh_v16i16:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ptrue p0.h, vl16
; VBITS_GE_256-NEXT:    ld1h { z0.h }, p0/z, [x0]
; VBITS_GE_256-NEXT:    ld1h { z1.h }, p0/z, [x1]
; VBITS_GE_256-NEXT:    umulh z0.h, p0/m, z0.h, z1.h
; VBITS_GE_256-NEXT:    st1h { z0.h }, p0, [x0]
; VBITS_GE_256-NEXT:    ret
  %op1 = load <16 x i16>, <16 x i16>* %a
  %op2 = load <16 x i16>, <16 x i16>* %b
  %1 = zext <16 x i16> %op1 to <16 x i32>
  %2 = zext <16 x i16> %op2 to <16 x i32>
  %mul = mul <16 x i32> %1, %2
  %shr = lshr <16 x i32> %mul, <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %res = trunc <16 x i32> %shr to <16 x i16>
  store <16 x i16> %res, <16 x i16>* %a
  ret void
}

define void @umulh_v32i16(<32 x i16>* %a, <32 x i16>* %b) #0 {
; VBITS_EQ_128-LABEL: umulh_v32i16:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0, #32]
; VBITS_EQ_128-NEXT:    ldp q2, q5, [x1, #32]
; VBITS_EQ_128-NEXT:    umull2 v6.4s, v0.8h, v2.8h
; VBITS_EQ_128-NEXT:    umull v0.4s, v0.4h, v2.4h
; VBITS_EQ_128-NEXT:    ldp q3, q4, [x0]
; VBITS_EQ_128-NEXT:    umull2 v7.4s, v1.8h, v5.8h
; VBITS_EQ_128-NEXT:    umull v1.4s, v1.4h, v5.4h
; VBITS_EQ_128-NEXT:    shrn v0.4h, v0.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v0.8h, v6.4s, #16
; VBITS_EQ_128-NEXT:    ldp q2, q5, [x1]
; VBITS_EQ_128-NEXT:    shrn v1.4h, v1.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v1.8h, v7.4s, #16
; VBITS_EQ_128-NEXT:    umull2 v16.4s, v3.8h, v2.8h
; VBITS_EQ_128-NEXT:    umull v2.4s, v3.4h, v2.4h
; VBITS_EQ_128-NEXT:    stp q0, q1, [x0, #32]
; VBITS_EQ_128-NEXT:    umull2 v3.4s, v4.8h, v5.8h
; VBITS_EQ_128-NEXT:    umull v4.4s, v4.4h, v5.4h
; VBITS_EQ_128-NEXT:    shrn v2.4h, v2.4s, #16
; VBITS_EQ_128-NEXT:    shrn v4.4h, v4.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v2.8h, v16.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v4.8h, v3.4s, #16
; VBITS_EQ_128-NEXT:    stp q2, q4, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_1024-LABEL: umulh_v32i16:
; VBITS_GE_1024:       // %bb.0:
; VBITS_GE_1024-NEXT:    ptrue p0.h, vl32
; VBITS_GE_1024-NEXT:    ld1h { z0.h }, p0/z, [x0]
; VBITS_GE_1024-NEXT:    ld1h { z1.h }, p0/z, [x1]
; VBITS_GE_1024-NEXT:    umulh z0.h, p0/m, z0.h, z1.h
; VBITS_GE_1024-NEXT:    st1h { z0.h }, p0, [x0]
; VBITS_GE_1024-NEXT:    ret
  %op1 = load <32 x i16>, <32 x i16>* %a
  %op2 = load <32 x i16>, <32 x i16>* %b
  %1 = zext <32 x i16> %op1 to <32 x i32>
  %2 = zext <32 x i16> %op2 to <32 x i32>
  %mul = mul <32 x i32> %1, %2
  %shr = lshr <32 x i32> %mul, <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %res = trunc <32 x i32> %shr to <32 x i16>
  store <32 x i16> %res, <32 x i16>* %a
  ret void
}

define void @umulh_v64i16(<64 x i16>* %a, <64 x i16>* %b) #0 {
; VBITS_EQ_128-LABEL: umulh_v64i16:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0, #96]
; VBITS_EQ_128-NEXT:    ldp q2, q5, [x1, #96]
; VBITS_EQ_128-NEXT:    umull2 v6.4s, v0.8h, v2.8h
; VBITS_EQ_128-NEXT:    umull v0.4s, v0.4h, v2.4h
; VBITS_EQ_128-NEXT:    ldp q3, q4, [x0, #64]
; VBITS_EQ_128-NEXT:    umull2 v7.4s, v1.8h, v5.8h
; VBITS_EQ_128-NEXT:    umull v1.4s, v1.4h, v5.4h
; VBITS_EQ_128-NEXT:    shrn v0.4h, v0.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v0.8h, v6.4s, #16
; VBITS_EQ_128-NEXT:    ldp q2, q16, [x1, #64]
; VBITS_EQ_128-NEXT:    shrn v1.4h, v1.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v1.8h, v7.4s, #16
; VBITS_EQ_128-NEXT:    umull2 v17.4s, v3.8h, v2.8h
; VBITS_EQ_128-NEXT:    umull v2.4s, v3.4h, v2.4h
; VBITS_EQ_128-NEXT:    ldp q5, q18, [x0, #32]
; VBITS_EQ_128-NEXT:    umull2 v19.4s, v4.8h, v16.8h
; VBITS_EQ_128-NEXT:    umull v4.4s, v4.4h, v16.4h
; VBITS_EQ_128-NEXT:    shrn v2.4h, v2.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v2.8h, v17.4s, #16
; VBITS_EQ_128-NEXT:    ldp q3, q20, [x1, #32]
; VBITS_EQ_128-NEXT:    shrn v4.4h, v4.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v4.8h, v19.4s, #16
; VBITS_EQ_128-NEXT:    umull2 v21.4s, v5.8h, v3.8h
; VBITS_EQ_128-NEXT:    umull v3.4s, v5.4h, v3.4h
; VBITS_EQ_128-NEXT:    ldp q16, q22, [x0]
; VBITS_EQ_128-NEXT:    umull2 v23.4s, v18.8h, v20.8h
; VBITS_EQ_128-NEXT:    umull v18.4s, v18.4h, v20.4h
; VBITS_EQ_128-NEXT:    shrn v3.4h, v3.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v3.8h, v21.4s, #16
; VBITS_EQ_128-NEXT:    ldp q5, q24, [x1]
; VBITS_EQ_128-NEXT:    shrn v18.4h, v18.4s, #16
; VBITS_EQ_128-NEXT:    stp q2, q4, [x0, #64]
; VBITS_EQ_128-NEXT:    stp q0, q1, [x0, #96]
; VBITS_EQ_128-NEXT:    shrn2 v18.8h, v23.4s, #16
; VBITS_EQ_128-NEXT:    umull v20.4s, v16.4h, v5.4h
; VBITS_EQ_128-NEXT:    umull2 v5.4s, v16.8h, v5.8h
; VBITS_EQ_128-NEXT:    stp q3, q18, [x0, #32]
; VBITS_EQ_128-NEXT:    umull v25.4s, v22.4h, v24.4h
; VBITS_EQ_128-NEXT:    umull2 v16.4s, v22.8h, v24.8h
; VBITS_EQ_128-NEXT:    shrn v20.4h, v20.4s, #16
; VBITS_EQ_128-NEXT:    shrn v22.4h, v25.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v20.8h, v5.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v22.8h, v16.4s, #16
; VBITS_EQ_128-NEXT:    stp q20, q22, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_1024-LABEL: umulh_v64i16:
; VBITS_GE_1024:       // %bb.0:
; VBITS_GE_1024-NEXT:    ptrue p0.h, vl64
; VBITS_GE_1024-NEXT:    ld1h { z0.h }, p0/z, [x0]
; VBITS_GE_1024-NEXT:    ld1h { z1.h }, p0/z, [x1]
; VBITS_GE_1024-NEXT:    umulh z0.h, p0/m, z0.h, z1.h
; VBITS_GE_1024-NEXT:    st1h { z0.h }, p0, [x0]
; VBITS_GE_1024-NEXT:    ret
  %op1 = load <64 x i16>, <64 x i16>* %a
  %op2 = load <64 x i16>, <64 x i16>* %b
  %1 = zext <64 x i16> %op1 to <64 x i32>
  %2 = zext <64 x i16> %op2 to <64 x i32>
  %mul = mul <64 x i32> %1, %2
  %shr = lshr <64 x i32> %mul, <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %res = trunc <64 x i32> %shr to <64 x i16>
  store <64 x i16> %res, <64 x i16>* %a
  ret void
}

define void @umulh_v128i16(<128 x i16>* %a, <128 x i16>* %b) #0 {
; VBITS_EQ_128-LABEL: umulh_v128i16:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    sub sp, sp, #96
; VBITS_EQ_128-NEXT:    .cfi_def_cfa_offset 96
; VBITS_EQ_128-NEXT:    stp d15, d14, [sp, #32] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d13, d12, [sp, #48] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d11, d10, [sp, #64] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d9, d8, [sp, #80] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_offset b8, -8
; VBITS_EQ_128-NEXT:    .cfi_offset b9, -16
; VBITS_EQ_128-NEXT:    .cfi_offset b10, -24
; VBITS_EQ_128-NEXT:    .cfi_offset b11, -32
; VBITS_EQ_128-NEXT:    .cfi_offset b12, -40
; VBITS_EQ_128-NEXT:    .cfi_offset b13, -48
; VBITS_EQ_128-NEXT:    .cfi_offset b14, -56
; VBITS_EQ_128-NEXT:    .cfi_offset b15, -64
; VBITS_EQ_128-NEXT:    ldp q2, q1, [x0, #224]
; VBITS_EQ_128-NEXT:    ldp q6, q3, [x1, #224]
; VBITS_EQ_128-NEXT:    ldp q7, q5, [x0, #192]
; VBITS_EQ_128-NEXT:    umull2 v0.4s, v1.8h, v3.8h
; VBITS_EQ_128-NEXT:    umull v4.4s, v1.4h, v3.4h
; VBITS_EQ_128-NEXT:    str q0, [sp, #16] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    ldp q16, q3, [x1, #192]
; VBITS_EQ_128-NEXT:    umull2 v0.4s, v2.8h, v6.8h
; VBITS_EQ_128-NEXT:    shrn v4.4h, v4.4s, #16
; VBITS_EQ_128-NEXT:    umull v6.4s, v2.4h, v6.4h
; VBITS_EQ_128-NEXT:    str q0, [sp] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    umull2 v2.4s, v5.8h, v3.8h
; VBITS_EQ_128-NEXT:    shrn v6.4h, v6.4s, #16
; VBITS_EQ_128-NEXT:    umull v5.4s, v5.4h, v3.4h
; VBITS_EQ_128-NEXT:    ldp q19, q18, [x0, #160]
; VBITS_EQ_128-NEXT:    umull2 v3.4s, v7.8h, v16.8h
; VBITS_EQ_128-NEXT:    umull v7.4s, v7.4h, v16.4h
; VBITS_EQ_128-NEXT:    shrn v5.4h, v5.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v5.8h, v2.4s, #16
; VBITS_EQ_128-NEXT:    ldp q16, q17, [x1, #160]
; VBITS_EQ_128-NEXT:    shrn v7.4h, v7.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v7.8h, v3.4s, #16
; VBITS_EQ_128-NEXT:    umull2 v31.4s, v19.8h, v16.8h
; VBITS_EQ_128-NEXT:    umull v9.4s, v19.4h, v16.4h
; VBITS_EQ_128-NEXT:    umull2 v21.4s, v18.8h, v17.8h
; VBITS_EQ_128-NEXT:    umull v30.4s, v18.4h, v17.4h
; VBITS_EQ_128-NEXT:    ldp q22, q17, [x0, #128]
; VBITS_EQ_128-NEXT:    shrn v9.4h, v9.4s, #16
; VBITS_EQ_128-NEXT:    shrn v30.4h, v30.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v9.8h, v31.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v30.8h, v21.4s, #16
; VBITS_EQ_128-NEXT:    ldp q19, q20, [x1, #128]
; VBITS_EQ_128-NEXT:    umull2 v16.4s, v17.8h, v20.8h
; VBITS_EQ_128-NEXT:    ldr q21, [sp, #16] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    umull v18.4s, v17.4h, v20.4h
; VBITS_EQ_128-NEXT:    ldp q24, q20, [x0, #96]
; VBITS_EQ_128-NEXT:    umull2 v17.4s, v22.8h, v19.8h
; VBITS_EQ_128-NEXT:    shrn2 v4.8h, v21.4s, #16
; VBITS_EQ_128-NEXT:    umull v19.4s, v22.4h, v19.4h
; VBITS_EQ_128-NEXT:    shrn v2.4h, v18.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v2.8h, v16.4s, #16
; VBITS_EQ_128-NEXT:    ldp q22, q23, [x1, #96]
; VBITS_EQ_128-NEXT:    shrn v3.4h, v19.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v3.8h, v17.4s, #16
; VBITS_EQ_128-NEXT:    umull2 v12.4s, v24.8h, v22.8h
; VBITS_EQ_128-NEXT:    umull v13.4s, v24.4h, v22.4h
; VBITS_EQ_128-NEXT:    umull2 v10.4s, v20.8h, v23.8h
; VBITS_EQ_128-NEXT:    ldr q21, [sp] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    umull v11.4s, v20.4h, v23.4h
; VBITS_EQ_128-NEXT:    ldp q26, q23, [x0, #64]
; VBITS_EQ_128-NEXT:    shrn2 v6.8h, v21.4s, #16
; VBITS_EQ_128-NEXT:    ldp q24, q25, [x1, #64]
; VBITS_EQ_128-NEXT:    umull2 v22.4s, v26.8h, v24.8h
; VBITS_EQ_128-NEXT:    umull v24.4s, v26.4h, v24.4h
; VBITS_EQ_128-NEXT:    umull2 v20.4s, v23.8h, v25.8h
; VBITS_EQ_128-NEXT:    umull v23.4s, v23.4h, v25.4h
; VBITS_EQ_128-NEXT:    ldp q28, q25, [x0, #32]
; VBITS_EQ_128-NEXT:    ldp q26, q27, [x1, #32]
; VBITS_EQ_128-NEXT:    umull2 v15.4s, v28.8h, v26.8h
; VBITS_EQ_128-NEXT:    umull v1.4s, v28.4h, v26.4h
; VBITS_EQ_128-NEXT:    umull2 v14.4s, v25.8h, v27.8h
; VBITS_EQ_128-NEXT:    umull v8.4s, v25.4h, v27.4h
; VBITS_EQ_128-NEXT:    ldp q0, q27, [x0]
; VBITS_EQ_128-NEXT:    shrn v8.4h, v8.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v8.8h, v14.4s, #16
; VBITS_EQ_128-NEXT:    ldp q28, q29, [x1]
; VBITS_EQ_128-NEXT:    stp q3, q2, [x0, #128]
; VBITS_EQ_128-NEXT:    shrn v2.4h, v23.4s, #16
; VBITS_EQ_128-NEXT:    stp q9, q30, [x0, #160]
; VBITS_EQ_128-NEXT:    shrn v3.4h, v24.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v2.8h, v20.4s, #16
; VBITS_EQ_128-NEXT:    stp q7, q5, [x0, #192]
; VBITS_EQ_128-NEXT:    umull2 v26.4s, v0.8h, v28.8h
; VBITS_EQ_128-NEXT:    shrn2 v3.8h, v22.4s, #16
; VBITS_EQ_128-NEXT:    umull v28.4s, v0.4h, v28.4h
; VBITS_EQ_128-NEXT:    stp q6, q4, [x0, #224]
; VBITS_EQ_128-NEXT:    umull2 v25.4s, v27.8h, v29.8h
; VBITS_EQ_128-NEXT:    stp q3, q2, [x0, #64]
; VBITS_EQ_128-NEXT:    umull v27.4s, v27.4h, v29.4h
; VBITS_EQ_128-NEXT:    shrn v29.4h, v1.4s, #16
; VBITS_EQ_128-NEXT:    shrn v0.4h, v13.4s, #16
; VBITS_EQ_128-NEXT:    shrn v1.4h, v11.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v29.8h, v15.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v0.8h, v12.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v1.8h, v10.4s, #16
; VBITS_EQ_128-NEXT:    stp q29, q8, [x0, #32]
; VBITS_EQ_128-NEXT:    stp q0, q1, [x0, #96]
; VBITS_EQ_128-NEXT:    shrn v0.4h, v27.4s, #16
; VBITS_EQ_128-NEXT:    shrn v1.4h, v28.4s, #16
; VBITS_EQ_128-NEXT:    ldp d9, d8, [sp, #80] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    shrn2 v0.8h, v25.4s, #16
; VBITS_EQ_128-NEXT:    shrn2 v1.8h, v26.4s, #16
; VBITS_EQ_128-NEXT:    ldp d11, d10, [sp, #64] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    stp q1, q0, [x0]
; VBITS_EQ_128-NEXT:    ldp d13, d12, [sp, #48] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d15, d14, [sp, #32] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add sp, sp, #96
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_2048-LABEL: umulh_v128i16:
; VBITS_GE_2048:       // %bb.0:
; VBITS_GE_2048-NEXT:    ptrue p0.h, vl128
; VBITS_GE_2048-NEXT:    ld1h { z0.h }, p0/z, [x0]
; VBITS_GE_2048-NEXT:    ld1h { z1.h }, p0/z, [x1]
; VBITS_GE_2048-NEXT:    umulh z0.h, p0/m, z0.h, z1.h
; VBITS_GE_2048-NEXT:    st1h { z0.h }, p0, [x0]
; VBITS_GE_2048-NEXT:    ret
  %op1 = load <128 x i16>, <128 x i16>* %a
  %op2 = load <128 x i16>, <128 x i16>* %b
  %1 = zext <128 x i16> %op1 to <128 x i32>
  %2 = zext <128 x i16> %op2 to <128 x i32>
  %mul = mul <128 x i32> %1, %2
  %shr = lshr <128 x i32> %mul, <i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16, i32 16>
  %res = trunc <128 x i32> %shr to <128 x i16>
  store <128 x i16> %res, <128 x i16>* %a
  ret void
}

; Vector i64 multiplications are not legal for NEON so use SVE when available.
define <2 x i32> @umulh_v2i32(<2 x i32> %op1, <2 x i32> %op2) #0 {
; CHECK-LABEL: umulh_v2i32:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ushll v0.2d, v0.2s, #0
; CHECK-NEXT:    ptrue p0.d, vl2
; CHECK-NEXT:    ushll v1.2d, v1.2s, #0
; CHECK-NEXT:    mul z0.d, p0/m, z0.d, z1.d
; CHECK-NEXT:    shrn v0.2s, v0.2d, #32
; CHECK-NEXT:    ret


  %1 = zext <2 x i32> %op1 to <2 x i64>
  %2 = zext <2 x i32> %op2 to <2 x i64>
  %mul = mul <2 x i64> %1, %2
  %shr = lshr <2 x i64> %mul, <i64 32, i64 32>
  %res = trunc <2 x i64> %shr to <2 x i32>
  ret <2 x i32> %res
}

; Don't use SVE for 128-bit vectors.
define <4 x i32> @umulh_v4i32(<4 x i32> %op1, <4 x i32> %op2) #0 {
; CHECK-LABEL: umulh_v4i32:
; CHECK:       // %bb.0:
; CHECK-NEXT:    umull2 v2.2d, v0.4s, v1.4s
; CHECK-NEXT:    umull v0.2d, v0.2s, v1.2s
; CHECK-NEXT:    uzp2 v0.4s, v0.4s, v2.4s
; CHECK-NEXT:    ret
  %1 = zext <4 x i32> %op1 to <4 x i64>
  %2 = zext <4 x i32> %op2 to <4 x i64>
  %mul = mul <4 x i64> %1, %2
  %shr = lshr <4 x i64> %mul, <i64 32, i64 32, i64 32, i64 32>
  %res = trunc <4 x i64> %shr to <4 x i32>
  ret <4 x i32> %res
}

define void @umulh_v8i32(<8 x i32>* %a, <8 x i32>* %b) #0 {
; VBITS_EQ_128-LABEL: umulh_v8i32:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0]
; VBITS_EQ_128-NEXT:    ptrue p0.d, vl2
; VBITS_EQ_128-NEXT:    ushll v5.2d, v0.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v0.4s, #0
; VBITS_EQ_128-NEXT:    ldp q2, q3, [x1]
; VBITS_EQ_128-NEXT:    ushll v4.2d, v1.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v1.2d, v1.4s, #0
; VBITS_EQ_128-NEXT:    ushll v7.2d, v2.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v2.2d, v2.4s, #0
; VBITS_EQ_128-NEXT:    ushll v6.2d, v3.2s, #0
; VBITS_EQ_128-NEXT:    mul z5.d, p0/m, z5.d, z7.d
; VBITS_EQ_128-NEXT:    ushll2 v3.2d, v3.4s, #0
; VBITS_EQ_128-NEXT:    mul z0.d, p0/m, z0.d, z2.d
; VBITS_EQ_128-NEXT:    mul z4.d, p0/m, z4.d, z6.d
; VBITS_EQ_128-NEXT:    shrn v5.2s, v5.2d, #32
; VBITS_EQ_128-NEXT:    shrn v2.2s, v4.2d, #32
; VBITS_EQ_128-NEXT:    mul z1.d, p0/m, z1.d, z3.d
; VBITS_EQ_128-NEXT:    shrn2 v5.4s, v0.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v2.4s, v1.2d, #32
; VBITS_EQ_128-NEXT:    stp q5, q2, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_256-LABEL: umulh_v8i32:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ptrue p0.s, vl8
; VBITS_GE_256-NEXT:    ld1w { z0.s }, p0/z, [x0]
; VBITS_GE_256-NEXT:    ld1w { z1.s }, p0/z, [x1]
; VBITS_GE_256-NEXT:    umulh z0.s, p0/m, z0.s, z1.s
; VBITS_GE_256-NEXT:    st1w { z0.s }, p0, [x0]
; VBITS_GE_256-NEXT:    ret
  %op1 = load <8 x i32>, <8 x i32>* %a
  %op2 = load <8 x i32>, <8 x i32>* %b
  %insert = insertelement <8 x i64> undef, i64 32, i64 0
  %splat = shufflevector <8 x i64> %insert, <8 x i64> undef, <8 x i32> zeroinitializer
  %1 = zext <8 x i32> %op1 to <8 x i64>
  %2 = zext <8 x i32> %op2 to <8 x i64>
  %mul = mul <8 x i64> %1, %2
  %shr = lshr <8 x i64> %mul, <i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32>
  %res = trunc <8 x i64> %shr to <8 x i32>
  store <8 x i32> %res, <8 x i32>* %a
  ret void
}

define void @umulh_v16i32(<16 x i32>* %a, <16 x i32>* %b) #0 {
; VBITS_EQ_128-LABEL: umulh_v16i32:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q1, q2, [x0, #32]
; VBITS_EQ_128-NEXT:    ptrue p0.d, vl2
; VBITS_EQ_128-NEXT:    ushll v19.2d, v1.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v1.2d, v1.4s, #0
; VBITS_EQ_128-NEXT:    ldp q3, q4, [x0]
; VBITS_EQ_128-NEXT:    ushll v18.2d, v2.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v2.2d, v2.4s, #0
; VBITS_EQ_128-NEXT:    ushll v7.2d, v3.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v3.2d, v3.4s, #0
; VBITS_EQ_128-NEXT:    ldp q5, q6, [x1, #32]
; VBITS_EQ_128-NEXT:    ushll v0.2d, v4.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v4.2d, v4.4s, #0
; VBITS_EQ_128-NEXT:    ushll2 v21.2d, v5.4s, #0
; VBITS_EQ_128-NEXT:    ushll v5.2d, v5.2s, #0
; VBITS_EQ_128-NEXT:    ldp q16, q17, [x1]
; VBITS_EQ_128-NEXT:    ushll2 v22.2d, v6.4s, #0
; VBITS_EQ_128-NEXT:    mul z5.d, p0/m, z5.d, z19.d
; VBITS_EQ_128-NEXT:    ushll v6.2d, v6.2s, #0
; VBITS_EQ_128-NEXT:    mul z1.d, p0/m, z1.d, z21.d
; VBITS_EQ_128-NEXT:    shrn v5.2s, v5.2d, #32
; VBITS_EQ_128-NEXT:    mul z2.d, p0/m, z2.d, z22.d
; VBITS_EQ_128-NEXT:    ushll v19.2d, v16.2s, #0
; VBITS_EQ_128-NEXT:    mul z6.d, p0/m, z6.d, z18.d
; VBITS_EQ_128-NEXT:    ushll2 v16.2d, v16.4s, #0
; VBITS_EQ_128-NEXT:    ushll v20.2d, v17.2s, #0
; VBITS_EQ_128-NEXT:    mul z7.d, p0/m, z7.d, z19.d
; VBITS_EQ_128-NEXT:    ushll2 v17.2d, v17.4s, #0
; VBITS_EQ_128-NEXT:    mul z3.d, p0/m, z3.d, z16.d
; VBITS_EQ_128-NEXT:    mul z0.d, p0/m, z0.d, z20.d
; VBITS_EQ_128-NEXT:    shrn v6.2s, v6.2d, #32
; VBITS_EQ_128-NEXT:    shrn v7.2s, v7.2d, #32
; VBITS_EQ_128-NEXT:    shrn v0.2s, v0.2d, #32
; VBITS_EQ_128-NEXT:    mul z4.d, p0/m, z4.d, z17.d
; VBITS_EQ_128-NEXT:    shrn2 v5.4s, v1.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v6.4s, v2.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v7.4s, v3.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v0.4s, v4.2d, #32
; VBITS_EQ_128-NEXT:    stp q5, q6, [x0, #32]
; VBITS_EQ_128-NEXT:    stp q7, q0, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_1024-LABEL: umulh_v16i32:
; VBITS_GE_1024:       // %bb.0:
; VBITS_GE_1024-NEXT:    ptrue p0.s, vl16
; VBITS_GE_1024-NEXT:    ld1w { z0.s }, p0/z, [x0]
; VBITS_GE_1024-NEXT:    ld1w { z1.s }, p0/z, [x1]
; VBITS_GE_1024-NEXT:    umulh z0.s, p0/m, z0.s, z1.s
; VBITS_GE_1024-NEXT:    st1w { z0.s }, p0, [x0]
; VBITS_GE_1024-NEXT:    ret
  %op1 = load <16 x i32>, <16 x i32>* %a
  %op2 = load <16 x i32>, <16 x i32>* %b
  %1 = zext <16 x i32> %op1 to <16 x i64>
  %2 = zext <16 x i32> %op2 to <16 x i64>
  %mul = mul <16 x i64> %1, %2
  %shr = lshr <16 x i64> %mul, <i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32>
  %res = trunc <16 x i64> %shr to <16 x i32>
  store <16 x i32> %res, <16 x i32>* %a
  ret void
}

define void @umulh_v32i32(<32 x i32>* %a, <32 x i32>* %b) #0 {
; VBITS_EQ_128-LABEL: umulh_v32i32:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    str d10, [sp, #-32]! // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_def_cfa_offset 32
; VBITS_EQ_128-NEXT:    stp d9, d8, [sp, #16] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_offset b8, -8
; VBITS_EQ_128-NEXT:    .cfi_offset b9, -16
; VBITS_EQ_128-NEXT:    .cfi_offset b10, -32
; VBITS_EQ_128-NEXT:    ldp q17, q16, [x0, #64]
; VBITS_EQ_128-NEXT:    ptrue p0.d, vl2
; VBITS_EQ_128-NEXT:    ushll v27.2d, v17.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v29.2d, v17.4s, #0
; VBITS_EQ_128-NEXT:    ldp q23, q28, [x0, #96]
; VBITS_EQ_128-NEXT:    ushll v19.2d, v16.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v22.2d, v16.4s, #0
; VBITS_EQ_128-NEXT:    ushll v31.2d, v23.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v8.2d, v23.4s, #0
; VBITS_EQ_128-NEXT:    ldp q26, q25, [x1, #96]
; VBITS_EQ_128-NEXT:    ushll v30.2d, v28.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v28.2d, v28.4s, #0
; VBITS_EQ_128-NEXT:    ushll2 v9.2d, v26.4s, #0
; VBITS_EQ_128-NEXT:    ushll v26.2d, v26.2s, #0
; VBITS_EQ_128-NEXT:    ldp q24, q21, [x1, #64]
; VBITS_EQ_128-NEXT:    mul z26.d, p0/m, z26.d, z31.d
; VBITS_EQ_128-NEXT:    mul z8.d, p0/m, z8.d, z9.d
; VBITS_EQ_128-NEXT:    ushll2 v10.2d, v25.4s, #0
; VBITS_EQ_128-NEXT:    ushll v25.2d, v25.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v31.2d, v24.4s, #0
; VBITS_EQ_128-NEXT:    mul z28.d, p0/m, z28.d, z10.d
; VBITS_EQ_128-NEXT:    ushll v24.2d, v24.2s, #0
; VBITS_EQ_128-NEXT:    mul z25.d, p0/m, z25.d, z30.d
; VBITS_EQ_128-NEXT:    ldp q7, q5, [x0, #32]
; VBITS_EQ_128-NEXT:    mul z24.d, p0/m, z24.d, z27.d
; VBITS_EQ_128-NEXT:    mul z29.d, p0/m, z29.d, z31.d
; VBITS_EQ_128-NEXT:    ushll2 v30.2d, v21.4s, #0
; VBITS_EQ_128-NEXT:    ushll v21.2d, v21.2s, #0
; VBITS_EQ_128-NEXT:    ushll v6.2d, v7.2s, #0
; VBITS_EQ_128-NEXT:    mul z22.d, p0/m, z22.d, z30.d
; VBITS_EQ_128-NEXT:    mul z19.d, p0/m, z19.d, z21.d
; VBITS_EQ_128-NEXT:    ldp q20, q18, [x1, #32]
; VBITS_EQ_128-NEXT:    ushll v4.2d, v5.2s, #0
; VBITS_EQ_128-NEXT:    shrn v19.2s, v19.2d, #32
; VBITS_EQ_128-NEXT:    ushll2 v5.2d, v5.4s, #0
; VBITS_EQ_128-NEXT:    ushll2 v7.2d, v7.4s, #0
; VBITS_EQ_128-NEXT:    ushll2 v27.2d, v20.4s, #0
; VBITS_EQ_128-NEXT:    ushll v20.2d, v20.2s, #0
; VBITS_EQ_128-NEXT:    ldp q3, q1, [x0]
; VBITS_EQ_128-NEXT:    mul z6.d, p0/m, z6.d, z20.d
; VBITS_EQ_128-NEXT:    mul z7.d, p0/m, z7.d, z27.d
; VBITS_EQ_128-NEXT:    ushll2 v21.2d, v18.4s, #0
; VBITS_EQ_128-NEXT:    shrn v6.2s, v6.2d, #32
; VBITS_EQ_128-NEXT:    ushll v18.2d, v18.2s, #0
; VBITS_EQ_128-NEXT:    ushll v2.2d, v3.2s, #0
; VBITS_EQ_128-NEXT:    mul z5.d, p0/m, z5.d, z21.d
; VBITS_EQ_128-NEXT:    ushll2 v3.2d, v3.4s, #0
; VBITS_EQ_128-NEXT:    mul z4.d, p0/m, z4.d, z18.d
; VBITS_EQ_128-NEXT:    ldp q16, q17, [x1]
; VBITS_EQ_128-NEXT:    ushll v0.2d, v1.2s, #0
; VBITS_EQ_128-NEXT:    shrn v4.2s, v4.2d, #32
; VBITS_EQ_128-NEXT:    ushll2 v1.2d, v1.4s, #0
; VBITS_EQ_128-NEXT:    shrn v18.2s, v24.2d, #32
; VBITS_EQ_128-NEXT:    ushll v20.2d, v16.2s, #0
; VBITS_EQ_128-NEXT:    shrn2 v19.4s, v22.2d, #32
; VBITS_EQ_128-NEXT:    ushll2 v16.2d, v16.4s, #0
; VBITS_EQ_128-NEXT:    ushll v23.2d, v17.2s, #0
; VBITS_EQ_128-NEXT:    mul z2.d, p0/m, z2.d, z20.d
; VBITS_EQ_128-NEXT:    ushll2 v17.2d, v17.4s, #0
; VBITS_EQ_128-NEXT:    mul z3.d, p0/m, z3.d, z16.d
; VBITS_EQ_128-NEXT:    shrn v16.2s, v26.2d, #32
; VBITS_EQ_128-NEXT:    mul z0.d, p0/m, z0.d, z23.d
; VBITS_EQ_128-NEXT:    mul z1.d, p0/m, z1.d, z17.d
; VBITS_EQ_128-NEXT:    shrn v0.2s, v0.2d, #32
; VBITS_EQ_128-NEXT:    shrn v2.2s, v2.2d, #32
; VBITS_EQ_128-NEXT:    shrn v17.2s, v25.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v16.4s, v8.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v18.4s, v29.2d, #32
; VBITS_EQ_128-NEXT:    ldp d9, d8, [sp, #16] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    shrn2 v17.4s, v28.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v6.4s, v7.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v4.4s, v5.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v2.4s, v3.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v0.4s, v1.2d, #32
; VBITS_EQ_128-NEXT:    stp q18, q19, [x0, #64]
; VBITS_EQ_128-NEXT:    stp q6, q4, [x0, #32]
; VBITS_EQ_128-NEXT:    stp q2, q0, [x0]
; VBITS_EQ_128-NEXT:    stp q16, q17, [x0, #96]
; VBITS_EQ_128-NEXT:    ldr d10, [sp], #32 // 8-byte Folded Reload
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_1024-LABEL: umulh_v32i32:
; VBITS_GE_1024:       // %bb.0:
; VBITS_GE_1024-NEXT:    ptrue p0.s, vl32
; VBITS_GE_1024-NEXT:    ld1w { z0.s }, p0/z, [x0]
; VBITS_GE_1024-NEXT:    ld1w { z1.s }, p0/z, [x1]
; VBITS_GE_1024-NEXT:    umulh z0.s, p0/m, z0.s, z1.s
; VBITS_GE_1024-NEXT:    st1w { z0.s }, p0, [x0]
; VBITS_GE_1024-NEXT:    ret
  %op1 = load <32 x i32>, <32 x i32>* %a
  %op2 = load <32 x i32>, <32 x i32>* %b
  %1 = zext <32 x i32> %op1 to <32 x i64>
  %2 = zext <32 x i32> %op2 to <32 x i64>
  %mul = mul <32 x i64> %1, %2
  %shr = lshr <32 x i64> %mul, <i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32>
  %res = trunc <32 x i64> %shr to <32 x i32>
  store <32 x i32> %res, <32 x i32>* %a
  ret void
}

define void @umulh_v64i32(<64 x i32>* %a, <64 x i32>* %b) #0 {
; VBITS_EQ_128-LABEL: umulh_v64i32:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    stp d15, d14, [sp, #-80]! // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_def_cfa_offset 80
; VBITS_EQ_128-NEXT:    stp d13, d12, [sp, #16] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d11, d10, [sp, #32] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d9, d8, [sp, #48] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    str x29, [sp, #64] // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_offset w29, -16
; VBITS_EQ_128-NEXT:    .cfi_offset b8, -24
; VBITS_EQ_128-NEXT:    .cfi_offset b9, -32
; VBITS_EQ_128-NEXT:    .cfi_offset b10, -40
; VBITS_EQ_128-NEXT:    .cfi_offset b11, -48
; VBITS_EQ_128-NEXT:    .cfi_offset b12, -56
; VBITS_EQ_128-NEXT:    .cfi_offset b13, -64
; VBITS_EQ_128-NEXT:    .cfi_offset b14, -72
; VBITS_EQ_128-NEXT:    .cfi_offset b15, -80
; VBITS_EQ_128-NEXT:    addvl sp, sp, #-12
; VBITS_EQ_128-NEXT:    .cfi_escape 0x0f, 0x0e, 0x8f, 0x00, 0x11, 0xd0, 0x00, 0x22, 0x11, 0xe0, 0x00, 0x92, 0x2e, 0x00, 0x1e, 0x22 // sp + 80 + 96 * VG
; VBITS_EQ_128-NEXT:    .cfi_escape 0x0f, 0x0e, 0x8f, 0x00, 0x11, 0xa0, 0x01, 0x22, 0x11, 0xe0, 0x00, 0x92, 0x2e, 0x00, 0x1e, 0x22 // sp + 160 + 96 * VG
; VBITS_EQ_128-NEXT:    ldp q4, q5, [x0, #96]
; VBITS_EQ_128-NEXT:    ptrue p0.d, vl2
; VBITS_EQ_128-NEXT:    stp q5, q4, [sp, #-80]! // 32-byte Folded Spill
; VBITS_EQ_128-NEXT:    ldp q0, q2, [x0, #48]
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ldr q1, [x0, #32]
; VBITS_EQ_128-NEXT:    ldr q3, [x0, #80]
; VBITS_EQ_128-NEXT:    str q1, [sp, #64] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    ushll v1.2d, v0.2s, #0
; VBITS_EQ_128-NEXT:    stp q3, q2, [sp, #32] // 32-byte Folded Spill
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v0.4s, #0
; VBITS_EQ_128-NEXT:    str z1, [x8, #11, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    str z0, [x8, #10, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v2.4s, #0
; VBITS_EQ_128-NEXT:    str z0, [x8, #9, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v3.4s, #0
; VBITS_EQ_128-NEXT:    ldp q23, q26, [x0, #128]
; VBITS_EQ_128-NEXT:    str z0, [x8, #8, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v4.4s, #0
; VBITS_EQ_128-NEXT:    str z0, [x8, #7, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v5.4s, #0
; VBITS_EQ_128-NEXT:    ldp q25, q24, [x0, #160]
; VBITS_EQ_128-NEXT:    str z0, [x8, #6, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v23.4s, #0
; VBITS_EQ_128-NEXT:    ushll2 v1.2d, v26.4s, #0
; VBITS_EQ_128-NEXT:    str z0, [x8, #5, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ushll2 v27.2d, v25.4s, #0
; VBITS_EQ_128-NEXT:    ldp q30, q0, [x0, #192]
; VBITS_EQ_128-NEXT:    str z1, [x8, #4, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ushll2 v9.2d, v24.4s, #0
; VBITS_EQ_128-NEXT:    ushll2 v12.2d, v30.4s, #0
; VBITS_EQ_128-NEXT:    ldp q31, q1, [x0, #224]
; VBITS_EQ_128-NEXT:    ushll v11.2d, v0.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v8.2d, v0.4s, #0
; VBITS_EQ_128-NEXT:    ushll v10.2d, v31.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v15.2d, v31.4s, #0
; VBITS_EQ_128-NEXT:    ldp q29, q28, [x1, #224]
; VBITS_EQ_128-NEXT:    ushll2 v18.2d, v1.4s, #0
; VBITS_EQ_128-NEXT:    ushll v31.2d, v1.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v2.2d, v29.4s, #0
; VBITS_EQ_128-NEXT:    ldp q14, q0, [x1, #192]
; VBITS_EQ_128-NEXT:    ushll v1.2d, v28.2s, #0
; VBITS_EQ_128-NEXT:    ushll v20.2d, v0.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v19.2d, v0.4s, #0
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v28.4s, #0
; VBITS_EQ_128-NEXT:    mul z11.d, p0/m, z11.d, z20.d
; VBITS_EQ_128-NEXT:    ldp q21, q22, [x0]
; VBITS_EQ_128-NEXT:    mul z0.d, p0/m, z0.d, z18.d
; VBITS_EQ_128-NEXT:    ushll v18.2d, v29.2s, #0
; VBITS_EQ_128-NEXT:    ushll v20.2d, v14.2s, #0
; VBITS_EQ_128-NEXT:    ldp q4, q13, [x1, #160]
; VBITS_EQ_128-NEXT:    ldp q5, q6, [x1, #128]
; VBITS_EQ_128-NEXT:    ldp q7, q3, [x1, #96]
; VBITS_EQ_128-NEXT:    str z0, [x8, #3, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ldp q17, q16, [x1, #64]
; VBITS_EQ_128-NEXT:    movprfx z0, z31
; VBITS_EQ_128-NEXT:    mul z0.d, p0/m, z0.d, z1.d
; VBITS_EQ_128-NEXT:    str z0, [x8, #1, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    movprfx z0, z15
; VBITS_EQ_128-NEXT:    mul z0.d, p0/m, z0.d, z2.d
; VBITS_EQ_128-NEXT:    ushll v1.2d, v30.2s, #0
; VBITS_EQ_128-NEXT:    str z0, [x8, #2, mul vl] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ldp q2, q29, [x1, #32]
; VBITS_EQ_128-NEXT:    movprfx z15, z10
; VBITS_EQ_128-NEXT:    mul z15.d, p0/m, z15.d, z18.d
; VBITS_EQ_128-NEXT:    movprfx z0, z8
; VBITS_EQ_128-NEXT:    mul z0.d, p0/m, z0.d, z19.d
; VBITS_EQ_128-NEXT:    str z0, [x8] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v14.4s, #0
; VBITS_EQ_128-NEXT:    ldp q19, q18, [x1]
; VBITS_EQ_128-NEXT:    movprfx z10, z12
; VBITS_EQ_128-NEXT:    mul z10.d, p0/m, z10.d, z0.d
; VBITS_EQ_128-NEXT:    movprfx z8, z1
; VBITS_EQ_128-NEXT:    mul z8.d, p0/m, z8.d, z20.d
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v13.4s, #0
; VBITS_EQ_128-NEXT:    ushll v12.2d, v24.2s, #0
; VBITS_EQ_128-NEXT:    ushll v1.2d, v13.2s, #0
; VBITS_EQ_128-NEXT:    mul z9.d, p0/m, z9.d, z0.d
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v4.4s, #0
; VBITS_EQ_128-NEXT:    mul z12.d, p0/m, z12.d, z1.d
; VBITS_EQ_128-NEXT:    ushll v1.2d, v4.2s, #0
; VBITS_EQ_128-NEXT:    mul z27.d, p0/m, z27.d, z0.d
; VBITS_EQ_128-NEXT:    ushll v20.2d, v25.2s, #0
; VBITS_EQ_128-NEXT:    movprfx z13, z20
; VBITS_EQ_128-NEXT:    mul z13.d, p0/m, z13.d, z1.d
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v6.4s, #0
; VBITS_EQ_128-NEXT:    ushll v1.2d, v6.2s, #0
; VBITS_EQ_128-NEXT:    ldr z6, [x8, #4, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    movprfx z14, z6
; VBITS_EQ_128-NEXT:    mul z14.d, p0/m, z14.d, z0.d
; VBITS_EQ_128-NEXT:    ushll v4.2d, v26.2s, #0
; VBITS_EQ_128-NEXT:    movprfx z30, z4
; VBITS_EQ_128-NEXT:    mul z30.d, p0/m, z30.d, z1.d
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v5.4s, #0
; VBITS_EQ_128-NEXT:    ldr z4, [x8, #5, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ushll v1.2d, v5.2s, #0
; VBITS_EQ_128-NEXT:    movprfx z31, z4
; VBITS_EQ_128-NEXT:    mul z31.d, p0/m, z31.d, z0.d
; VBITS_EQ_128-NEXT:    ushll v6.2d, v23.2s, #0
; VBITS_EQ_128-NEXT:    ldr q4, [sp] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v3.4s, #0
; VBITS_EQ_128-NEXT:    movprfx z28, z6
; VBITS_EQ_128-NEXT:    mul z28.d, p0/m, z28.d, z1.d
; VBITS_EQ_128-NEXT:    ushll v1.2d, v3.2s, #0
; VBITS_EQ_128-NEXT:    ldr z3, [x8, #6, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    movprfx z23, z3
; VBITS_EQ_128-NEXT:    mul z23.d, p0/m, z23.d, z0.d
; VBITS_EQ_128-NEXT:    ushll v5.2d, v4.2s, #0
; VBITS_EQ_128-NEXT:    ldr q3, [sp, #16] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    movprfx z20, z5
; VBITS_EQ_128-NEXT:    mul z20.d, p0/m, z20.d, z1.d
; VBITS_EQ_128-NEXT:    ldr z1, [x8, #7, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v7.4s, #0
; VBITS_EQ_128-NEXT:    ushll v4.2d, v7.2s, #0
; VBITS_EQ_128-NEXT:    movprfx z7, z1
; VBITS_EQ_128-NEXT:    mul z7.d, p0/m, z7.d, z0.d
; VBITS_EQ_128-NEXT:    ldr q1, [sp, #32] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ushll v3.2d, v3.2s, #0
; VBITS_EQ_128-NEXT:    movprfx z6, z3
; VBITS_EQ_128-NEXT:    mul z6.d, p0/m, z6.d, z4.d
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v16.4s, #0
; VBITS_EQ_128-NEXT:    ushll v5.2d, v1.2s, #0
; VBITS_EQ_128-NEXT:    ldr z1, [x8, #8, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    movprfx z26, z1
; VBITS_EQ_128-NEXT:    mul z26.d, p0/m, z26.d, z0.d
; VBITS_EQ_128-NEXT:    ldr q1, [sp, #48] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ushll v3.2d, v16.2s, #0
; VBITS_EQ_128-NEXT:    movprfx z24, z5
; VBITS_EQ_128-NEXT:    mul z24.d, p0/m, z24.d, z3.d
; VBITS_EQ_128-NEXT:    ushll v16.2d, v1.2s, #0
; VBITS_EQ_128-NEXT:    ldr z1, [x8, #9, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v17.4s, #0
; VBITS_EQ_128-NEXT:    movprfx z25, z1
; VBITS_EQ_128-NEXT:    mul z25.d, p0/m, z25.d, z0.d
; VBITS_EQ_128-NEXT:    ushll v5.2d, v17.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v29.4s, #0
; VBITS_EQ_128-NEXT:    ushll v17.2d, v29.2s, #0
; VBITS_EQ_128-NEXT:    movprfx z29, z16
; VBITS_EQ_128-NEXT:    mul z29.d, p0/m, z29.d, z5.d
; VBITS_EQ_128-NEXT:    ldr z1, [x8, #10, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    movprfx z4, z1
; VBITS_EQ_128-NEXT:    mul z4.d, p0/m, z4.d, z0.d
; VBITS_EQ_128-NEXT:    ushll v5.2d, v22.2s, #0
; VBITS_EQ_128-NEXT:    ldr z0, [x8, #11, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    ushll2 v16.2d, v22.4s, #0
; VBITS_EQ_128-NEXT:    movprfx z22, z0
; VBITS_EQ_128-NEXT:    mul z22.d, p0/m, z22.d, z17.d
; VBITS_EQ_128-NEXT:    ldr q0, [sp, #64] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ushll v1.2d, v2.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v2.2d, v2.4s, #0
; VBITS_EQ_128-NEXT:    ushll v17.2d, v0.2s, #0
; VBITS_EQ_128-NEXT:    ushll2 v0.2d, v0.4s, #0
; VBITS_EQ_128-NEXT:    ushll v3.2d, v18.2s, #0
; VBITS_EQ_128-NEXT:    mul z1.d, p0/m, z1.d, z17.d
; VBITS_EQ_128-NEXT:    ushll2 v18.2d, v18.4s, #0
; VBITS_EQ_128-NEXT:    mul z0.d, p0/m, z0.d, z2.d
; VBITS_EQ_128-NEXT:    movprfx z2, z5
; VBITS_EQ_128-NEXT:    mul z2.d, p0/m, z2.d, z3.d
; VBITS_EQ_128-NEXT:    mul z18.d, p0/m, z18.d, z16.d
; VBITS_EQ_128-NEXT:    ushll2 v5.2d, v21.4s, #0
; VBITS_EQ_128-NEXT:    ushll2 v16.2d, v19.4s, #0
; VBITS_EQ_128-NEXT:    ushll v17.2d, v19.2s, #0
; VBITS_EQ_128-NEXT:    mul z5.d, p0/m, z5.d, z16.d
; VBITS_EQ_128-NEXT:    shrn v16.2s, v1.2d, #32
; VBITS_EQ_128-NEXT:    ushll v3.2d, v21.2s, #0
; VBITS_EQ_128-NEXT:    shrn v21.2s, v22.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v16.4s, v0.2d, #32
; VBITS_EQ_128-NEXT:    shrn v0.2s, v6.2d, #32
; VBITS_EQ_128-NEXT:    ldr z6, [x8, #1, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    shrn v1.2s, v20.2d, #32
; VBITS_EQ_128-NEXT:    mul z17.d, p0/m, z17.d, z3.d
; VBITS_EQ_128-NEXT:    shrn2 v21.4s, v4.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v0.4s, v7.2d, #32
; VBITS_EQ_128-NEXT:    shrn v3.2s, v13.2d, #32
; VBITS_EQ_128-NEXT:    ldr z19, [x8, #3, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    shrn v4.2s, v12.2d, #32
; VBITS_EQ_128-NEXT:    shrn v6.2s, v6.2d, #32
; VBITS_EQ_128-NEXT:    shrn v7.2s, v15.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v1.4s, v23.2d, #32
; VBITS_EQ_128-NEXT:    ldr z20, [x8, #2, mul vl] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add x8, sp, #80
; VBITS_EQ_128-NEXT:    shrn2 v3.4s, v27.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v4.4s, v9.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v6.4s, v19.2d, #32
; VBITS_EQ_128-NEXT:    shrn v19.2s, v11.2d, #32
; VBITS_EQ_128-NEXT:    ldr z22, [x8] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    stp q16, q21, [x0, #32]
; VBITS_EQ_128-NEXT:    shrn2 v7.4s, v20.2d, #32
; VBITS_EQ_128-NEXT:    shrn v20.2s, v8.2d, #32
; VBITS_EQ_128-NEXT:    stp q0, q1, [x0, #96]
; VBITS_EQ_128-NEXT:    shrn v0.2s, v2.2d, #32
; VBITS_EQ_128-NEXT:    stp q3, q4, [x0, #160]
; VBITS_EQ_128-NEXT:    shrn v3.2s, v24.2d, #32
; VBITS_EQ_128-NEXT:    stp q7, q6, [x0, #224]
; VBITS_EQ_128-NEXT:    shrn v6.2s, v30.2d, #32
; VBITS_EQ_128-NEXT:    shrn v7.2s, v28.2d, #32
; VBITS_EQ_128-NEXT:    shrn v4.2s, v29.2d, #32
; VBITS_EQ_128-NEXT:    shrn v1.2s, v17.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v19.4s, v22.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v20.4s, v10.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v6.4s, v14.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v7.4s, v31.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v3.4s, v26.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v4.4s, v25.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v0.4s, v18.2d, #32
; VBITS_EQ_128-NEXT:    shrn2 v1.4s, v5.2d, #32
; VBITS_EQ_128-NEXT:    stp q7, q6, [x0, #128]
; VBITS_EQ_128-NEXT:    stp q4, q3, [x0, #64]
; VBITS_EQ_128-NEXT:    stp q1, q0, [x0]
; VBITS_EQ_128-NEXT:    stp q20, q19, [x0, #192]
; VBITS_EQ_128-NEXT:    addvl sp, sp, #12
; VBITS_EQ_128-NEXT:    add sp, sp, #80
; VBITS_EQ_128-NEXT:    ldp d9, d8, [sp, #48] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d11, d10, [sp, #32] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d13, d12, [sp, #16] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldr x29, [sp, #64] // 8-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d15, d14, [sp], #80 // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_2048-LABEL: umulh_v64i32:
; VBITS_GE_2048:       // %bb.0:
; VBITS_GE_2048-NEXT:    ptrue p0.s, vl64
; VBITS_GE_2048-NEXT:    ld1w { z0.s }, p0/z, [x0]
; VBITS_GE_2048-NEXT:    ld1w { z1.s }, p0/z, [x1]
; VBITS_GE_2048-NEXT:    umulh z0.s, p0/m, z0.s, z1.s
; VBITS_GE_2048-NEXT:    st1w { z0.s }, p0, [x0]
; VBITS_GE_2048-NEXT:    ret
  %op1 = load <64 x i32>, <64 x i32>* %a
  %op2 = load <64 x i32>, <64 x i32>* %b
  %1 = zext <64 x i32> %op1 to <64 x i64>
  %2 = zext <64 x i32> %op2 to <64 x i64>
  %mul = mul <64 x i64> %1, %2
  %shr = lshr <64 x i64> %mul, <i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32, i64 32>
  %res = trunc <64 x i64> %shr to <64 x i32>
  store <64 x i32> %res, <64 x i32>* %a
  ret void
}

; Vector i64 multiplications are not legal for NEON so use SVE when available.
define <1 x i64> @umulh_v1i64(<1 x i64> %op1, <1 x i64> %op2) #0 {
; VBITS_EQ_128-LABEL: umulh_v1i64:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    // kill: def $d1 killed $d1 def $q1
; VBITS_EQ_128-NEXT:    // kill: def $d0 killed $d0 def $q0
; VBITS_EQ_128-NEXT:    fmov x8, d0
; VBITS_EQ_128-NEXT:    fmov x9, d1
; VBITS_EQ_128-NEXT:    umulh x8, x8, x9
; VBITS_EQ_128-NEXT:    fmov d0, x8
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_256-LABEL: umulh_v1i64:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    // kill: def $d0 killed $d0 def $z0
; VBITS_GE_256-NEXT:    ptrue p0.d, vl1
; VBITS_GE_256-NEXT:    // kill: def $d1 killed $d1 def $z1
; VBITS_GE_256-NEXT:    umulh z0.d, p0/m, z0.d, z1.d
; VBITS_GE_256-NEXT:    // kill: def $d0 killed $d0 killed $z0
; VBITS_GE_256-NEXT:    ret
  %1 = zext <1 x i64> %op1 to <1 x i128>
  %2 = zext <1 x i64> %op2 to <1 x i128>
  %mul = mul <1 x i128> %1, %2
  %shr = lshr <1 x i128> %mul, <i128 64>
  %res = trunc <1 x i128> %shr to <1 x i64>
  ret <1 x i64> %res
}

; Vector i64 multiplications are not legal for NEON so use SVE when available.
define <2 x i64> @umulh_v2i64(<2 x i64> %op1, <2 x i64> %op2) #0 {
; VBITS_EQ_128-LABEL: umulh_v2i64:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    mov x8, v0.d[1]
; VBITS_EQ_128-NEXT:    fmov x10, d0
; VBITS_EQ_128-NEXT:    mov x9, v1.d[1]
; VBITS_EQ_128-NEXT:    fmov x11, d1
; VBITS_EQ_128-NEXT:    umulh x10, x10, x11
; VBITS_EQ_128-NEXT:    umulh x8, x8, x9
; VBITS_EQ_128-NEXT:    fmov d0, x10
; VBITS_EQ_128-NEXT:    fmov d1, x8
; VBITS_EQ_128-NEXT:    mov v0.d[1], v1.d[0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_256-LABEL: umulh_v2i64:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    // kill: def $q0 killed $q0 def $z0
; VBITS_GE_256-NEXT:    ptrue p0.d, vl2
; VBITS_GE_256-NEXT:    // kill: def $q1 killed $q1 def $z1
; VBITS_GE_256-NEXT:    umulh z0.d, p0/m, z0.d, z1.d
; VBITS_GE_256-NEXT:    // kill: def $q0 killed $q0 killed $z0
; VBITS_GE_256-NEXT:    ret
  %1 = zext <2 x i64> %op1 to <2 x i128>
  %2 = zext <2 x i64> %op2 to <2 x i128>
  %mul = mul <2 x i128> %1, %2
  %shr = lshr <2 x i128> %mul, <i128 64, i128 64>
  %res = trunc <2 x i128> %shr to <2 x i64>
  ret <2 x i64> %res
}

define void @umulh_v4i64(<4 x i64>* %a, <4 x i64>* %b) #0 {
; VBITS_EQ_128-LABEL: umulh_v4i64:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0]
; VBITS_EQ_128-NEXT:    mov x10, v0.d[1]
; VBITS_EQ_128-NEXT:    fmov x11, d0
; VBITS_EQ_128-NEXT:    ldp q2, q3, [x1]
; VBITS_EQ_128-NEXT:    mov x8, v1.d[1]
; VBITS_EQ_128-NEXT:    fmov x9, d1
; VBITS_EQ_128-NEXT:    mov x12, v2.d[1]
; VBITS_EQ_128-NEXT:    fmov x13, d2
; VBITS_EQ_128-NEXT:    mov x14, v3.d[1]
; VBITS_EQ_128-NEXT:    fmov x15, d3
; VBITS_EQ_128-NEXT:    umulh x11, x11, x13
; VBITS_EQ_128-NEXT:    umulh x10, x10, x12
; VBITS_EQ_128-NEXT:    umulh x9, x9, x15
; VBITS_EQ_128-NEXT:    umulh x8, x8, x14
; VBITS_EQ_128-NEXT:    fmov d0, x11
; VBITS_EQ_128-NEXT:    fmov d1, x10
; VBITS_EQ_128-NEXT:    fmov d2, x9
; VBITS_EQ_128-NEXT:    fmov d3, x8
; VBITS_EQ_128-NEXT:    mov v0.d[1], v1.d[0]
; VBITS_EQ_128-NEXT:    mov v2.d[1], v3.d[0]
; VBITS_EQ_128-NEXT:    stp q0, q2, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_256-LABEL: umulh_v4i64:
; VBITS_GE_256:       // %bb.0:
; VBITS_GE_256-NEXT:    ptrue p0.d, vl4
; VBITS_GE_256-NEXT:    ld1d { z0.d }, p0/z, [x0]
; VBITS_GE_256-NEXT:    ld1d { z1.d }, p0/z, [x1]
; VBITS_GE_256-NEXT:    umulh z0.d, p0/m, z0.d, z1.d
; VBITS_GE_256-NEXT:    st1d { z0.d }, p0, [x0]
; VBITS_GE_256-NEXT:    ret
  %op1 = load <4 x i64>, <4 x i64>* %a
  %op2 = load <4 x i64>, <4 x i64>* %b
  %1 = zext <4 x i64> %op1 to <4 x i128>
  %2 = zext <4 x i64> %op2 to <4 x i128>
  %mul = mul <4 x i128> %1, %2
  %shr = lshr <4 x i128> %mul, <i128 64, i128 64, i128 64, i128 64>
  %res = trunc <4 x i128> %shr to <4 x i64>
  store <4 x i64> %res, <4 x i64>* %a
  ret void
}

define void @umulh_v8i64(<8 x i64>* %a, <8 x i64>* %b) #0 {
; VBITS_EQ_128-LABEL: umulh_v8i64:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0, #32]
; VBITS_EQ_128-NEXT:    fmov x14, d0
; VBITS_EQ_128-NEXT:    mov x13, v0.d[1]
; VBITS_EQ_128-NEXT:    ldp q2, q3, [x0]
; VBITS_EQ_128-NEXT:    mov x11, v1.d[1]
; VBITS_EQ_128-NEXT:    fmov x12, d1
; VBITS_EQ_128-NEXT:    mov x10, v2.d[1]
; VBITS_EQ_128-NEXT:    ldp q4, q5, [x1, #32]
; VBITS_EQ_128-NEXT:    mov x8, v3.d[1]
; VBITS_EQ_128-NEXT:    fmov x9, d3
; VBITS_EQ_128-NEXT:    fmov x17, d4
; VBITS_EQ_128-NEXT:    mov x15, v4.d[1]
; VBITS_EQ_128-NEXT:    ldp q3, q1, [x1]
; VBITS_EQ_128-NEXT:    fmov x1, d5
; VBITS_EQ_128-NEXT:    umulh x14, x14, x17
; VBITS_EQ_128-NEXT:    mov x18, v5.d[1]
; VBITS_EQ_128-NEXT:    umulh x13, x13, x15
; VBITS_EQ_128-NEXT:    fmov x15, d2
; VBITS_EQ_128-NEXT:    umulh x12, x12, x1
; VBITS_EQ_128-NEXT:    mov x1, v3.d[1]
; VBITS_EQ_128-NEXT:    fmov x17, d1
; VBITS_EQ_128-NEXT:    umulh x11, x11, x18
; VBITS_EQ_128-NEXT:    mov x16, v1.d[1]
; VBITS_EQ_128-NEXT:    fmov d2, x13
; VBITS_EQ_128-NEXT:    fmov d5, x12
; VBITS_EQ_128-NEXT:    umulh x9, x9, x17
; VBITS_EQ_128-NEXT:    fmov x17, d3
; VBITS_EQ_128-NEXT:    umulh x10, x10, x1
; VBITS_EQ_128-NEXT:    fmov d3, x14
; VBITS_EQ_128-NEXT:    umulh x8, x8, x16
; VBITS_EQ_128-NEXT:    fmov d4, x11
; VBITS_EQ_128-NEXT:    umulh x15, x15, x17
; VBITS_EQ_128-NEXT:    fmov d1, x9
; VBITS_EQ_128-NEXT:    fmov d6, x10
; VBITS_EQ_128-NEXT:    fmov d0, x8
; VBITS_EQ_128-NEXT:    fmov d7, x15
; VBITS_EQ_128-NEXT:    mov v3.d[1], v2.d[0]
; VBITS_EQ_128-NEXT:    mov v5.d[1], v4.d[0]
; VBITS_EQ_128-NEXT:    mov v7.d[1], v6.d[0]
; VBITS_EQ_128-NEXT:    mov v1.d[1], v0.d[0]
; VBITS_EQ_128-NEXT:    stp q3, q5, [x0, #32]
; VBITS_EQ_128-NEXT:    stp q7, q1, [x0]
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_1024-LABEL: umulh_v8i64:
; VBITS_GE_1024:       // %bb.0:
; VBITS_GE_1024-NEXT:    ptrue p0.d, vl8
; VBITS_GE_1024-NEXT:    ld1d { z0.d }, p0/z, [x0]
; VBITS_GE_1024-NEXT:    ld1d { z1.d }, p0/z, [x1]
; VBITS_GE_1024-NEXT:    umulh z0.d, p0/m, z0.d, z1.d
; VBITS_GE_1024-NEXT:    st1d { z0.d }, p0, [x0]
; VBITS_GE_1024-NEXT:    ret
  %op1 = load <8 x i64>, <8 x i64>* %a
  %op2 = load <8 x i64>, <8 x i64>* %b
  %1 = zext <8 x i64> %op1 to <8 x i128>
  %2 = zext <8 x i64> %op2 to <8 x i128>
  %mul = mul <8 x i128> %1, %2
  %shr = lshr <8 x i128> %mul, <i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64>
  %res = trunc <8 x i128> %shr to <8 x i64>
  store <8 x i64> %res, <8 x i64>* %a
  ret void
}

define void @umulh_v16i64(<16 x i64>* %a, <16 x i64>* %b) #0 {
; VBITS_EQ_128-LABEL: umulh_v16i64:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    str x21, [sp, #-32]! // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_def_cfa_offset 32
; VBITS_EQ_128-NEXT:    stp x20, x19, [sp, #16] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_offset w19, -8
; VBITS_EQ_128-NEXT:    .cfi_offset w20, -16
; VBITS_EQ_128-NEXT:    .cfi_offset w21, -32
; VBITS_EQ_128-NEXT:    ldp q2, q3, [x0]
; VBITS_EQ_128-NEXT:    mov x10, v2.d[1]
; VBITS_EQ_128-NEXT:    fmov x11, d2
; VBITS_EQ_128-NEXT:    ldp q4, q5, [x0, #32]
; VBITS_EQ_128-NEXT:    mov x8, v3.d[1]
; VBITS_EQ_128-NEXT:    fmov x9, d3
; VBITS_EQ_128-NEXT:    mov x14, v4.d[1]
; VBITS_EQ_128-NEXT:    fmov x15, d4
; VBITS_EQ_128-NEXT:    ldp q0, q1, [x0, #96]
; VBITS_EQ_128-NEXT:    mov x12, v5.d[1]
; VBITS_EQ_128-NEXT:    fmov x13, d5
; VBITS_EQ_128-NEXT:    fmov x5, d0
; VBITS_EQ_128-NEXT:    mov x4, v0.d[1]
; VBITS_EQ_128-NEXT:    ldp q2, q3, [x0, #64]
; VBITS_EQ_128-NEXT:    mov x3, v1.d[1]
; VBITS_EQ_128-NEXT:    mov x18, v2.d[1]
; VBITS_EQ_128-NEXT:    fmov x2, d2
; VBITS_EQ_128-NEXT:    ldp q5, q6, [x1, #96]
; VBITS_EQ_128-NEXT:    mov x16, v3.d[1]
; VBITS_EQ_128-NEXT:    fmov x17, d3
; VBITS_EQ_128-NEXT:    fmov x19, d5
; VBITS_EQ_128-NEXT:    mov x6, v5.d[1]
; VBITS_EQ_128-NEXT:    ldp q4, q7, [x1, #64]
; VBITS_EQ_128-NEXT:    mov x20, v6.d[1]
; VBITS_EQ_128-NEXT:    fmov x21, d6
; VBITS_EQ_128-NEXT:    umulh x5, x5, x19
; VBITS_EQ_128-NEXT:    umulh x4, x4, x6
; VBITS_EQ_128-NEXT:    mov x19, v4.d[1]
; VBITS_EQ_128-NEXT:    fmov x6, d4
; VBITS_EQ_128-NEXT:    umulh x3, x3, x20
; VBITS_EQ_128-NEXT:    ldp q3, q16, [x1, #32]
; VBITS_EQ_128-NEXT:    fmov x20, d7
; VBITS_EQ_128-NEXT:    umulh x2, x2, x6
; VBITS_EQ_128-NEXT:    umulh x18, x18, x19
; VBITS_EQ_128-NEXT:    fmov d18, x4
; VBITS_EQ_128-NEXT:    fmov d19, x5
; VBITS_EQ_128-NEXT:    fmov d20, x3
; VBITS_EQ_128-NEXT:    umulh x17, x17, x20
; VBITS_EQ_128-NEXT:    fmov x19, d3
; VBITS_EQ_128-NEXT:    fmov d23, x2
; VBITS_EQ_128-NEXT:    ldp q2, q17, [x1]
; VBITS_EQ_128-NEXT:    fmov x1, d1
; VBITS_EQ_128-NEXT:    fmov x20, d16
; VBITS_EQ_128-NEXT:    umulh x15, x15, x19
; VBITS_EQ_128-NEXT:    fmov d22, x18
; VBITS_EQ_128-NEXT:    mov v19.d[1], v18.d[0]
; VBITS_EQ_128-NEXT:    umulh x1, x1, x21
; VBITS_EQ_128-NEXT:    mov x21, v7.d[1]
; VBITS_EQ_128-NEXT:    umulh x13, x13, x20
; VBITS_EQ_128-NEXT:    mov x7, v17.d[1]
; VBITS_EQ_128-NEXT:    mov x6, v2.d[1]
; VBITS_EQ_128-NEXT:    mov x20, v16.d[1]
; VBITS_EQ_128-NEXT:    umulh x16, x16, x21
; VBITS_EQ_128-NEXT:    fmov x21, d2
; VBITS_EQ_128-NEXT:    fmov x19, d17
; VBITS_EQ_128-NEXT:    umulh x8, x8, x7
; VBITS_EQ_128-NEXT:    umulh x10, x10, x6
; VBITS_EQ_128-NEXT:    fmov d5, x13
; VBITS_EQ_128-NEXT:    umulh x11, x11, x21
; VBITS_EQ_128-NEXT:    fmov d7, x15
; VBITS_EQ_128-NEXT:    mov x21, v3.d[1]
; VBITS_EQ_128-NEXT:    umulh x9, x9, x19
; VBITS_EQ_128-NEXT:    umulh x12, x12, x20
; VBITS_EQ_128-NEXT:    fmov d0, x8
; VBITS_EQ_128-NEXT:    fmov d2, x10
; VBITS_EQ_128-NEXT:    fmov d16, x16
; VBITS_EQ_128-NEXT:    fmov d3, x11
; VBITS_EQ_128-NEXT:    fmov d17, x17
; VBITS_EQ_128-NEXT:    umulh x14, x14, x21
; VBITS_EQ_128-NEXT:    fmov d1, x9
; VBITS_EQ_128-NEXT:    fmov d4, x12
; VBITS_EQ_128-NEXT:    fmov d21, x1
; VBITS_EQ_128-NEXT:    mov v23.d[1], v22.d[0]
; VBITS_EQ_128-NEXT:    mov v17.d[1], v16.d[0]
; VBITS_EQ_128-NEXT:    fmov d6, x14
; VBITS_EQ_128-NEXT:    mov v21.d[1], v20.d[0]
; VBITS_EQ_128-NEXT:    mov v5.d[1], v4.d[0]
; VBITS_EQ_128-NEXT:    mov v7.d[1], v6.d[0]
; VBITS_EQ_128-NEXT:    stp q23, q17, [x0, #64]
; VBITS_EQ_128-NEXT:    mov v3.d[1], v2.d[0]
; VBITS_EQ_128-NEXT:    mov v1.d[1], v0.d[0]
; VBITS_EQ_128-NEXT:    stp q19, q21, [x0, #96]
; VBITS_EQ_128-NEXT:    ldp x20, x19, [sp, #16] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    stp q7, q5, [x0, #32]
; VBITS_EQ_128-NEXT:    stp q3, q1, [x0]
; VBITS_EQ_128-NEXT:    ldr x21, [sp], #32 // 8-byte Folded Reload
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_1024-LABEL: umulh_v16i64:
; VBITS_GE_1024:       // %bb.0:
; VBITS_GE_1024-NEXT:    ptrue p0.d, vl16
; VBITS_GE_1024-NEXT:    ld1d { z0.d }, p0/z, [x0]
; VBITS_GE_1024-NEXT:    ld1d { z1.d }, p0/z, [x1]
; VBITS_GE_1024-NEXT:    umulh z0.d, p0/m, z0.d, z1.d
; VBITS_GE_1024-NEXT:    st1d { z0.d }, p0, [x0]
; VBITS_GE_1024-NEXT:    ret
  %op1 = load <16 x i64>, <16 x i64>* %a
  %op2 = load <16 x i64>, <16 x i64>* %b
  %1 = zext <16 x i64> %op1 to <16 x i128>
  %2 = zext <16 x i64> %op2 to <16 x i128>
  %mul = mul <16 x i128> %1, %2
  %shr = lshr <16 x i128> %mul, <i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64>
  %res = trunc <16 x i128> %shr to <16 x i64>
  store <16 x i64> %res, <16 x i64>* %a
  ret void
}

define void @umulh_v32i64(<32 x i64>* %a, <32 x i64>* %b) #0 {
; VBITS_EQ_128-LABEL: umulh_v32i64:
; VBITS_EQ_128:       // %bb.0:
; VBITS_EQ_128-NEXT:    sub sp, sp, #224
; VBITS_EQ_128-NEXT:    .cfi_def_cfa_offset 224
; VBITS_EQ_128-NEXT:    stp d15, d14, [sp, #64] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d13, d12, [sp, #80] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d11, d10, [sp, #96] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp d9, d8, [sp, #112] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp x29, x30, [sp, #128] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp x28, x27, [sp, #144] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp x26, x25, [sp, #160] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp x24, x23, [sp, #176] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp x22, x21, [sp, #192] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    stp x20, x19, [sp, #208] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    .cfi_offset w19, -8
; VBITS_EQ_128-NEXT:    .cfi_offset w20, -16
; VBITS_EQ_128-NEXT:    .cfi_offset w21, -24
; VBITS_EQ_128-NEXT:    .cfi_offset w22, -32
; VBITS_EQ_128-NEXT:    .cfi_offset w23, -40
; VBITS_EQ_128-NEXT:    .cfi_offset w24, -48
; VBITS_EQ_128-NEXT:    .cfi_offset w25, -56
; VBITS_EQ_128-NEXT:    .cfi_offset w26, -64
; VBITS_EQ_128-NEXT:    .cfi_offset w27, -72
; VBITS_EQ_128-NEXT:    .cfi_offset w28, -80
; VBITS_EQ_128-NEXT:    .cfi_offset w30, -88
; VBITS_EQ_128-NEXT:    .cfi_offset w29, -96
; VBITS_EQ_128-NEXT:    .cfi_offset b8, -104
; VBITS_EQ_128-NEXT:    .cfi_offset b9, -112
; VBITS_EQ_128-NEXT:    .cfi_offset b10, -120
; VBITS_EQ_128-NEXT:    .cfi_offset b11, -128
; VBITS_EQ_128-NEXT:    .cfi_offset b12, -136
; VBITS_EQ_128-NEXT:    .cfi_offset b13, -144
; VBITS_EQ_128-NEXT:    .cfi_offset b14, -152
; VBITS_EQ_128-NEXT:    .cfi_offset b15, -160
; VBITS_EQ_128-NEXT:    ldp q3, q2, [x0]
; VBITS_EQ_128-NEXT:    mov x8, v3.d[1]
; VBITS_EQ_128-NEXT:    ldp q5, q4, [x0, #64]
; VBITS_EQ_128-NEXT:    fmov x2, d2
; VBITS_EQ_128-NEXT:    str x8, [sp, #16] // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    fmov x8, d3
; VBITS_EQ_128-NEXT:    mov x6, v5.d[1]
; VBITS_EQ_128-NEXT:    fmov x7, d5
; VBITS_EQ_128-NEXT:    str x8, [sp] // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    ldp q6, q3, [x0, #96]
; VBITS_EQ_128-NEXT:    mov x20, v4.d[1]
; VBITS_EQ_128-NEXT:    fmov x21, d4
; VBITS_EQ_128-NEXT:    mov x23, v6.d[1]
; VBITS_EQ_128-NEXT:    fmov x24, d6
; VBITS_EQ_128-NEXT:    ldp q16, q4, [x0, #128]
; VBITS_EQ_128-NEXT:    mov x26, v3.d[1]
; VBITS_EQ_128-NEXT:    fmov x27, d3
; VBITS_EQ_128-NEXT:    mov x28, v16.d[1]
; VBITS_EQ_128-NEXT:    fmov x25, d16
; VBITS_EQ_128-NEXT:    ldp q7, q5, [x0, #224]
; VBITS_EQ_128-NEXT:    mov x22, v4.d[1]
; VBITS_EQ_128-NEXT:    fmov x19, d4
; VBITS_EQ_128-NEXT:    mov x13, v7.d[1]
; VBITS_EQ_128-NEXT:    fmov x11, d7
; VBITS_EQ_128-NEXT:    ldp q17, q6, [x0, #192]
; VBITS_EQ_128-NEXT:    mov x12, v5.d[1]
; VBITS_EQ_128-NEXT:    fmov x10, d5
; VBITS_EQ_128-NEXT:    mov x17, v17.d[1]
; VBITS_EQ_128-NEXT:    fmov x16, d17
; VBITS_EQ_128-NEXT:    ldp q18, q3, [x0, #160]
; VBITS_EQ_128-NEXT:    mov x15, v6.d[1]
; VBITS_EQ_128-NEXT:    fmov x14, d6
; VBITS_EQ_128-NEXT:    mov x5, v18.d[1]
; VBITS_EQ_128-NEXT:    fmov x4, d18
; VBITS_EQ_128-NEXT:    ldp q19, q16, [x1, #224]
; VBITS_EQ_128-NEXT:    mov x29, v3.d[1]
; VBITS_EQ_128-NEXT:    fmov x18, d3
; VBITS_EQ_128-NEXT:    fmov x8, d19
; VBITS_EQ_128-NEXT:    mov x9, v19.d[1]
; VBITS_EQ_128-NEXT:    ldp q21, q20, [x1, #192]
; VBITS_EQ_128-NEXT:    mov x30, v16.d[1]
; VBITS_EQ_128-NEXT:    umulh x8, x11, x8
; VBITS_EQ_128-NEXT:    umulh x11, x13, x9
; VBITS_EQ_128-NEXT:    fmov x9, d21
; VBITS_EQ_128-NEXT:    str x8, [sp, #48] // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    ldp q22, q18, [x1, #160]
; VBITS_EQ_128-NEXT:    ldp q24, q23, [x1, #128]
; VBITS_EQ_128-NEXT:    ldp q25, q17, [x1, #96]
; VBITS_EQ_128-NEXT:    ldp q26, q6, [x1, #64]
; VBITS_EQ_128-NEXT:    ldp q4, q3, [x1, #32]
; VBITS_EQ_128-NEXT:    ldp q7, q5, [x1]
; VBITS_EQ_128-NEXT:    fmov x1, d16
; VBITS_EQ_128-NEXT:    umulh x10, x10, x1
; VBITS_EQ_128-NEXT:    mov x1, v20.d[1]
; VBITS_EQ_128-NEXT:    ldp q1, q0, [x0, #32]
; VBITS_EQ_128-NEXT:    str x10, [sp, #56] // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    umulh x10, x12, x30
; VBITS_EQ_128-NEXT:    mov x30, v21.d[1]
; VBITS_EQ_128-NEXT:    fmov x3, d1
; VBITS_EQ_128-NEXT:    str x10, [sp, #24] // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    fmov x10, d20
; VBITS_EQ_128-NEXT:    ldr x13, [sp, #16] // 8-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d13, d11, [sp, #48] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    umulh x8, x14, x10
; VBITS_EQ_128-NEXT:    umulh x10, x15, x1
; VBITS_EQ_128-NEXT:    fmov x15, d18
; VBITS_EQ_128-NEXT:    umulh x14, x16, x9
; VBITS_EQ_128-NEXT:    mov x9, v22.d[1]
; VBITS_EQ_128-NEXT:    umulh x16, x17, x30
; VBITS_EQ_128-NEXT:    stp x11, x8, [sp, #32] // 16-byte Folded Spill
; VBITS_EQ_128-NEXT:    fmov x17, d22
; VBITS_EQ_128-NEXT:    mov x8, v18.d[1]
; VBITS_EQ_128-NEXT:    umulh x18, x18, x15
; VBITS_EQ_128-NEXT:    mov x15, v23.d[1]
; VBITS_EQ_128-NEXT:    str x10, [sp, #8] // 8-byte Folded Spill
; VBITS_EQ_128-NEXT:    umulh x4, x4, x17
; VBITS_EQ_128-NEXT:    fmov d8, x16
; VBITS_EQ_128-NEXT:    mov x17, v24.d[1]
; VBITS_EQ_128-NEXT:    umulh x5, x5, x9
; VBITS_EQ_128-NEXT:    umulh x1, x29, x8
; VBITS_EQ_128-NEXT:    fmov x8, d23
; VBITS_EQ_128-NEXT:    fmov x9, d24
; VBITS_EQ_128-NEXT:    umulh x22, x22, x15
; VBITS_EQ_128-NEXT:    fmov x15, d17
; VBITS_EQ_128-NEXT:    fmov d9, x14
; VBITS_EQ_128-NEXT:    umulh x19, x19, x8
; VBITS_EQ_128-NEXT:    ldr d14, [sp, #8] // 8-byte Folded Reload
; VBITS_EQ_128-NEXT:    mov x8, v17.d[1]
; VBITS_EQ_128-NEXT:    umulh x25, x25, x9
; VBITS_EQ_128-NEXT:    mov x9, v25.d[1]
; VBITS_EQ_128-NEXT:    umulh x28, x28, x17
; VBITS_EQ_128-NEXT:    fmov x17, d25
; VBITS_EQ_128-NEXT:    umulh x15, x27, x15
; VBITS_EQ_128-NEXT:    mov x27, v6.d[1]
; VBITS_EQ_128-NEXT:    ldr d15, [sp, #40] // 8-byte Folded Reload
; VBITS_EQ_128-NEXT:    umulh x12, x26, x8
; VBITS_EQ_128-NEXT:    fmov x26, d6
; VBITS_EQ_128-NEXT:    umulh x17, x24, x17
; VBITS_EQ_128-NEXT:    ldr x8, [sp] // 8-byte Folded Reload
; VBITS_EQ_128-NEXT:    mov x24, v26.d[1]
; VBITS_EQ_128-NEXT:    umulh x11, x23, x9
; VBITS_EQ_128-NEXT:    fmov x23, d26
; VBITS_EQ_128-NEXT:    umulh x21, x21, x26
; VBITS_EQ_128-NEXT:    fmov x26, d0
; VBITS_EQ_128-NEXT:    umulh x20, x20, x27
; VBITS_EQ_128-NEXT:    fmov x27, d3
; VBITS_EQ_128-NEXT:    fmov d20, x17
; VBITS_EQ_128-NEXT:    umulh x7, x7, x23
; VBITS_EQ_128-NEXT:    fmov x23, d4
; VBITS_EQ_128-NEXT:    umulh x6, x6, x24
; VBITS_EQ_128-NEXT:    fmov x24, d5
; VBITS_EQ_128-NEXT:    umulh x26, x26, x27
; VBITS_EQ_128-NEXT:    fmov x27, d7
; VBITS_EQ_128-NEXT:    umulh x3, x3, x23
; VBITS_EQ_128-NEXT:    fmov d19, x20
; VBITS_EQ_128-NEXT:    mov x23, v2.d[1]
; VBITS_EQ_128-NEXT:    umulh x2, x2, x24
; VBITS_EQ_128-NEXT:    mov x24, v1.d[1]
; VBITS_EQ_128-NEXT:    umulh x27, x8, x27
; VBITS_EQ_128-NEXT:    mov x29, v0.d[1]
; VBITS_EQ_128-NEXT:    mov x30, v7.d[1]
; VBITS_EQ_128-NEXT:    mov x8, v5.d[1]
; VBITS_EQ_128-NEXT:    mov x9, v4.d[1]
; VBITS_EQ_128-NEXT:    mov x10, v3.d[1]
; VBITS_EQ_128-NEXT:    ldp d10, d12, [sp, #24] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    umulh x30, x13, x30
; VBITS_EQ_128-NEXT:    fmov d0, x27
; VBITS_EQ_128-NEXT:    umulh x8, x23, x8
; VBITS_EQ_128-NEXT:    fmov d2, x2
; VBITS_EQ_128-NEXT:    umulh x9, x24, x9
; VBITS_EQ_128-NEXT:    fmov d4, x3
; VBITS_EQ_128-NEXT:    umulh x10, x29, x10
; VBITS_EQ_128-NEXT:    fmov d6, x26
; VBITS_EQ_128-NEXT:    mov v11.d[1], v10.d[0]
; VBITS_EQ_128-NEXT:    fmov d1, x30
; VBITS_EQ_128-NEXT:    mov v13.d[1], v12.d[0]
; VBITS_EQ_128-NEXT:    mov v15.d[1], v14.d[0]
; VBITS_EQ_128-NEXT:    mov v9.d[1], v8.d[0]
; VBITS_EQ_128-NEXT:    fmov d3, x8
; VBITS_EQ_128-NEXT:    fmov d5, x9
; VBITS_EQ_128-NEXT:    fmov d7, x10
; VBITS_EQ_128-NEXT:    fmov d17, x6
; VBITS_EQ_128-NEXT:    fmov d16, x7
; VBITS_EQ_128-NEXT:    fmov d18, x21
; VBITS_EQ_128-NEXT:    fmov d21, x11
; VBITS_EQ_128-NEXT:    fmov d22, x12
; VBITS_EQ_128-NEXT:    fmov d23, x15
; VBITS_EQ_128-NEXT:    fmov d24, x28
; VBITS_EQ_128-NEXT:    fmov d25, x25
; VBITS_EQ_128-NEXT:    fmov d26, x22
; VBITS_EQ_128-NEXT:    fmov d27, x19
; VBITS_EQ_128-NEXT:    fmov d28, x5
; VBITS_EQ_128-NEXT:    fmov d29, x4
; VBITS_EQ_128-NEXT:    fmov d30, x1
; VBITS_EQ_128-NEXT:    fmov d31, x18
; VBITS_EQ_128-NEXT:    mov v27.d[1], v26.d[0]
; VBITS_EQ_128-NEXT:    stp q9, q15, [x0, #192]
; VBITS_EQ_128-NEXT:    stp q13, q11, [x0, #224]
; VBITS_EQ_128-NEXT:    mov v31.d[1], v30.d[0]
; VBITS_EQ_128-NEXT:    mov v29.d[1], v28.d[0]
; VBITS_EQ_128-NEXT:    mov v25.d[1], v24.d[0]
; VBITS_EQ_128-NEXT:    mov v23.d[1], v22.d[0]
; VBITS_EQ_128-NEXT:    mov v20.d[1], v21.d[0]
; VBITS_EQ_128-NEXT:    mov v18.d[1], v19.d[0]
; VBITS_EQ_128-NEXT:    stp q29, q31, [x0, #160]
; VBITS_EQ_128-NEXT:    mov v16.d[1], v17.d[0]
; VBITS_EQ_128-NEXT:    stp q25, q27, [x0, #128]
; VBITS_EQ_128-NEXT:    mov v6.d[1], v7.d[0]
; VBITS_EQ_128-NEXT:    mov v4.d[1], v5.d[0]
; VBITS_EQ_128-NEXT:    stp q20, q23, [x0, #96]
; VBITS_EQ_128-NEXT:    mov v2.d[1], v3.d[0]
; VBITS_EQ_128-NEXT:    mov v0.d[1], v1.d[0]
; VBITS_EQ_128-NEXT:    stp q16, q18, [x0, #64]
; VBITS_EQ_128-NEXT:    ldp x20, x19, [sp, #208] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    stp q4, q6, [x0, #32]
; VBITS_EQ_128-NEXT:    ldp x22, x21, [sp, #192] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    stp q0, q2, [x0]
; VBITS_EQ_128-NEXT:    ldp x24, x23, [sp, #176] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp x26, x25, [sp, #160] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp x28, x27, [sp, #144] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp x29, x30, [sp, #128] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d9, d8, [sp, #112] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d11, d10, [sp, #96] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d13, d12, [sp, #80] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    ldp d15, d14, [sp, #64] // 16-byte Folded Reload
; VBITS_EQ_128-NEXT:    add sp, sp, #224
; VBITS_EQ_128-NEXT:    ret
;
; VBITS_GE_2048-LABEL: umulh_v32i64:
; VBITS_GE_2048:       // %bb.0:
; VBITS_GE_2048-NEXT:    ptrue p0.d, vl32
; VBITS_GE_2048-NEXT:    ld1d { z0.d }, p0/z, [x0]
; VBITS_GE_2048-NEXT:    ld1d { z1.d }, p0/z, [x1]
; VBITS_GE_2048-NEXT:    umulh z0.d, p0/m, z0.d, z1.d
; VBITS_GE_2048-NEXT:    st1d { z0.d }, p0, [x0]
; VBITS_GE_2048-NEXT:    ret
  %op1 = load <32 x i64>, <32 x i64>* %a
  %op2 = load <32 x i64>, <32 x i64>* %b
  %1 = zext <32 x i64> %op1 to <32 x i128>
  %2 = zext <32 x i64> %op2 to <32 x i128>
  %mul = mul <32 x i128> %1, %2
  %shr = lshr <32 x i128> %mul, <i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64, i128 64>
  %res = trunc <32 x i128> %shr to <32 x i64>
  store <32 x i64> %res, <32 x i64>* %a
  ret void
}
attributes #0 = { "target-features"="+sve" }
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; VBITS_GE_512: {{.*}}
