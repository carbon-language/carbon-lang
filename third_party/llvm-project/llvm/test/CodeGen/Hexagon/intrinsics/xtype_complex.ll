; RUN: llc -march=hexagon -O0 < %s | FileCheck %s
; RUN: llc -march=hexagon -O0 < %s | FileCheck -check-prefix=CHECK-CALL %s
; Hexagon Programmer's Reference Manual 11.10.3 XTYPE/COMPLEX

; CHECK-CALL-NOT: call

; Complex add/sub halfwords
declare i64 @llvm.hexagon.S4.vxaddsubh(i64, i64)
define i64 @S4_vxaddsubh(i64 %a, i64 %b) {
  %z = call i64 @llvm.hexagon.S4.vxaddsubh(i64 %a, i64 %b)
  ret i64 %z
}
; CHECK: = vxaddsubh({{.*}},{{.*}}):sat

declare i64 @llvm.hexagon.S4.vxsubaddh(i64, i64)
define i64 @S4_vxsubaddh(i64 %a, i64 %b) {
  %z = call i64 @llvm.hexagon.S4.vxsubaddh(i64 %a, i64 %b)
  ret i64 %z
}
; CHECK: = vxsubaddh({{.*}},{{.*}}):sat

declare i64 @llvm.hexagon.S4.vxaddsubhr(i64, i64)
define i64 @S4_vxaddsubhr(i64 %a, i64 %b) {
  %z = call i64 @llvm.hexagon.S4.vxaddsubhr(i64 %a, i64 %b)
  ret i64 %z
}
; CHECK: = vxaddsubh({{.*}},{{.*}}):rnd:>>1:sat

declare i64 @llvm.hexagon.S4.vxsubaddhr(i64, i64)
define i64 @S4_vxsubaddhr(i64 %a, i64 %b) {
  %z = call i64 @llvm.hexagon.S4.vxsubaddhr(i64 %a, i64 %b)
  ret i64 %z
}
; CHECK: = vxsubaddh({{.*}},{{.*}}):rnd:>>1:sat

; Complex add/sub words
declare i64 @llvm.hexagon.S4.vxaddsubw(i64, i64)
define i64 @S4_vxaddsubw(i64 %a, i64 %b) {
  %z = call i64 @llvm.hexagon.S4.vxaddsubw(i64 %a, i64 %b)
  ret i64 %z
}
; CHECK: = vxaddsubw({{.*}},{{.*}}):sat

declare i64 @llvm.hexagon.S4.vxsubaddw(i64, i64)
define i64 @S4_vxsubaddw(i64 %a, i64 %b) {
  %z = call i64 @llvm.hexagon.S4.vxsubaddw(i64 %a, i64 %b)
  ret i64 %z
}
; CHECK: = vxsubaddw({{.*}},{{.*}}):sat

; Complex multiply
declare i64 @llvm.hexagon.M2.cmpys.s0(i32, i32)
define i64 @M2_cmpys_s0(i32 %a, i32 %b) {
  %z = call i64 @llvm.hexagon.M2.cmpys.s0(i32 %a, i32 %b)
  ret i64 %z
}
; CHECK: = cmpy({{.*}},{{.*}}):sat

declare i64 @llvm.hexagon.M2.cmpys.s1(i32, i32)
define i64 @M2_cmpys_s1(i32 %a, i32 %b) {
  %z = call i64 @llvm.hexagon.M2.cmpys.s1(i32 %a, i32 %b)
  ret i64 %z
}
; CHECK: = cmpy({{.*}},{{.*}}):<<1:sat

declare i64 @llvm.hexagon.M2.cmpysc.s0(i32, i32)
define i64 @M2_cmpysc_s0(i32 %a, i32 %b) {
  %z = call i64 @llvm.hexagon.M2.cmpysc.s0(i32 %a, i32 %b)
  ret i64 %z
}
; CHECK: = cmpy({{.*}},{{.*}}*):sat

declare i64 @llvm.hexagon.M2.cmpysc.s1(i32, i32)
define i64 @M2_cmpysc_s1(i32 %a, i32 %b) {
  %z = call i64 @llvm.hexagon.M2.cmpysc.s1(i32 %a, i32 %b)
  ret i64 %z
}
; CHECK: = cmpy({{.*}},{{.*}}*):<<1:sat

declare i64 @llvm.hexagon.M2.cmacs.s0(i64, i32, i32)
define i64 @M2_cmacs_s0(i64 %a, i32 %b, i32 %c) {
  %z = call i64 @llvm.hexagon.M2.cmacs.s0(i64 %a, i32 %b, i32 %c)
  ret i64 %z
}
; CHECK: += cmpy({{.*}},{{.*}}):sat

declare i64 @llvm.hexagon.M2.cmacs.s1(i64, i32, i32)
define i64 @M2_cmacs_s1(i64 %a, i32 %b, i32 %c) {
  %z = call i64 @llvm.hexagon.M2.cmacs.s1(i64 %a, i32 %b, i32 %c)
  ret i64 %z
}
; CHECK: += cmpy({{.*}},{{.*}}):<<1:sat

declare i64 @llvm.hexagon.M2.cnacs.s0(i64, i32, i32)
define i64 @M2_cnacs_s0(i64 %a, i32 %b, i32 %c) {
  %z = call i64 @llvm.hexagon.M2.cnacs.s0(i64 %a, i32 %b, i32 %c)
  ret i64 %z
}
; CHECK: -= cmpy({{.*}},{{.*}}):sat

declare i64 @llvm.hexagon.M2.cnacs.s1(i64, i32, i32)
define i64 @M2_cnacs_s1(i64 %a, i32 %b, i32 %c) {
  %z = call i64 @llvm.hexagon.M2.cnacs.s1(i64 %a, i32 %b, i32 %c)
  ret i64 %z
}
; CHECK: -= cmpy({{.*}},{{.*}}):<<1:sat

declare i64 @llvm.hexagon.M2.cmacsc.s0(i64, i32, i32)
define i64 @M2_cmacsc_s0(i64 %a, i32 %b, i32 %c) {
  %z = call i64 @llvm.hexagon.M2.cmacsc.s0(i64 %a, i32 %b, i32 %c)
  ret i64 %z
}
; CHECK: += cmpy({{.*}},{{.*}}*):sat

declare i64 @llvm.hexagon.M2.cmacsc.s1(i64, i32, i32)
define i64 @M2_cmacsc_s1(i64 %a, i32 %b, i32 %c) {
  %z = call i64 @llvm.hexagon.M2.cmacsc.s1(i64 %a, i32 %b, i32 %c)
  ret i64 %z
}
; CHECK: += cmpy({{.*}},{{.*}}*):<<1:sat

declare i64 @llvm.hexagon.M2.cnacsc.s0(i64, i32, i32)
define i64 @M2_cnacsc_s0(i64 %a, i32 %b, i32 %c) {
  %z = call i64 @llvm.hexagon.M2.cnacsc.s0(i64 %a, i32 %b, i32 %c)
  ret i64 %z
}
; CHECK: -= cmpy({{.*}},{{.*}}*):sat

declare i64 @llvm.hexagon.M2.cnacsc.s1(i64, i32, i32)
define i64 @M2_cnacsc_s1(i64 %a, i32 %b, i32 %c) {
  %z = call i64 @llvm.hexagon.M2.cnacsc.s1(i64 %a, i32 %b, i32 %c)
  ret i64 %z
}
; CHECK: -= cmpy({{.*}},{{.*}}*):<<1:sat

; Complex multiply real or imaginary
declare i64 @llvm.hexagon.M2.cmpyi.s0(i32, i32)
define i64 @M2_cmpyi_s0(i32 %a, i32 %b) {
  %z = call i64 @llvm.hexagon.M2.cmpyi.s0(i32 %a, i32 %b)
  ret i64 %z
}
; CHECK: = cmpyi({{.*}},{{.*}})

declare i64 @llvm.hexagon.M2.cmpyr.s0(i32, i32)
define i64 @M2_cmpyr_s0(i32 %a, i32 %b) {
  %z = call i64 @llvm.hexagon.M2.cmpyr.s0(i32 %a, i32 %b)
  ret i64 %z
}
; CHECK: = cmpyr({{.*}},{{.*}})

declare i64 @llvm.hexagon.M2.cmaci.s0(i64, i32, i32)
define i64 @M2_cmaci_s0(i64 %a, i32 %b, i32 %c) {
  %z = call i64 @llvm.hexagon.M2.cmaci.s0(i64 %a, i32 %b, i32 %c)
  ret i64 %z
}
; CHECK: += cmpyi({{.*}},{{.*}})

declare i64 @llvm.hexagon.M2.cmacr.s0(i64, i32, i32)
define i64 @M2_cmacr_s0(i64 %a, i32 %b, i32 %c) {
  %z = call i64 @llvm.hexagon.M2.cmacr.s0(i64 %a, i32 %b, i32 %c)
  ret i64 %z
}
; CHECK: += cmpyr({{.*}},{{.*}})

; Complex multiply with round and pack
declare i32 @llvm.hexagon.M2.cmpyrs.s0(i32, i32)
define i32 @M2_cmpyrs_s0(i32 %a, i32 %b) {
  %z = call i32 @llvm.hexagon.M2.cmpyrs.s0(i32 %a, i32 %b)
  ret i32 %z
}
; CHECK: = cmpy({{.*}},{{.*}}):rnd:sat

declare i32 @llvm.hexagon.M2.cmpyrs.s1(i32, i32)
define i32 @M2_cmpyrs_s1(i32 %a, i32 %b) {
  %z = call i32 @llvm.hexagon.M2.cmpyrs.s1(i32 %a, i32 %b)
  ret i32 %z
}
; CHECK: = cmpy({{.*}},{{.*}}):<<1:rnd:sat

declare i32 @llvm.hexagon.M2.cmpyrsc.s0(i32, i32)
define i32 @M2_cmpyrsc_s0(i32 %a, i32 %b) {
  %z = call i32 @llvm.hexagon.M2.cmpyrsc.s0(i32 %a, i32 %b)
  ret i32 %z
}
; CHECK: = cmpy({{.*}},{{.*}}*):rnd:sat

declare i32 @llvm.hexagon.M2.cmpyrsc.s1(i32, i32)
define i32 @M2_cmpyrsc_s1(i32 %a, i32 %b) {
  %z = call i32 @llvm.hexagon.M2.cmpyrsc.s1(i32 %a, i32 %b)
  ret i32 %z
}
; CHECK: = cmpy({{.*}},{{.*}}*):<<1:rnd:sat

; Complex multiply 32x16
declare i32 @llvm.hexagon.M4.cmpyi.wh(i64, i32)
define i32 @M4_cmpyi_wh(i64 %a, i32 %b) {
  %z = call i32 @llvm.hexagon.M4.cmpyi.wh(i64 %a, i32 %b)
  ret i32 %z
}
; CHECK: = cmpyiwh({{.*}},{{.*}}):<<1:rnd:sat

declare i32 @llvm.hexagon.M4.cmpyi.whc(i64, i32)
define i32 @M4_cmpyi_whc(i64 %a, i32 %b) {
  %z = call i32 @llvm.hexagon.M4.cmpyi.whc(i64 %a, i32 %b)
  ret i32 %z
}
; CHECK: = cmpyiwh({{.*}},{{.*}}*):<<1:rnd:sat

declare i32 @llvm.hexagon.M4.cmpyr.wh(i64, i32)
define i32 @M4_cmpyr_wh(i64 %a, i32 %b) {
  %z = call i32 @llvm.hexagon.M4.cmpyr.wh(i64 %a, i32 %b)
  ret i32 %z
}
; CHECK: = cmpyrwh({{.*}},{{.*}}):<<1:rnd:sat

declare i32 @llvm.hexagon.M4.cmpyr.whc(i64, i32)
define i32 @M4_cmpyr_whc(i64 %a, i32 %b) {
  %z = call i32 @llvm.hexagon.M4.cmpyr.whc(i64 %a, i32 %b)
  ret i32 %z
}
; CHECK: = cmpyrwh({{.*}},{{.*}}*):<<1:rnd:sat

; Vector complex multiply real or imaginary
declare i64 @llvm.hexagon.M2.vcmpy.s0.sat.r(i64, i64)
define i64 @M2_vcmpy_s0_sat_r(i64 %a, i64 %b) {
  %z = call i64 @llvm.hexagon.M2.vcmpy.s0.sat.r(i64 %a, i64 %b)
  ret i64 %z
}
; CHECK: = vcmpyr({{.*}},{{.*}}):sat

declare i64 @llvm.hexagon.M2.vcmpy.s1.sat.r(i64, i64)
define i64 @M2_vcmpy_s1_sat_r(i64 %a, i64 %b) {
  %z = call i64 @llvm.hexagon.M2.vcmpy.s1.sat.r(i64 %a, i64 %b)
  ret i64 %z
}
; CHECK: = vcmpyr({{.*}},{{.*}}):<<1:sat

declare i64 @llvm.hexagon.M2.vcmpy.s0.sat.i(i64, i64)
define i64 @M2_vcmpy_s0_sat_i(i64 %a, i64 %b) {
  %z = call i64 @llvm.hexagon.M2.vcmpy.s0.sat.i(i64 %a, i64 %b)
  ret i64 %z
}
; CHECK: = vcmpyi({{.*}},{{.*}}):sat

declare i64 @llvm.hexagon.M2.vcmpy.s1.sat.i(i64, i64)
define i64 @M2_vcmpy_s1_sat_i(i64 %a, i64 %b) {
  %z = call i64 @llvm.hexagon.M2.vcmpy.s1.sat.i(i64 %a, i64 %b)
  ret i64 %z
}
; CHECK: = vcmpyi({{.*}},{{.*}}):<<1:sat

declare i64 @llvm.hexagon.M2.vcmac.s0.sat.r(i64, i64, i64)
define i64 @M2_vcmac_s0_sat_r(i64 %a, i64 %b, i64 %c) {
  %z = call i64 @llvm.hexagon.M2.vcmac.s0.sat.r(i64 %a, i64 %b, i64 %c)
  ret i64 %z
}
; CHECK: += vcmpyr({{.*}},r5:4):sat

declare i64 @llvm.hexagon.M2.vcmac.s0.sat.i(i64, i64, i64)
define i64 @M2_vcmac_s0_sat_i(i64 %a, i64 %b, i64 %c) {
  %z = call i64 @llvm.hexagon.M2.vcmac.s0.sat.i(i64 %a, i64 %b, i64 %c)
  ret i64 %z
}
; CHECK: += vcmpyi({{.*}},r5:4):sat

; Vector complex conjugate
declare i64 @llvm.hexagon.A2.vconj(i64)
define i64 @A2_vconj(i64 %a) {
  %z = call i64 @llvm.hexagon.A2.vconj(i64 %a)
  ret i64 %z
}
; CHECK: = vconj({{.*}}):sat

; Vector complex rotate
declare i64 @llvm.hexagon.S2.vcrotate(i64, i32)
define i64 @S2_vcrotate(i64 %a, i32 %b) {
  %z = call i64 @llvm.hexagon.S2.vcrotate(i64 %a, i32 %b)
  ret i64 %z
}
; CHECK: = vcrotate({{.*}},{{.*}})

; Vector reduce complex multiply real or imaginary
declare i64 @llvm.hexagon.M2.vrcmpyi.s0(i64, i64)
define i64 @M2_vrcmpyi_s0(i64 %a, i64 %b) {
  %z = call i64 @llvm.hexagon.M2.vrcmpyi.s0(i64 %a, i64 %b)
  ret i64 %z
}
; CHECK: = vrcmpyi({{.*}},{{.*}})

declare i64 @llvm.hexagon.M2.vrcmpyr.s0(i64, i64)
define i64 @M2_vrcmpyr_s0(i64 %a, i64 %b) {
  %z = call i64 @llvm.hexagon.M2.vrcmpyr.s0(i64 %a, i64 %b)
  ret i64 %z
}
; CHECK: = vrcmpyr({{.*}},{{.*}})

declare i64 @llvm.hexagon.M2.vrcmpyi.s0c(i64, i64)
define i64 @M2_vrcmpyi_s0c(i64 %a, i64 %b) {
  %z = call i64 @llvm.hexagon.M2.vrcmpyi.s0c(i64 %a, i64 %b)
  ret i64 %z
}
; CHECK: = vrcmpyi({{.*}},{{.*}}*)

declare i64 @llvm.hexagon.M2.vrcmpyr.s0c(i64, i64)
define i64 @M2_vrcmpyr_s0c(i64 %a, i64 %b) {
  %z = call i64 @llvm.hexagon.M2.vrcmpyr.s0c(i64 %a, i64 %b)
  ret i64 %z
}
; CHECK: = vrcmpyr({{.*}},{{.*}}*)

declare i64 @llvm.hexagon.M2.vrcmaci.s0(i64, i64, i64)
define i64 @M2_vrcmaci_s0(i64 %a, i64 %b, i64 %c) {
  %z = call i64 @llvm.hexagon.M2.vrcmaci.s0(i64 %a, i64 %b, i64 %c)
  ret i64 %z
}
; CHECK: += vrcmpyi({{.*}},r5:4)

declare i64 @llvm.hexagon.M2.vrcmacr.s0(i64, i64, i64)
define i64 @M2_vrcmacr_s0(i64 %a, i64 %b, i64 %c) {
  %z = call i64 @llvm.hexagon.M2.vrcmacr.s0(i64 %a, i64 %b, i64 %c)
  ret i64 %z
}
; CHECK: += vrcmpyr({{.*}},r5:4)

declare i64 @llvm.hexagon.M2.vrcmaci.s0c(i64, i64, i64)
define i64 @M2_vrcmaci_s0c(i64 %a, i64 %b, i64 %c) {
  %z = call i64 @llvm.hexagon.M2.vrcmaci.s0c(i64 %a, i64 %b, i64 %c)
  ret i64 %z
}
; CHECK: += vrcmpyi({{.*}},r5:4*)

declare i64 @llvm.hexagon.M2.vrcmacr.s0c(i64, i64, i64)
define i64 @M2_vrcmacr_s0c(i64 %a, i64 %b, i64 %c) {
  %z = call i64 @llvm.hexagon.M2.vrcmacr.s0c(i64 %a, i64 %b, i64 %c)
  ret i64 %z
}
; CHECK: += vrcmpyr({{.*}},r5:4*)

; Vector reduce complex rotate
declare i64 @llvm.hexagon.S4.vrcrotate(i64, i32, i32)
define i64 @S4_vrcrotate(i64 %a, i32 %b) {
  %z = call i64 @llvm.hexagon.S4.vrcrotate(i64 %a, i32 %b, i32 0)
  ret i64 %z
}
; CHECK: = vrcrotate({{.*}},{{.*}},#0)

declare i64 @llvm.hexagon.S4.vrcrotate.acc(i64, i64, i32, i32)
define i64 @S4_vrcrotate_acc(i64 %a, i64 %b, i32 %c) {
  %z = call i64 @llvm.hexagon.S4.vrcrotate.acc(i64 %a, i64 %b, i32 %c, i32 0)
  ret i64 %z
}
; CHECK: += vrcrotate({{.*}},{{.*}},#0)
