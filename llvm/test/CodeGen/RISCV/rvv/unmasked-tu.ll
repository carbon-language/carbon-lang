; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: sed 's/iXLen/i32/g' %s | llc -mtriple=riscv32 -mattr=+v,+zfh \
; RUN:   -verify-machineinstrs -target-abi=ilp32d | FileCheck %s --check-prefix=RV32
; RUN: sed 's/iXLen/i64/g' %s | llc -mtriple=riscv64 -mattr=+v,+zfh \
; RUN:   -verify-machineinstrs -target-abi=lp64d | FileCheck %s --check-prefix=RV64

declare <vscale x 1 x i8> @llvm.riscv.vle.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>*,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vle_v_tu_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8>* %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vle_v_tu_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a1, e8, mf8, tu, mu
; RV32-NEXT:    vle8.v v8, (a0)
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vle_v_tu_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a1, e8, mf8, tu, mu
; RV64-NEXT:    vle8.v v8, (a0)
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vle.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8>* %1,
    iXLen %2)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vlse(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>*,
  iXLen,
  iXLen);


define <vscale x 1 x i8> @intrinsic_vlse_v_tu(<vscale x 1 x i8> %0, <vscale x 1 x i8>* %1, iXLen %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vlse_v_tu:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a2, e8, mf8, tu, mu
; RV32-NEXT:    vlse8.v v8, (a0), a1
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vlse_v_tu:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a2, e8, mf8, tu, mu
; RV64-NEXT:    vlse8.v v8, (a0), a1
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vlse(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8>* %1,
    iXLen %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare { <vscale x 1 x i8>, iXLen } @llvm.riscv.vleff(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>*,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vleff_v_tu(<vscale x 1 x i8> %0, <vscale x 1 x i8>* %1, iXLen %2, iXLen* %3) nounwind {
; RV32-LABEL: intrinsic_vleff_v_tu:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a1, e8, mf8, tu, mu
; RV32-NEXT:    vle8ff.v v8, (a0)
; RV32-NEXT:    csrr a0, vl
; RV32-NEXT:    sw a0, 0(a2)
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vleff_v_tu:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a1, e8, mf8, tu, mu
; RV64-NEXT:    vle8ff.v v8, (a0)
; RV64-NEXT:    csrr a0, vl
; RV64-NEXT:    sd a0, 0(a2)
; RV64-NEXT:    ret
entry:
  %a = call { <vscale x 1 x i8>, iXLen } @llvm.riscv.vleff(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8>* %1,
    iXLen %2)
  %b = extractvalue { <vscale x 1 x i8>, iXLen } %a, 0
  %c = extractvalue { <vscale x 1 x i8>, iXLen } %a, 1
  store iXLen %c, iXLen* %3
  ret <vscale x 1 x i8> %b
}

declare <vscale x 1 x i8> @llvm.riscv.vloxei.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>*,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vloxei_v_tu_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8>* %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vloxei_v_tu_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a1, e8, mf8, tu, mu
; RV32-NEXT:    vloxei8.v v8, (a0), v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vloxei_v_tu_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a1, e8, mf8, tu, mu
; RV64-NEXT:    vloxei8.v v8, (a0), v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vloxei.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8>* %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vaadd.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vaadd_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vaadd_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vaadd.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vaadd_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vaadd.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vaadd.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vaaddu.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vaaddu_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vaaddu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vaaddu.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vaaddu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vaaddu.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vaaddu.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vadd.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vadd_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vadd_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vadd.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vadd_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vadd.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vadd.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}
declare <vscale x 1 x i8> @llvm.riscv.vand.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vand_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vand_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vand.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vand_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vand.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vand.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vasub.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vasub_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vasub_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vasub.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vasub_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vasub.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vasub.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vasubu.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vasubu_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vasubu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vasubu.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vasubu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vasubu.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vasubu.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vdiv.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vdiv_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vdiv_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vdiv.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vdiv_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vdiv.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vdiv.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vdivu.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vdivu_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vdivu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vdivu.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vdivu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vdivu.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vdivu.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfadd.nxv1f16.nxv1f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfadd_vv_nxv1f16_nxv1f16_nxv1f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, <vscale x 1 x half> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfadd_vv_nxv1f16_nxv1f16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfadd.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfadd_vv_nxv1f16_nxv1f16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfadd.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfadd.nxv1f16.nxv1f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    <vscale x 1 x half> %2,
    iXLen %3)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfdiv.nxv1f16.nxv1f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfdiv_vv_nxv1f16_nxv1f16_nxv1f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, <vscale x 1 x half> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfdiv_vv_nxv1f16_nxv1f16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfdiv.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfdiv_vv_nxv1f16_nxv1f16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfdiv.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfdiv.nxv1f16.nxv1f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    <vscale x 1 x half> %2,
    iXLen %3)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfmax.nxv1f16.nxv1f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfmax_vv_nxv1f16_nxv1f16_nxv1f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, <vscale x 1 x half> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfmax_vv_nxv1f16_nxv1f16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfmax.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfmax_vv_nxv1f16_nxv1f16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfmax.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfmax.nxv1f16.nxv1f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    <vscale x 1 x half> %2,
    iXLen %3)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfmin.nxv1f16.nxv1f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfmin_vv_nxv1f16_nxv1f16_nxv1f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, <vscale x 1 x half> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfmin_vv_nxv1f16_nxv1f16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfmin.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfmin_vv_nxv1f16_nxv1f16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfmin.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfmin.nxv1f16.nxv1f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    <vscale x 1 x half> %2,
    iXLen %3)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfmul.nxv1f16.nxv1f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfmul_vv_nxv1f16_nxv1f16_nxv1f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, <vscale x 1 x half> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfmul_vv_nxv1f16_nxv1f16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfmul.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfmul_vv_nxv1f16_nxv1f16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfmul.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfmul.nxv1f16.nxv1f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    <vscale x 1 x half> %2,
    iXLen %3)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfrdiv.nxv1f16.f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  half,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfrdiv_vf_nxv1f16_nxv1f16_f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, half %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfrdiv_vf_nxv1f16_nxv1f16_f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfrdiv.vf v8, v9, fa0
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfrdiv_vf_nxv1f16_nxv1f16_f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfrdiv.vf v8, v9, fa0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfrdiv.nxv1f16.f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    half %2,
    iXLen %3)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfsgnj.nxv1f16.nxv1f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfsgnj_vv_nxv1f16_nxv1f16_nxv1f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, <vscale x 1 x half> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfsgnj_vv_nxv1f16_nxv1f16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfsgnj.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfsgnj_vv_nxv1f16_nxv1f16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfsgnj.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfsgnj.nxv1f16.nxv1f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    <vscale x 1 x half> %2,
    iXLen %3)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfsgnjn.nxv1f16.nxv1f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfsgnjn_vv_nxv1f16_nxv1f16_nxv1f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, <vscale x 1 x half> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfsgnjn_vv_nxv1f16_nxv1f16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfsgnjn.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfsgnjn_vv_nxv1f16_nxv1f16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfsgnjn.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfsgnjn.nxv1f16.nxv1f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    <vscale x 1 x half> %2,
    iXLen %3)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfsgnjx.nxv1f16.nxv1f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfsgnjx_vv_nxv1f16_nxv1f16_nxv1f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, <vscale x 1 x half> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfsgnjx_vv_nxv1f16_nxv1f16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfsgnjx.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfsgnjx_vv_nxv1f16_nxv1f16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfsgnjx.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfsgnjx.nxv1f16.nxv1f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    <vscale x 1 x half> %2,
    iXLen %3)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfrsub.nxv1f16.f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  half,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfrsub_vf_nxv1f16_nxv1f16_f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, half %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfrsub_vf_nxv1f16_nxv1f16_f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfrsub.vf v8, v9, fa0
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfrsub_vf_nxv1f16_nxv1f16_f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfrsub.vf v8, v9, fa0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfrsub.nxv1f16.f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    half %2,
    iXLen %3)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfslide1down.nxv1f16.f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  half,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfslide1down_vf_nxv1f16_nxv1f16_f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, half %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfslide1down_vf_nxv1f16_nxv1f16_f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfslide1down.vf v8, v9, fa0
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfslide1down_vf_nxv1f16_nxv1f16_f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfslide1down.vf v8, v9, fa0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfslide1down.nxv1f16.f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    half %2,
    iXLen %3)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfslide1up.nxv1f16.f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  half,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfslide1up_vf_nxv1f16_nxv1f16_f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, half %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfslide1up_vf_nxv1f16_nxv1f16_f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfslide1up.vf v8, v9, fa0
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfslide1up_vf_nxv1f16_nxv1f16_f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfslide1up.vf v8, v9, fa0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfslide1up.nxv1f16.f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    half %2,
    iXLen %3)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x float> @llvm.riscv.vfwsub.nxv1f32.nxv1f16.nxv1f16(
  <vscale x 1 x float>,
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x float> @intrinsic_vfwsub_vv_nxv1f32_nxv1f16_nxv1f16(<vscale x 1 x float> %0, <vscale x 1 x half> %1, <vscale x 1 x half> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfwsub_vv_nxv1f32_nxv1f16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfwsub.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfwsub_vv_nxv1f32_nxv1f16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfwsub.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x float> @llvm.riscv.vfwsub.nxv1f32.nxv1f16.nxv1f16(
    <vscale x 1 x float> %0,
    <vscale x 1 x half> %1,
    <vscale x 1 x half> %2,
    iXLen %3)

  ret <vscale x 1 x float> %a
}

declare <vscale x 1 x float> @llvm.riscv.vfwsub.w.nxv1f32.nxv1f16(
  <vscale x 1 x float>,
  <vscale x 1 x float>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x float> @intrinsic_vfwsub.w_wv_nxv1f32_nxv1f32_nxv1f16(<vscale x 1 x float> %0, <vscale x 1 x float> %1, <vscale x 1 x half> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfwsub.w_wv_nxv1f32_nxv1f32_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfwsub.wv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfwsub.w_wv_nxv1f32_nxv1f32_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfwsub.wv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x float> @llvm.riscv.vfwsub.w.nxv1f32.nxv1f16(
    <vscale x 1 x float> %0,
    <vscale x 1 x float> %1,
    <vscale x 1 x half> %2,
    iXLen %3)

  ret <vscale x 1 x float> %a
}

declare <vscale x 16 x float> @llvm.riscv.vfwsub.w.nxv16f32.nxv16f16(
  <vscale x 16 x float>,
  <vscale x 16 x float>,
  <vscale x 16 x half>,
  iXLen);

define <vscale x 16 x float> @intrinsic_vfwsub.w_wv_nxv16f32_nxv16f32_nxv16f16(<vscale x 16 x float> %0, <vscale x 16 x float> %1, <vscale x 16 x half> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfwsub.w_wv_nxv16f32_nxv16f32_nxv16f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vl4re16.v v24, (a0)
; RV32-NEXT:    vsetvli zero, a1, e16, m4, tu, mu
; RV32-NEXT:    vfwsub.wv v8, v16, v24
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfwsub.w_wv_nxv16f32_nxv16f32_nxv16f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vl4re16.v v24, (a0)
; RV64-NEXT:    vsetvli zero, a1, e16, m4, tu, mu
; RV64-NEXT:    vfwsub.wv v8, v16, v24
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 16 x float> @llvm.riscv.vfwsub.w.nxv16f32.nxv16f16(
    <vscale x 16 x float> %0,
    <vscale x 16 x float> %1,
    <vscale x 16 x half> %2,
    iXLen %3)

  ret <vscale x 16 x float> %a
}

declare <vscale x 1 x float> @llvm.riscv.vfwmul.nxv1f32.nxv1f16.nxv1f16(
  <vscale x 1 x float>,
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x float> @intrinsic_vfwmul_vv_nxv1f32_nxv1f16_nxv1f16(<vscale x 1 x float> %0, <vscale x 1 x half> %1, <vscale x 1 x half> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfwmul_vv_nxv1f32_nxv1f16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfwmul.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfwmul_vv_nxv1f32_nxv1f16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfwmul.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x float> @llvm.riscv.vfwmul.nxv1f32.nxv1f16.nxv1f16(
    <vscale x 1 x float> %0,
    <vscale x 1 x half> %1,
    <vscale x 1 x half> %2,
    iXLen %3)

  ret <vscale x 1 x float> %a
}

declare <vscale x 1 x float> @llvm.riscv.vfwadd.w.nxv1f32.nxv1f16(
  <vscale x 1 x float>,
  <vscale x 1 x float>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x float> @intrinsic_vfwadd.w_wv_nxv1f32_nxv1f32_nxv1f16(<vscale x 1 x float> %0, <vscale x 1 x float> %1, <vscale x 1 x half> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfwadd.w_wv_nxv1f32_nxv1f32_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfwadd.wv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfwadd.w_wv_nxv1f32_nxv1f32_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfwadd.wv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x float> @llvm.riscv.vfwadd.w.nxv1f32.nxv1f16(
    <vscale x 1 x float> %0,
    <vscale x 1 x float> %1,
    <vscale x 1 x half> %2,
    iXLen %3)

  ret <vscale x 1 x float> %a
}

declare <vscale x 1 x float> @llvm.riscv.vfwadd.nxv1f32.nxv1f16.nxv1f16(
  <vscale x 1 x float>,
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x float> @intrinsic_vfwadd_vv_nxv1f32_nxv1f16_nxv1f16(<vscale x 1 x float> %0, <vscale x 1 x half> %1, <vscale x 1 x half> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfwadd_vv_nxv1f32_nxv1f16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfwadd.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfwadd_vv_nxv1f32_nxv1f16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfwadd.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x float> @llvm.riscv.vfwadd.nxv1f32.nxv1f16.nxv1f16(
    <vscale x 1 x float> %0,
    <vscale x 1 x half> %1,
    <vscale x 1 x half> %2,
    iXLen %3)

  ret <vscale x 1 x float> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfsub.nxv1f16.nxv1f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfsub_vv_nxv1f16_nxv1f16_nxv1f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, <vscale x 1 x half> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfsub_vv_nxv1f16_nxv1f16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfsub.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfsub_vv_nxv1f16_nxv1f16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfsub.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfsub.nxv1f16.nxv1f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    <vscale x 1 x half> %2,
    iXLen %3)

  ret <vscale x 1 x half> %a
}


declare <vscale x 1 x i64> @llvm.riscv.vslide1down.nxv1i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  iXLen);

define <vscale x 1 x i64> @intrinsic_vslide1down_vx_nxv1i64_nxv1i64_i64(<vscale x 1 x i64> %0, <vscale x 1 x i64> %1, i64 %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vslide1down_vx_nxv1i64_nxv1i64_i64:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    vsetvli zero, a2, e32, m1, tu, mu
; RV32-NEXT:    vmv1r.v v10, v8
; RV32-NEXT:    vslide1down.vx v10, v9, a0
; RV32-NEXT:    vslide1down.vx v8, v10, a1
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vslide1down_vx_nxv1i64_nxv1i64_i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a1, e64, m1, tu, mu
; RV64-NEXT:    vslide1down.vx v8, v9, a0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i64> @llvm.riscv.vslide1down.nxv1i64(
    <vscale x 1 x i64> %0,
    <vscale x 1 x i64> %1,
    i64 %2,
    iXLen %3)

  ret <vscale x 1 x i64> %a
}

declare <vscale x 1 x i64> @llvm.riscv.vslide1up.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  iXLen);

define <vscale x 1 x i64> @intrinsic_vslide1up_vx_nxv1i64_nxv1i64_i64(<vscale x 1 x i64> %0, <vscale x 1 x i64> %1, i64 %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vslide1up_vx_nxv1i64_nxv1i64_i64:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    vsetvli zero, a2, e32, m1, tu, mu
; RV32-NEXT:    vmv1r.v v10, v8
; RV32-NEXT:    vslide1up.vx v10, v9, a1
; RV32-NEXT:    vslide1up.vx v8, v10, a0
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vslide1up_vx_nxv1i64_nxv1i64_i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a1, e64, m1, tu, mu
; RV64-NEXT:    vslide1up.vx v8, v9, a0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i64> @llvm.riscv.vslide1up.nxv1i64.i64(
    <vscale x 1 x i64> %0,
    <vscale x 1 x i64> %1,
    i64 %2,
    iXLen %3)

  ret <vscale x 1 x i64> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vmax.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vmax_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vmax_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vmax.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vmax_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vmax.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vmax.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vmaxu.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vmaxu_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vmaxu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vmaxu.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vmaxu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vmaxu.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vmaxu.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vmin.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vmin_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vmin_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vmin.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vmin_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vmin.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vmin.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vminu.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vminu_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vminu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vminu.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vminu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vminu.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vminu.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vmul.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vmul_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vmul_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vmul.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vmul_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vmul.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vmul.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vmulh.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vmulh_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vmulh_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vmulh.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vmulh_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vmulh.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vmulh.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vmulhsu.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vmulhsu_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vmulhsu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vmulhsu.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vmulhsu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vmulhsu.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vmulhsu.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vmulhu.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vmulhu_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vmulhu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vmulhu.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vmulhu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vmulhu.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vmulhu.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vnclip.nxv1i8.nxv1i16.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i16>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vnclip_wv_nxv1i8_nxv1i16_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i16> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vnclip_wv_nxv1i8_nxv1i16_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vnclip.wv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vnclip_wv_nxv1i8_nxv1i16_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vnclip.wv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vnclip.nxv1i8.nxv1i16.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i16> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vnclipu.nxv1i8.nxv1i16.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i16>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vnclipu_wv_nxv1i8_nxv1i16_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i16> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vnclipu_wv_nxv1i8_nxv1i16_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vnclipu.wv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vnclipu_wv_nxv1i8_nxv1i16_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vnclipu.wv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vnclipu.nxv1i8.nxv1i16.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i16> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vnsra.nxv1i8.nxv1i16.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i16>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vnsra_wv_nxv1i8_nxv1i16_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i16> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vnsra_wv_nxv1i8_nxv1i16_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vnsra.wv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vnsra_wv_nxv1i8_nxv1i16_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vnsra.wv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vnsra.nxv1i8.nxv1i16.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i16> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vnsrl.nxv1i8.nxv1i16.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i16>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vnsrl_wv_nxv1i8_nxv1i16_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i16> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vnsrl_wv_nxv1i8_nxv1i16_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vnsrl.wv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vnsrl_wv_nxv1i8_nxv1i16_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vnsrl.wv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vnsrl.nxv1i8.nxv1i16.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i16> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vor.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vor_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vor_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vor.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vor_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vor.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vor.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vrem.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vrem_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vrem_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vrem.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vrem_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vrem.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vrem.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vremu.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);
declare <vscale x 1 x i8> @llvm.riscv.vrgather.vv.nxv1i8.i32(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vrgather_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vrgather_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vrgather.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vrgather_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vrgather.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vrgather.vv.nxv1i8.i32(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vrgather.vx.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vrgather_vx_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, iXLen %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vrgather_vx_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a1, e8, mf8, tu, mu
; RV32-NEXT:    vrgather.vx v8, v9, a0
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vrgather_vx_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a1, e8, mf8, tu, mu
; RV64-NEXT:    vrgather.vx v8, v9, a0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vrgather.vx.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    iXLen %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vrgatherei16.vv.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i16>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vrgatherei16_vv_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i16> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vrgatherei16_vv_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vrgatherei16.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vrgatherei16_vv_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vrgatherei16.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vrgatherei16.vv.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i16> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i64> @llvm.riscv.vrsub.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  iXLen);

define <vscale x 1 x i64> @intrinsic_vrsub_vx_nxv1i64_nxv1i64_i64(<vscale x 1 x i64> %0, <vscale x 1 x i64> %1, i64 %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vrsub_vx_nxv1i64_nxv1i64_i64:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    sw a1, 12(sp)
; RV32-NEXT:    sw a0, 8(sp)
; RV32-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vlse64.v v10, (a0), zero
; RV32-NEXT:    vsetvli zero, zero, e64, m1, tu, mu
; RV32-NEXT:    vsub.vv v8, v10, v9
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vrsub_vx_nxv1i64_nxv1i64_i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a1, e64, m1, tu, mu
; RV64-NEXT:    vrsub.vx v8, v9, a0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i64> @llvm.riscv.vrsub.nxv1i64.i64(
    <vscale x 1 x i64> %0,
    <vscale x 1 x i64> %1,
    i64 %2,
    iXLen %3)

  ret <vscale x 1 x i64> %a
}

declare <vscale x 1 x i64> @llvm.riscv.vsadd.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  iXLen);

define <vscale x 1 x i64> @intrinsic_vsadd_vx_nxv1i64_nxv1i64_i64(<vscale x 1 x i64> %0, <vscale x 1 x i64> %1, i64 %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vsadd_vx_nxv1i64_nxv1i64_i64:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    sw a1, 12(sp)
; RV32-NEXT:    sw a0, 8(sp)
; RV32-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vlse64.v v10, (a0), zero
; RV32-NEXT:    vsetvli zero, zero, e64, m1, tu, mu
; RV32-NEXT:    vsadd.vv v8, v9, v10
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vsadd_vx_nxv1i64_nxv1i64_i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a1, e64, m1, tu, mu
; RV64-NEXT:    vsadd.vx v8, v9, a0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i64> @llvm.riscv.vsadd.nxv1i64.i64(
    <vscale x 1 x i64> %0,
    <vscale x 1 x i64> %1,
    i64 %2,
    iXLen %3)

  ret <vscale x 1 x i64> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vsaddu.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vsaddu_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vsaddu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vsaddu.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vsaddu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vsaddu.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vsaddu.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vsll.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vsll_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vsll_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vsll.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vsll_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vsll.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vsll.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vsmul.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vsmul_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vsmul_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vsmul.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vsmul_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vsmul.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vsmul.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i64> @llvm.riscv.vsmul.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  iXLen);

define <vscale x 1 x i64> @intrinsic_vsmul_vx_nxv1i64_nxv1i64_i64(<vscale x 1 x i64> %0, <vscale x 1 x i64> %1, i64 %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vsmul_vx_nxv1i64_nxv1i64_i64:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    sw a1, 12(sp)
; RV32-NEXT:    sw a0, 8(sp)
; RV32-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vlse64.v v10, (a0), zero
; RV32-NEXT:    vsetvli zero, zero, e64, m1, tu, mu
; RV32-NEXT:    vsmul.vv v8, v9, v10
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vsmul_vx_nxv1i64_nxv1i64_i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a1, e64, m1, tu, mu
; RV64-NEXT:    vsmul.vx v8, v9, a0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i64> @llvm.riscv.vsmul.nxv1i64.i64(
    <vscale x 1 x i64> %0,
    <vscale x 1 x i64> %1,
    i64 %2,
    iXLen %3)

  ret <vscale x 1 x i64> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vsra.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vsra_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vsra_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vsra.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vsra_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vsra.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vsra.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}
declare <vscale x 1 x i8> @llvm.riscv.vsrl.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vsrl_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vsrl_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vsrl.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vsrl_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vsrl.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vsrl.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vssra.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vssra_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vssra_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vssra.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vssra_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vssra.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vssra.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vssrl.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vssrl_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vssrl_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vssrl.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vssrl_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vssrl.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vssrl.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vssub.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vssub_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vssub_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vssub.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vssub_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vssub.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vssub.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vssubu.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vssubu_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vssubu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vssubu.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vssubu_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vssubu.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vssubu.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i64> @llvm.riscv.vssub.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  iXLen);

define <vscale x 1 x i64> @intrinsic_vssub_vx_nxv1i64_nxv1i64_i64(<vscale x 1 x i64> %0, <vscale x 1 x i64> %1, i64 %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vssub_vx_nxv1i64_nxv1i64_i64:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    sw a1, 12(sp)
; RV32-NEXT:    sw a0, 8(sp)
; RV32-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vlse64.v v10, (a0), zero
; RV32-NEXT:    vsetvli zero, zero, e64, m1, tu, mu
; RV32-NEXT:    vssub.vv v8, v9, v10
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vssub_vx_nxv1i64_nxv1i64_i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a1, e64, m1, tu, mu
; RV64-NEXT:    vssub.vx v8, v9, a0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i64> @llvm.riscv.vssub.nxv1i64.i64(
    <vscale x 1 x i64> %0,
    <vscale x 1 x i64> %1,
    i64 %2,
    iXLen %3)

  ret <vscale x 1 x i64> %a
}

declare <vscale x 1 x i64> @llvm.riscv.vssubu.nxv1i64.i64(
  <vscale x 1 x i64>,
  <vscale x 1 x i64>,
  i64,
  iXLen);

define <vscale x 1 x i64> @intrinsic_vssubu_vx_nxv1i64_nxv1i64_i64(<vscale x 1 x i64> %0, <vscale x 1 x i64> %1, i64 %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vssubu_vx_nxv1i64_nxv1i64_i64:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    sw a1, 12(sp)
; RV32-NEXT:    sw a0, 8(sp)
; RV32-NEXT:    vsetvli zero, a2, e64, m1, ta, mu
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vlse64.v v10, (a0), zero
; RV32-NEXT:    vsetvli zero, zero, e64, m1, tu, mu
; RV32-NEXT:    vssubu.vv v8, v9, v10
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vssubu_vx_nxv1i64_nxv1i64_i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a1, e64, m1, tu, mu
; RV64-NEXT:    vssubu.vx v8, v9, a0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i64> @llvm.riscv.vssubu.nxv1i64.i64(
    <vscale x 1 x i64> %0,
    <vscale x 1 x i64> %1,
    i64 %2,
    iXLen %3)

  ret <vscale x 1 x i64> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vsub.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vsub_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vsub_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vsub.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vsub_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vsub.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vsub.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i16> @llvm.riscv.vwadd.nxv1i16.nxv1i8.nxv1i8(
  <vscale x 1 x i16>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i16> @intrinsic_vwadd_vv_nxv1i16_nxv1i8_nxv1i8(<vscale x 1 x i16> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vwadd_vv_nxv1i16_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vwadd.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vwadd_vv_nxv1i16_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vwadd.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i16> @llvm.riscv.vwadd.nxv1i16.nxv1i8.nxv1i8(
    <vscale x 1 x i16> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i16> %a
}

declare <vscale x 1 x i16> @llvm.riscv.vwadd.w.nxv1i16.nxv1i8(
  <vscale x 1 x i16>,
  <vscale x 1 x i16>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i16> @intrinsic_vwadd.w_wv_nxv1i16_nxv1i16_nxv1i8(<vscale x 1 x i16> %0, <vscale x 1 x i16> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vwadd.w_wv_nxv1i16_nxv1i16_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vwadd.wv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vwadd.w_wv_nxv1i16_nxv1i16_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vwadd.wv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i16> @llvm.riscv.vwadd.w.nxv1i16.nxv1i8(
    <vscale x 1 x i16> %0,
    <vscale x 1 x i16> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i16> %a
}

declare <vscale x 1 x i16> @llvm.riscv.vwaddu.nxv1i16.nxv1i8.nxv1i8(
  <vscale x 1 x i16>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i16> @intrinsic_vwaddu_vv_nxv1i16_nxv1i8_nxv1i8(<vscale x 1 x i16> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vwaddu_vv_nxv1i16_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vwaddu.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vwaddu_vv_nxv1i16_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vwaddu.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i16> @llvm.riscv.vwaddu.nxv1i16.nxv1i8.nxv1i8(
    <vscale x 1 x i16> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i16> %a
}

declare <vscale x 1 x i16> @llvm.riscv.vwmul.nxv1i16.nxv1i8.nxv1i8(
  <vscale x 1 x i16>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i16> @intrinsic_vwmul_vv_nxv1i16_nxv1i8_nxv1i8(<vscale x 1 x i16> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vwmul_vv_nxv1i16_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vwmul.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vwmul_vv_nxv1i16_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vwmul.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i16> @llvm.riscv.vwmul.nxv1i16.nxv1i8.nxv1i8(
    <vscale x 1 x i16> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i16> %a
}

declare <vscale x 1 x i16> @llvm.riscv.vwmulu.nxv1i16.nxv1i8.nxv1i8(
  <vscale x 1 x i16>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i16> @intrinsic_vwmulu_vv_nxv1i16_nxv1i8_nxv1i8(<vscale x 1 x i16> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vwmulu_vv_nxv1i16_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vwmulu.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vwmulu_vv_nxv1i16_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vwmulu.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i16> @llvm.riscv.vwmulu.nxv1i16.nxv1i8.nxv1i8(
    <vscale x 1 x i16> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i16> %a
}

declare <vscale x 1 x i16> @llvm.riscv.vwmulsu.nxv1i16.nxv1i8.nxv1i8(
  <vscale x 1 x i16>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i16> @intrinsic_vwmulsu_vv_nxv1i16_nxv1i8_nxv1i8(<vscale x 1 x i16> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vwmulsu_vv_nxv1i16_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vwmulsu.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vwmulsu_vv_nxv1i16_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vwmulsu.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i16> @llvm.riscv.vwmulsu.nxv1i16.nxv1i8.nxv1i8(
    <vscale x 1 x i16> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i16> %a
}

declare <vscale x 1 x i16> @llvm.riscv.vwsub.nxv1i16.nxv1i8.nxv1i8(
  <vscale x 1 x i16>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i16> @intrinsic_vwsub_vv_nxv1i16_nxv1i8_nxv1i8(<vscale x 1 x i16> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vwsub_vv_nxv1i16_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vwsub.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vwsub_vv_nxv1i16_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vwsub.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i16> @llvm.riscv.vwsub.nxv1i16.nxv1i8.nxv1i8(
    <vscale x 1 x i16> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i16> %a
}

declare <vscale x 1 x i16> @llvm.riscv.vwsub.w.nxv1i16.nxv1i8(
  <vscale x 1 x i16>,
  <vscale x 1 x i16>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i16> @intrinsic_vwsub.w_wv_nxv1i16_nxv1i16_nxv1i8(<vscale x 1 x i16> %0, <vscale x 1 x i16> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vwsub.w_wv_nxv1i16_nxv1i16_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vwsub.wv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vwsub.w_wv_nxv1i16_nxv1i16_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vwsub.wv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i16> @llvm.riscv.vwsub.w.nxv1i16.nxv1i8(
    <vscale x 1 x i16> %0,
    <vscale x 1 x i16> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i16> %a
}

define <vscale x 1 x i16> @intrinsic_vwsub.w_wv_nxv1i16_nxv1i16_nxv1i8_tied(<vscale x 1 x i16> %0, <vscale x 1 x i8> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vwsub.w_wv_nxv1i16_nxv1i16_nxv1i8_tied:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vwsub.wv v8, v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vwsub.w_wv_nxv1i16_nxv1i16_nxv1i8_tied:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vwsub.wv v8, v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i16> @llvm.riscv.vwsub.w.nxv1i16.nxv1i8(
    <vscale x 1 x i16> %0,
    <vscale x 1 x i16> %0,
    <vscale x 1 x i8> %1,
    iXLen %2)

  ret <vscale x 1 x i16> %a
}

declare <vscale x 1 x i16> @llvm.riscv.vwsubu.nxv1i16.nxv1i8.nxv1i8(
  <vscale x 1 x i16>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i16> @intrinsic_vwsubu_vv_nxv1i16_nxv1i8_nxv1i8(<vscale x 1 x i16> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vwsubu_vv_nxv1i16_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vwsubu.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vwsubu_vv_nxv1i16_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vwsubu.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i16> @llvm.riscv.vwsubu.nxv1i16.nxv1i8.nxv1i8(
    <vscale x 1 x i16> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i16> %a
}

declare <vscale x 1 x i16> @llvm.riscv.vwsubu.w.nxv1i16.nxv1i8(
  <vscale x 1 x i16>,
  <vscale x 1 x i16>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i16> @intrinsic_vwsubu.w_wv_nxv1i16_nxv1i16_nxv1i8(<vscale x 1 x i16> %0, <vscale x 1 x i16> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vwsubu.w_wv_nxv1i16_nxv1i16_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vwsubu.wv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vwsubu.w_wv_nxv1i16_nxv1i16_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vwsubu.wv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i16> @llvm.riscv.vwsubu.w.nxv1i16.nxv1i8(
    <vscale x 1 x i16> %0,
    <vscale x 1 x i16> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i16> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vxor.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vxor_vv_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vxor_vv_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vxor.vv v8, v9, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vxor_vv_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vxor.vv v8, v9, v10
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vxor.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    iXLen %3)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i64> @llvm.riscv.vsext.nxv1i64.nxv1i8(
  <vscale x 1 x i64>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i64> @intrinsic_vsext_vf8_nxv1i64(<vscale x 1 x i64> %0, <vscale x 1 x i8> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vsext_vf8_nxv1i64:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e64, m1, tu, mu
; RV32-NEXT:    vsext.vf8 v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vsext_vf8_nxv1i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e64, m1, tu, mu
; RV64-NEXT:    vsext.vf8 v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i64> @llvm.riscv.vsext.nxv1i64.nxv1i8(
    <vscale x 1 x i64> %0,
    <vscale x 1 x i8> %1,
    iXLen %2)

  ret <vscale x 1 x i64> %a
}

declare <vscale x 1 x i64> @llvm.riscv.vzext.nxv1i64.nxv1i8(
  <vscale x 1 x i64>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i64> @intrinsic_vzext_vf8_nxv1i64(<vscale x 1 x i64> %0, <vscale x 1 x i8> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vzext_vf8_nxv1i64:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e64, m1, tu, mu
; RV32-NEXT:    vzext.vf8 v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vzext_vf8_nxv1i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e64, m1, tu, mu
; RV64-NEXT:    vzext.vf8 v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i64> @llvm.riscv.vzext.nxv1i64.nxv1i8(
    <vscale x 1 x i64> %0,
    <vscale x 1 x i8> %1,
    iXLen %2)

  ret <vscale x 1 x i64> %a
}

declare <vscale x 2 x i16> @llvm.riscv.vfncvt.x.f.w.nxv2i16.nxv2f32(
  <vscale x 2 x i16>,
  <vscale x 2 x float>,
  iXLen);

define <vscale x 2 x i16> @intrinsic_vfncvt_x.f.w_nxv2i16_nxv2f32( <vscale x 2 x i16> %0, <vscale x 2 x float> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfncvt_x.f.w_nxv2i16_nxv2f32:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf2, tu, mu
; RV32-NEXT:    vfncvt.x.f.w v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfncvt_x.f.w_nxv2i16_nxv2f32:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf2, tu, mu
; RV64-NEXT:    vfncvt.x.f.w v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 2 x i16> @llvm.riscv.vfncvt.x.f.w.nxv2i16.nxv2f32(
    <vscale x 2 x i16> %0,
    <vscale x 2 x float> %1,
    iXLen %2)

  ret <vscale x 2 x i16> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vid.nxv1i8(
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vid_v_nxv1i8(<vscale x 1 x i8> %0, iXLen %1) nounwind {
; RV32-LABEL: intrinsic_vid_v_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vid.v v8
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vid_v_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vid.v v8
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vid.nxv1i8(
    <vscale x 1 x i8> %0,
    iXLen %1)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i16> @llvm.riscv.vfclass.nxv1i16(
  <vscale x 1 x i16>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x i16> @intrinsic_vfclass_v_nxv1i16_nxv1f16(
; RV32-LABEL: intrinsic_vfclass_v_nxv1i16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfclass.v v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfclass_v_nxv1i16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfclass.v v8, v9
; RV64-NEXT:    ret
  <vscale x 1 x i16> %0,
  <vscale x 1 x half> %1,
  iXLen %2) nounwind {
entry:
  %a = call <vscale x 1 x i16> @llvm.riscv.vfclass.nxv1i16(
    <vscale x 1 x i16> %0,
    <vscale x 1 x half> %1,
    iXLen %2)

  ret <vscale x 1 x i16> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfcvt.f.x.v.nxv1f16.nxv1i16(
  <vscale x 1 x half>,
  <vscale x 1 x i16>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfcvt_f.x.v_nxv1f16_nxv1i16(<vscale x 1 x half> %0, <vscale x 1 x i16> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfcvt_f.x.v_nxv1f16_nxv1i16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfcvt.f.x.v v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfcvt_f.x.v_nxv1f16_nxv1i16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfcvt.f.x.v v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfcvt.f.x.v.nxv1f16.nxv1i16(
    <vscale x 1 x half> %0,
    <vscale x 1 x i16> %1,
    iXLen %2)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfcvt.f.xu.v.nxv1f16.nxv1i16(
  <vscale x 1 x half>,
  <vscale x 1 x i16>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfcvt_f.xu.v_nxv1f16_nxv1i16(<vscale x 1 x half> %0, <vscale x 1 x i16> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfcvt_f.xu.v_nxv1f16_nxv1i16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfcvt.f.xu.v v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfcvt_f.xu.v_nxv1f16_nxv1i16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfcvt.f.xu.v v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfcvt.f.xu.v.nxv1f16.nxv1i16(
    <vscale x 1 x half> %0,
    <vscale x 1 x i16> %1,
    iXLen %2)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x i16> @llvm.riscv.vfcvt.rtz.x.f.v.nxv1i16.nxv1f16(
  <vscale x 1 x i16>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x i16> @intrinsic_vfcvt_rtz.x.f.v_nxv1i16_nxv1f16(<vscale x 1 x i16> %0, <vscale x 1 x half> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfcvt_rtz.x.f.v_nxv1i16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfcvt.rtz.x.f.v v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfcvt_rtz.x.f.v_nxv1i16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfcvt.rtz.x.f.v v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i16> @llvm.riscv.vfcvt.rtz.x.f.v.nxv1i16.nxv1f16(
    <vscale x 1 x i16> %0,
    <vscale x 1 x half> %1,
    iXLen %2)

  ret <vscale x 1 x i16> %a
}

declare <vscale x 1 x i16> @llvm.riscv.vfcvt.rtz.xu.f.v.nxv1i16.nxv1f16(
  <vscale x 1 x i16>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x i16> @intrinsic_vfcvt_rtz.xu.f.v_nxv1i16_nxv1f16(<vscale x 1 x i16> %0, <vscale x 1 x half> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfcvt_rtz.xu.f.v_nxv1i16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfcvt.rtz.xu.f.v v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfcvt_rtz.xu.f.v_nxv1i16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfcvt.rtz.xu.f.v v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i16> @llvm.riscv.vfcvt.rtz.xu.f.v.nxv1i16.nxv1f16(
    <vscale x 1 x i16> %0,
    <vscale x 1 x half> %1,
    iXLen %2)

  ret <vscale x 1 x i16> %a
}

declare <vscale x 1 x i16> @llvm.riscv.vfcvt.x.f.v.nxv1i16.nxv1f16(
  <vscale x 1 x i16>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x i16> @intrinsic_vfcvt_x.f.v_nxv1i16_nxv1f16(<vscale x 1 x i16> %0, <vscale x 1 x half> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfcvt_x.f.v_nxv1i16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfcvt.x.f.v v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfcvt_x.f.v_nxv1i16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfcvt.x.f.v v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i16> @llvm.riscv.vfcvt.x.f.v.nxv1i16.nxv1f16(
    <vscale x 1 x i16> %0,
    <vscale x 1 x half> %1,
    iXLen %2)

  ret <vscale x 1 x i16> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfncvt.f.f.w.nxv1f16.nxv1f32(
  <vscale x 1 x half>,
  <vscale x 1 x float>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfncvt_f.f.w_nxv1f16_nxv1f32(<vscale x 1 x half> %0, <vscale x 1 x float> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfncvt_f.f.w_nxv1f16_nxv1f32:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfncvt.f.f.w v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfncvt_f.f.w_nxv1f16_nxv1f32:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfncvt.f.f.w v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfncvt.f.f.w.nxv1f16.nxv1f32(
    <vscale x 1 x half> %0,
    <vscale x 1 x float> %1,
    iXLen %2)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x i16> @llvm.riscv.vfcvt.xu.f.v.nxv1i16.nxv1f16(
  <vscale x 1 x i16>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x i16> @intrinsic_vfcvt_xu.f.v_nxv1i16_nxv1f16(<vscale x 1 x i16> %0, <vscale x 1 x half> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfcvt_xu.f.v_nxv1i16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfcvt.xu.f.v v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfcvt_xu.f.v_nxv1i16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfcvt.xu.f.v v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i16> @llvm.riscv.vfcvt.xu.f.v.nxv1i16.nxv1f16(
    <vscale x 1 x i16> %0,
    <vscale x 1 x half> %1,
    iXLen %2)

  ret <vscale x 1 x i16> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfncvt.f.x.w.nxv1f16.nxv1i32(
  <vscale x 1 x half>,
  <vscale x 1 x i32>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfncvt_f.x.w_nxv1f16_nxv1i32(<vscale x 1 x half> %0, <vscale x 1 x i32> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfncvt_f.x.w_nxv1f16_nxv1i32:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfncvt.f.x.w v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfncvt_f.x.w_nxv1f16_nxv1i32:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfncvt.f.x.w v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfncvt.f.x.w.nxv1f16.nxv1i32(
    <vscale x 1 x half> %0,
    <vscale x 1 x i32> %1,
    iXLen %2)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfncvt.f.xu.w.nxv1f16.nxv1i32(
  <vscale x 1 x half>,
  <vscale x 1 x i32>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfncvt_f.xu.w_nxv1f16_nxv1i32(<vscale x 1 x half> %0, <vscale x 1 x i32> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfncvt_f.xu.w_nxv1f16_nxv1i32:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfncvt.f.xu.w v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfncvt_f.xu.w_nxv1f16_nxv1i32:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfncvt.f.xu.w v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfncvt.f.xu.w.nxv1f16.nxv1i32(
    <vscale x 1 x half> %0,
    <vscale x 1 x i32> %1,
    iXLen %2)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfncvt.rod.f.f.w.nxv1f16.nxv1f32(
  <vscale x 1 x half>,
  <vscale x 1 x float>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfncvt_rod.f.f.w_nxv1f16_nxv1f32(<vscale x 1 x half> %0, <vscale x 1 x float> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfncvt_rod.f.f.w_nxv1f16_nxv1f32:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfncvt.rod.f.f.w v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfncvt_rod.f.f.w_nxv1f16_nxv1f32:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfncvt.rod.f.f.w v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfncvt.rod.f.f.w.nxv1f16.nxv1f32(
    <vscale x 1 x half> %0,
    <vscale x 1 x float> %1,
    iXLen %2)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vfncvt.rtz.x.f.w.nxv1i8.nxv1f16(
  <vscale x 1 x i8>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vfncvt_rtz.x.f.w_nxv1i8_nxv1f16(<vscale x 1 x i8> %0, <vscale x 1 x half> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfncvt_rtz.x.f.w_nxv1i8_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vfncvt.rtz.x.f.w v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfncvt_rtz.x.f.w_nxv1i8_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vfncvt.rtz.x.f.w v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vfncvt.rtz.x.f.w.nxv1i8.nxv1f16(
    <vscale x 1 x i8> %0,
    <vscale x 1 x half> %1,
    iXLen %2)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vfncvt.rtz.xu.f.w.nxv1i8.nxv1f16(
  <vscale x 1 x i8>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vfncvt_rtz.xu.f.w_nxv1i8_nxv1f16(<vscale x 1 x i8> %0, <vscale x 1 x half> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfncvt_rtz.xu.f.w_nxv1i8_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vfncvt.rtz.xu.f.w v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfncvt_rtz.xu.f.w_nxv1i8_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vfncvt.rtz.xu.f.w v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vfncvt.rtz.xu.f.w.nxv1i8.nxv1f16(
    <vscale x 1 x i8> %0,
    <vscale x 1 x half> %1,
    iXLen %2)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vfncvt.x.f.w.nxv1i8.nxv1f16(
  <vscale x 1 x i8>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vfncvt_x.f.w_nxv1i8_nxv1f16(<vscale x 1 x i8> %0, <vscale x 1 x half> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfncvt_x.f.w_nxv1i8_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vfncvt.x.f.w v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfncvt_x.f.w_nxv1i8_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vfncvt.x.f.w v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vfncvt.x.f.w.nxv1i8.nxv1f16(
    <vscale x 1 x i8> %0,
    <vscale x 1 x half> %1,
    iXLen %2)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vfncvt.xu.f.w.nxv1i8.nxv1f16(
  <vscale x 1 x i8>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vfncvt_xu.f.w_nxv1i8_nxv1f16(<vscale x 1 x i8> %0, <vscale x 1 x half> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfncvt_xu.f.w_nxv1i8_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vfncvt.xu.f.w v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfncvt_xu.f.w_nxv1i8_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vfncvt.xu.f.w v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vfncvt.xu.f.w.nxv1i8.nxv1f16(
    <vscale x 1 x i8> %0,
    <vscale x 1 x half> %1,
    iXLen %2)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfrec7.nxv1f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfrec7_v_nxv1f16_nxv1f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfrec7_v_nxv1f16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfrec7.v v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfrec7_v_nxv1f16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfrec7.v v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfrec7.nxv1f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    iXLen %2)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfrsqrt7.nxv1f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfrsqrt7_v_nxv1f16_nxv1f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfrsqrt7_v_nxv1f16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfrsqrt7.v v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfrsqrt7_v_nxv1f16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfrsqrt7.v v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfrsqrt7.nxv1f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    iXLen %2)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfsqrt.nxv1f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfsqrt_v_nxv1f16_nxv1f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfsqrt_v_nxv1f16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfsqrt.v v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfsqrt_v_nxv1f16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfsqrt.v v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfsqrt.nxv1f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    iXLen %2)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x float> @llvm.riscv.vfwcvt.f.f.v.nxv1f32.nxv1f16(
  <vscale x 1 x float>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x float> @intrinsic_vfwcvt_f.f.v_nxv1f32_nxv1f16(<vscale x 1 x float> %0, <vscale x 1 x half> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfwcvt_f.f.v_nxv1f32_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfwcvt.f.f.v v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfwcvt_f.f.v_nxv1f32_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfwcvt.f.f.v v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x float> @llvm.riscv.vfwcvt.f.f.v.nxv1f32.nxv1f16(
    <vscale x 1 x float> %0,
    <vscale x 1 x half> %1,
    iXLen %2)

  ret <vscale x 1 x float> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfwcvt.f.x.v.nxv1f16.nxv1i8(
  <vscale x 1 x half>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfwcvt_f.x.v_nxv1f16_nxv1i8(<vscale x 1 x half> %0, <vscale x 1 x i8> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfwcvt_f.x.v_nxv1f16_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vfwcvt.f.x.v v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfwcvt_f.x.v_nxv1f16_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vfwcvt.f.x.v v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfwcvt.f.x.v.nxv1f16.nxv1i8(
    <vscale x 1 x half> %0,
    <vscale x 1 x i8> %1,
    iXLen %2)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfwcvt.f.xu.v.nxv1f16.nxv1i8(
  <vscale x 1 x half>,
  <vscale x 1 x i8>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfwcvt_f.xu.v_nxv1f16_nxv1i8(<vscale x 1 x half> %0, <vscale x 1 x i8> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfwcvt_f.xu.v_nxv1f16_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vfwcvt.f.xu.v v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfwcvt_f.xu.v_nxv1f16_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vfwcvt.f.xu.v v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfwcvt.f.xu.v.nxv1f16.nxv1i8(
    <vscale x 1 x half> %0,
    <vscale x 1 x i8> %1,
    iXLen %2)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x i32> @llvm.riscv.vfwcvt.rtz.x.f.v.nxv1i32.nxv1f16(
  <vscale x 1 x i32>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x i32> @intrinsic_vfwcvt_rtz.x.f.v_nxv1i32_nxv1f16(<vscale x 1 x i32> %0, <vscale x 1 x half> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfwcvt_rtz.x.f.v_nxv1i32_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfwcvt.rtz.x.f.v v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfwcvt_rtz.x.f.v_nxv1i32_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfwcvt.rtz.x.f.v v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i32> @llvm.riscv.vfwcvt.rtz.x.f.v.nxv1i32.nxv1f16(
    <vscale x 1 x i32> %0,
    <vscale x 1 x half> %1,
    iXLen %2)

  ret <vscale x 1 x i32> %a
}

declare <vscale x 1 x i32> @llvm.riscv.vfwcvt.rtz.xu.f.v.nxv1i32.nxv1f16(
  <vscale x 1 x i32>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x i32> @intrinsic_vfwcvt_rtz.xu.f.v_nxv1i32_nxv1f16(<vscale x 1 x i32> %0, <vscale x 1 x half> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfwcvt_rtz.xu.f.v_nxv1i32_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfwcvt.rtz.xu.f.v v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfwcvt_rtz.xu.f.v_nxv1i32_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfwcvt.rtz.xu.f.v v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i32> @llvm.riscv.vfwcvt.rtz.xu.f.v.nxv1i32.nxv1f16(
    <vscale x 1 x i32> %0,
    <vscale x 1 x half> %1,
    iXLen %2)

  ret <vscale x 1 x i32> %a
}

declare <vscale x 1 x i32> @llvm.riscv.vfwcvt.x.f.v.nxv1i32.nxv1f16(
  <vscale x 1 x i32>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x i32> @intrinsic_vfwcvt_x.f.v_nxv1i32_nxv1f16(<vscale x 1 x i32> %0, <vscale x 1 x half> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfwcvt_x.f.v_nxv1i32_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfwcvt.x.f.v v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfwcvt_x.f.v_nxv1i32_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfwcvt.x.f.v v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i32> @llvm.riscv.vfwcvt.x.f.v.nxv1i32.nxv1f16(
    <vscale x 1 x i32> %0,
    <vscale x 1 x half> %1,
    iXLen %2)

  ret <vscale x 1 x i32> %a
}

declare <vscale x 1 x i32> @llvm.riscv.vfwcvt.xu.f.v.nxv1i32.nxv1f16(
  <vscale x 1 x i32>,
  <vscale x 1 x half>,
  iXLen);

define <vscale x 1 x i32> @intrinsic_vfwcvt_xu.f.v_nxv1i32_nxv1f16(<vscale x 1 x i32> %0, <vscale x 1 x half> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_vfwcvt_xu.f.v_nxv1i32_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vfwcvt.xu.f.v v8, v9
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfwcvt_xu.f.v_nxv1i32_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vfwcvt.xu.f.v v8, v9
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i32> @llvm.riscv.vfwcvt.xu.f.v.nxv1i32.nxv1f16(
    <vscale x 1 x i32> %0,
    <vscale x 1 x half> %1,
    iXLen %2)

  ret <vscale x 1 x i32> %a
}

declare <vscale x 1 x i8> @llvm.riscv.viota.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i1>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_viota_m_nxv1i8_nxv1i1(<vscale x 1 x i8> %0, <vscale x 1 x i1> %1, iXLen %2) nounwind {
; RV32-LABEL: intrinsic_viota_m_nxv1i8_nxv1i1:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    viota.m v8, v0
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_viota_m_nxv1i8_nxv1i1:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    viota.m v8, v0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.viota.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i1> %1,
    iXLen %2)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vadc.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i1>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vadc_vvm_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, <vscale x 1 x i1> %3, iXLen %4) nounwind {
; RV32-LABEL: intrinsic_vadc_vvm_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vadc.vvm v8, v9, v10, v0
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vadc_vvm_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vadc.vvm v8, v9, v10, v0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vadc.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    <vscale x 1 x i1> %3,
    iXLen %4)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vsbc.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i1>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vsbc_vvm_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, <vscale x 1 x i1> %3, iXLen %4) nounwind {
; RV32-LABEL: intrinsic_vsbc_vvm_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vsbc.vvm v8, v9, v10, v0
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vsbc_vvm_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vsbc.vvm v8, v9, v10, v0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vsbc.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    <vscale x 1 x i1> %3,
    iXLen %4)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8(
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i8>,
  <vscale x 1 x i1>,
  iXLen);

define <vscale x 1 x i8> @intrinsic_vmerge_vvm_nxv1i8_nxv1i8_nxv1i8(<vscale x 1 x i8> %0, <vscale x 1 x i8> %1, <vscale x 1 x i8> %2, <vscale x 1 x i1> %3, iXLen %4) nounwind {
; RV32-LABEL: intrinsic_vmerge_vvm_nxv1i8_nxv1i8_nxv1i8:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV32-NEXT:    vmerge.vvm v8, v9, v10, v0
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vmerge_vvm_nxv1i8_nxv1i8_nxv1i8:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e8, mf8, tu, mu
; RV64-NEXT:    vmerge.vvm v8, v9, v10, v0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x i8> @llvm.riscv.vmerge.nxv1i8.nxv1i8(
    <vscale x 1 x i8> %0,
    <vscale x 1 x i8> %1,
    <vscale x 1 x i8> %2,
    <vscale x 1 x i1> %3,
    iXLen %4)

  ret <vscale x 1 x i8> %a
}

declare <vscale x 8 x i64> @llvm.riscv.vmerge.nxv8i64.i64(
  <vscale x 8 x i64>,
  <vscale x 8 x i64>,
  i64,
  <vscale x 8 x i1>,
  iXLen);

define <vscale x 8 x i64> @intrinsic_vmerge_vxm_nxv8i64_nxv8i64_i64(<vscale x 8 x i64> %0, <vscale x 8 x i64> %1, i64 %2, <vscale x 8 x i1> %3, iXLen %4) nounwind {
; RV32-LABEL: intrinsic_vmerge_vxm_nxv8i64_nxv8i64_i64:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    sw a1, 12(sp)
; RV32-NEXT:    sw a0, 8(sp)
; RV32-NEXT:    vsetvli zero, a2, e64, m8, ta, mu
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vlse64.v v24, (a0), zero
; RV32-NEXT:    vsetvli zero, zero, e64, m8, tu, mu
; RV32-NEXT:    vmerge.vvm v8, v16, v24, v0
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vmerge_vxm_nxv8i64_nxv8i64_i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a1, e64, m8, tu, mu
; RV64-NEXT:    vmerge.vxm v8, v16, a0, v0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 8 x i64> @llvm.riscv.vmerge.nxv8i64.i64(
    <vscale x 8 x i64> %0,
    <vscale x 8 x i64> %1,
    i64 %2,
    <vscale x 8 x i1> %3,
    iXLen %4)

  ret <vscale x 8 x i64> %a
}

define <vscale x 8 x i64> @intrinsic_vmerge_vim_nxv8i64_nxv8i64_i64(<vscale x 8 x i64> %0, <vscale x 8 x i64> %1, <vscale x 8 x i1> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vmerge_vim_nxv8i64_nxv8i64_i64:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    li a1, 15
; RV32-NEXT:    sw a1, 12(sp)
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    sw a1, 8(sp)
; RV32-NEXT:    vsetvli zero, a0, e64, m8, ta, mu
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vlse64.v v24, (a0), zero
; RV32-NEXT:    vsetvli zero, zero, e64, m8, tu, mu
; RV32-NEXT:    vmerge.vvm v8, v16, v24, v0
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vmerge_vim_nxv8i64_nxv8i64_i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    li a1, -1
; RV64-NEXT:    srli a1, a1, 28
; RV64-NEXT:    vsetvli zero, a0, e64, m8, tu, mu
; RV64-NEXT:    vmerge.vxm v8, v16, a1, v0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 8 x i64> @llvm.riscv.vmerge.nxv8i64.i64(
    <vscale x 8 x i64> %0,
    <vscale x 8 x i64> %1,
    i64 68719476735,
    <vscale x 8 x i1> %2,
    iXLen %3)

  ret <vscale x 8 x i64> %a
}

declare <vscale x 8 x double> @llvm.riscv.vfmerge.nxv8f64.f64(
  <vscale x 8 x double>,
  <vscale x 8 x double>,
  double,
  <vscale x 8 x i1>,
  iXLen);

define <vscale x 8 x double> @intrinsic_vfmerge_vfm_nxv8f64_nxv8f64_f64(<vscale x 8 x double> %0, <vscale x 8 x double> %1, double %2, <vscale x 8 x i1> %3, iXLen %4) nounwind {
; RV32-LABEL: intrinsic_vfmerge_vfm_nxv8f64_nxv8f64_f64:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e64, m8, tu, mu
; RV32-NEXT:    vfmerge.vfm v8, v16, fa0, v0
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfmerge_vfm_nxv8f64_nxv8f64_f64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e64, m8, tu, mu
; RV64-NEXT:    vfmerge.vfm v8, v16, fa0, v0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 8 x double> @llvm.riscv.vfmerge.nxv8f64.f64(
    <vscale x 8 x double> %0,
    <vscale x 8 x double> %1,
    double %2,
    <vscale x 8 x i1> %3,
    iXLen %4)

  ret <vscale x 8 x double> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfmerge.nxv1f16.nxv1f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  <vscale x 1 x i1>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfmerge_vvm_nxv1f16_nxv1f16_nxv1f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, <vscale x 1 x half> %2, <vscale x 1 x i1> %3, iXLen %4) nounwind {
; RV32-LABEL: intrinsic_vfmerge_vvm_nxv1f16_nxv1f16_nxv1f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vmerge.vvm v8, v9, v10, v0
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfmerge_vvm_nxv1f16_nxv1f16_nxv1f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vmerge.vvm v8, v9, v10, v0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfmerge.nxv1f16.nxv1f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    <vscale x 1 x half> %2,
    <vscale x 1 x i1> %3,
    iXLen %4)

  ret <vscale x 1 x half> %a
}

declare <vscale x 1 x half> @llvm.riscv.vfmerge.nxv1f16.f16(
  <vscale x 1 x half>,
  <vscale x 1 x half>,
  half,
  <vscale x 1 x i1>,
  iXLen);

define <vscale x 1 x half> @intrinsic_vfmerge_vzm_nxv1f16_nxv1f16_f16(<vscale x 1 x half> %0, <vscale x 1 x half> %1, <vscale x 1 x i1> %2, iXLen %3) nounwind {
; RV32-LABEL: intrinsic_vfmerge_vzm_nxv1f16_nxv1f16_f16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV32-NEXT:    vmerge.vim v8, v9, 0, v0
; RV32-NEXT:    ret
;
; RV64-LABEL: intrinsic_vfmerge_vzm_nxv1f16_nxv1f16_f16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    vsetvli zero, a0, e16, mf4, tu, mu
; RV64-NEXT:    vmerge.vim v8, v9, 0, v0
; RV64-NEXT:    ret
entry:
  %a = call <vscale x 1 x half> @llvm.riscv.vfmerge.nxv1f16.f16(
    <vscale x 1 x half> %0,
    <vscale x 1 x half> %1,
    half zeroinitializer,
    <vscale x 1 x i1> %2,
    iXLen %3)

  ret <vscale x 1 x half> %a
}
